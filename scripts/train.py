"""
å•æ¬¡è®­ç»ƒå¯åŠ¨è„šæœ¬
è§£æå‚æ•°å¹¶è°ƒç”¨æ ¸å¿ƒè®­ç»ƒæ¨¡å—

è¯¥è„šæœ¬æ˜¯è®­ç»ƒæµç¨‹çš„å…¥å£ç‚¹ï¼Œä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š
1. è§£æå‘½ä»¤è¡Œå‚æ•°å’Œé…ç½®æ–‡ä»¶
2. æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦ä½¿ç”¨å¤šGPUè®­ç»ƒ
3. å¯åŠ¨è®­ç»ƒè¿‡ç¨‹
4. æ˜¾ç¤ºè®­ç»ƒç»“æœ
"""

import sys
import os
import subprocess

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„ï¼Œç¡®ä¿å¯ä»¥æ­£ç¡®å¯¼å…¥é¡¹ç›®å†…çš„æ¨¡å—
# é€šè¿‡os.path.dirnameçš„ä¸¤å±‚åµŒå¥—è°ƒç”¨ï¼Œè·å–åˆ°é¡¹ç›®æ ¹ç›®å½•
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.utils.config_parser import parse_single_training_arguments  # å‚æ•°è§£æå™¨
from src.trainers.base_trainer import run_training                   # æ ¸å¿ƒè®­ç»ƒå‡½æ•°


def launch_with_accelerate(original_args):
    """
    ä½¿ç”¨accelerate launché‡æ–°å¯åŠ¨è„šæœ¬ä»¥æ”¯æŒå¤šGPUè®­ç»ƒ
    
    å½“ç”¨æˆ·æŒ‡å®š--multi_gpuå‚æ•°ä½†è„šæœ¬æœªåœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­è¿è¡Œæ—¶ï¼Œ
    éœ€è¦ä½¿ç”¨accelerate launché‡æ–°å¯åŠ¨è„šæœ¬ä»¥æ­£ç¡®åˆå§‹åŒ–å¤šGPUç¯å¢ƒã€‚

    Args:
        original_args: åŸå§‹å‘½ä»¤è¡Œå‚æ•°å¯¹è±¡

    Returns:
        int: å­è¿›ç¨‹è¿”å›ç 
    """
    # æ„å»ºaccelerate launchå‘½ä»¤
    cmd = ["accelerate", "launch"]
    
    # å¦‚æœæœ‰é¢å¤–çš„accelerateå‚æ•°ï¼Œåˆ™æ·»åŠ åˆ°å‘½ä»¤ä¸­
    if hasattr(original_args, 'accelerate_args') and original_args.accelerate_args:
        cmd.extend(original_args.accelerate_args.split())
    
    # æ·»åŠ å½“å‰è„šæœ¬è·¯å¾„å’Œè¿‡æ»¤æ‰--multi_gpuçš„å‚æ•°
    cmd.append(__file__)
    cmd.extend([arg for arg in sys.argv[1:] if arg != "--multi_gpu"])
    
    # æ‰§è¡Œå‘½ä»¤å¹¶è¿”å›ç»“æœ
    result = subprocess.run(cmd)
    return result.returncode


def print_training_info(args, config):
    """
    æ‰“å°è®­ç»ƒå®Œæˆåçš„ä¿¡æ¯æ‘˜è¦
    
    å±•ç¤ºå®éªŒåç§°ã€è¶…å‚æ•°é…ç½®å’Œè®­ç»ƒç¯å¢ƒç­‰å…³é”®ä¿¡æ¯ï¼Œ
    å¸®åŠ©ç”¨æˆ·å¿«é€Ÿäº†è§£è®­ç»ƒç»“æœå’Œé…ç½®ã€‚
    åªåœ¨ä¸»è¿›ç¨‹ä¸­æ‰“å°ï¼Œé¿å…å¤šå¡è®­ç»ƒæ—¶çš„é‡å¤è¾“å‡ºã€‚

    Args:
        args: å‘½ä»¤è¡Œå‚æ•°å¯¹è±¡
        config: è®­ç»ƒé…ç½®å­—å…¸
    """
    # åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œåªè®©ä¸»è¿›ç¨‹æ‰“å°ä¿¡æ¯
    local_rank = int(os.environ.get('LOCAL_RANK', 0))
    if local_rank != 0:
        return
        
    print("\n" + "="*60)
    print("ğŸ¯ è®­ç»ƒå®Œæˆä¿¡æ¯")
    print("="*60)
    
    # å®éªŒä¿¡æ¯
    experiment_name = config['training']['experiment_name']
    print(f"ğŸ“‹ å®éªŒåç§°: {experiment_name}")
    
    # è¶…å‚æ•°ä¿¡æ¯
    hp = config['hyperparameters']
    print(f"ğŸ“Š è¶…å‚æ•°:")
    print(f"   å­¦ä¹ ç‡: {hp.get('learning_rate', 'N/A')}")
    print(f"   æ‰¹å¤§å°: {hp.get('batch_size', 'N/A')}")
    print(f"   è®­ç»ƒè½®æ•°: {hp.get('epochs', 'N/A')}")
    print(f"   Dropout: {hp.get('dropout', 'N/A')}")
    
    # GPUç¯å¢ƒä¿¡æ¯
    if args.use_cpu:
        # ä½¿ç”¨CPUè®­ç»ƒ
        print(f"ğŸ’» è®­ç»ƒç¯å¢ƒ: CPU")
    else:
        # ä½¿ç”¨GPUè®­ç»ƒï¼Œè·å–å¯è§çš„GPUè®¾å¤‡
        gpu_ids = os.environ.get('CUDA_VISIBLE_DEVICES', 'æ‰€æœ‰å¯ç”¨GPU')
        
        # æ£€æŸ¥æ˜¯å¦åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­è¿è¡Œ
        is_distributed = (os.environ.get('LOCAL_RANK') is not None or 
                         os.environ.get('RANK') is not None)
        
        if is_distributed:
            # åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ
            world_size = int(os.environ.get('WORLD_SIZE', '1'))
            training_mode = "å¤šå¡è®­ç»ƒ" if world_size > 1 else "å•å¡è®­ç»ƒ"
            local_rank = os.environ.get('LOCAL_RANK', 'N/A')
            print(f"ğŸ–¥ï¸  è®­ç»ƒç¯å¢ƒ: {training_mode}")
            print(f"ğŸ”§ GPUè®¾å¤‡: {gpu_ids}")
            print(f"ğŸŒ åˆ†å¸ƒå¼ä¿¡æ¯: LOCAL_RANK={local_rank}, WORLD_SIZE={world_size}")
        else:
            # éåˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ
            training_mode = "å¤šå¡è®­ç»ƒ" if args.multi_gpu else "å•å¡è®­ç»ƒ"
            print(f"ğŸ–¥ï¸  è®­ç»ƒç¯å¢ƒ: {training_mode}")
            print(f"ğŸ”§ GPUè®¾å¤‡: {gpu_ids}")
    
    print("="*60)


def main():
    """
    ä¸»å‡½æ•°ï¼Œå¤„ç†å•æ¬¡å®éªŒè®­ç»ƒ
    
    æ•´ä¸ªè®­ç»ƒæµç¨‹çš„æ§åˆ¶ä¸­å¿ƒï¼Œåè°ƒå‚æ•°è§£æã€ç¯å¢ƒæ£€æŸ¥ã€
    è®­ç»ƒå¯åŠ¨å’Œç»“æœå±•ç¤ºç­‰å„ä¸ªç¯èŠ‚ã€‚
    
    Returns:
        int: ç¨‹åºé€€å‡ºç ï¼Œ0è¡¨ç¤ºæˆåŠŸ
    """
    # è§£æå‘½ä»¤è¡Œå‚æ•°å’Œé…ç½®æ–‡ä»¶
    args, config = parse_single_training_arguments()
    
    # æ£€æŸ¥å½“å‰æ˜¯å¦å·²åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­è¿è¡Œ
    # é€šè¿‡æ£€æŸ¥accelerateè®¾ç½®çš„ç¯å¢ƒå˜é‡æ¥åˆ¤æ–­
    is_distributed = (os.environ.get('LOCAL_RANK') is not None or 
                     os.environ.get('RANK') is not None)
    
    # å¦‚æœç”¨æˆ·æŒ‡å®šå¤šGPUè®­ç»ƒä½†å½“å‰ä¸åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­ï¼Œä½¿ç”¨accelerateé‡æ–°å¯åŠ¨
    if args.multi_gpu and not is_distributed:
        return launch_with_accelerate(args)
    
    # è·å–å®éªŒåç§°å¹¶å¯åŠ¨è®­ç»ƒ
    experiment_name = config['training']['experiment_name']
    result = run_training(config, experiment_name)
    
    # æ‰“å°è®­ç»ƒå®Œæˆä¿¡æ¯
    print_training_info(args, config)
    
    return 0


# ç¨‹åºå…¥å£ç‚¹
if __name__ == "__main__":
    # æ‰§è¡Œä¸»å‡½æ•°å¹¶è·å–é€€å‡ºç 
    exit_code = main()
    # é€€å‡ºç¨‹åº
    sys.exit(exit_code)