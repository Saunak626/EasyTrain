"""
å¤šæ ‡ç­¾åˆ†ç±»è¯„ä¼°æŒ‡æ ‡
æä¾›ä¸“é—¨é’ˆå¯¹å¤šæ ‡ç­¾åˆ†ç±»ä»»åŠ¡çš„è¯„ä¼°æŒ‡æ ‡è®¡ç®—
"""

import torch
import numpy as np
from typing import Dict, Tuple
import logging

logger = logging.getLogger(__name__)


def calculate_multilabel_metrics(outputs: torch.Tensor, targets: torch.Tensor, 
                                threshold: float = 0.5) -> Dict[str, float]:
    """è®¡ç®—å¤šæ ‡ç­¾åˆ†ç±»çš„å„ç§è¯„ä¼°æŒ‡æ ‡
    
    Args:
        outputs (torch.Tensor): æ¨¡å‹è¾“å‡ºçš„logitsï¼Œå½¢çŠ¶ä¸º(batch_size, num_classes)
        targets (torch.Tensor): çœŸå®æ ‡ç­¾ï¼Œå½¢çŠ¶ä¸º(batch_size, num_classes)
        threshold (float): äºŒå€¼åŒ–é˜ˆå€¼ï¼Œé»˜è®¤0.5
        
    Returns:
        Dict[str, float]: åŒ…å«å„ç§æŒ‡æ ‡çš„å­—å…¸
    """
    # è·å–é¢„æµ‹
    predictions = torch.sigmoid(outputs) > threshold
    targets_bool = targets.bool()
    
    batch_size, num_classes = targets.shape
    
    # 1. æ±‰æ˜å‡†ç¡®ç‡ï¼ˆHamming Accuracyï¼‰- æ¨èä½œä¸ºä¸»è¦æŒ‡æ ‡
    hamming_accuracy = (predictions == targets_bool).float().mean().item()
    
    # 2. å®Œå…¨åŒ¹é…å‡†ç¡®ç‡ï¼ˆExact Match Accuracy / Subset Accuracyï¼‰
    exact_match_accuracy = (predictions == targets_bool).all(dim=1).float().mean().item()
    
    # 3. è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°
    tp = (predictions * targets_bool).sum(dim=1).float()  # True Positives
    fp = (predictions * (~targets_bool)).sum(dim=1).float()  # False Positives
    fn = ((~predictions) * targets_bool).sum(dim=1).float()  # False Negatives
    
    # é¿å…é™¤é›¶é”™è¯¯
    epsilon = 1e-8
    
    # æ ·æœ¬çº§ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1
    precision_per_sample = tp / (tp + fp + epsilon)
    recall_per_sample = tp / (tp + fn + epsilon)
    f1_per_sample = 2 * precision_per_sample * recall_per_sample / (precision_per_sample + recall_per_sample + epsilon)
    
    # å¹³å‡æŒ‡æ ‡
    precision = precision_per_sample.mean().item()
    recall = recall_per_sample.mean().item()
    f1_score = f1_per_sample.mean().item()
    
    # 4. æ ‡ç­¾çº§æŒ‡æ ‡ï¼ˆLabel-wise metricsï¼‰
    # æ¯ä¸ªæ ‡ç­¾çš„ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1
    tp_label = (predictions * targets_bool).sum(dim=0).float()
    fp_label = (predictions * (~targets_bool)).sum(dim=0).float()
    fn_label = ((~predictions) * targets_bool).sum(dim=0).float()
    
    precision_label = tp_label / (tp_label + fp_label + epsilon)
    recall_label = tp_label / (tp_label + fn_label + epsilon)
    f1_label = 2 * precision_label * recall_label / (precision_label + recall_label + epsilon)
    
    # æ ‡ç­¾çº§å¹³å‡æŒ‡æ ‡
    macro_precision = precision_label.mean().item()
    macro_recall = recall_label.mean().item()
    macro_f1 = f1_label.mean().item()
    
    # 5. è¦†ç›–ç‡ç›¸å…³æŒ‡æ ‡
    # å¹³å‡æ¿€æ´»æ ‡ç­¾æ•°
    avg_predicted_labels = predictions.sum(dim=1).float().mean().item()
    avg_true_labels = targets_bool.sum(dim=1).float().mean().item()
    
    # æ ‡ç­¾è¦†ç›–ç‡
    label_coverage = (predictions.sum(dim=0) > 0).float().mean().item()
    
    return {
        # ä¸»è¦æŒ‡æ ‡
        'hamming_accuracy': hamming_accuracy,
        'exact_match_accuracy': exact_match_accuracy,
        'f1_score': f1_score,
        'precision': precision,
        'recall': recall,
        
        # æ ‡ç­¾çº§æŒ‡æ ‡
        'macro_precision': macro_precision,
        'macro_recall': macro_recall,
        'macro_f1': macro_f1,
        
        # è¦†ç›–ç‡æŒ‡æ ‡
        'avg_predicted_labels': avg_predicted_labels,
        'avg_true_labels': avg_true_labels,
        'label_coverage': label_coverage,
    }


def calculate_per_label_metrics(outputs: torch.Tensor, targets: torch.Tensor, 
                               label_names: list = None, threshold: float = 0.5) -> Dict[str, Dict[str, float]]:
    """è®¡ç®—æ¯ä¸ªæ ‡ç­¾çš„è¯¦ç»†æŒ‡æ ‡
    
    Args:
        outputs (torch.Tensor): æ¨¡å‹è¾“å‡ºçš„logits
        targets (torch.Tensor): çœŸå®æ ‡ç­¾
        label_names (list): æ ‡ç­¾åç§°åˆ—è¡¨
        threshold (float): äºŒå€¼åŒ–é˜ˆå€¼
        
    Returns:
        Dict[str, Dict[str, float]]: æ¯ä¸ªæ ‡ç­¾çš„æŒ‡æ ‡å­—å…¸
    """
    predictions = torch.sigmoid(outputs) > threshold
    targets_bool = targets.bool()
    
    num_classes = targets.shape[1]
    if label_names is None:
        label_names = [f'label_{i}' for i in range(num_classes)]
    
    per_label_metrics = {}
    
    for i, label_name in enumerate(label_names):
        pred_i = predictions[:, i]
        target_i = targets_bool[:, i]
        
        tp = (pred_i * target_i).sum().float().item()
        fp = (pred_i * (~target_i)).sum().float().item()
        fn = ((~pred_i) * target_i).sum().float().item()
        tn = ((~pred_i) * (~target_i)).sum().float().item()
        
        epsilon = 1e-8
        precision = tp / (tp + fp + epsilon)
        recall = tp / (tp + fn + epsilon)
        f1 = 2 * precision * recall / (precision + recall + epsilon)
        accuracy = (tp + tn) / (tp + tn + fp + fn + epsilon)
        
        per_label_metrics[label_name] = {
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'accuracy': accuracy,
            'support': int(target_i.sum().item()),  # æ­£æ ·æœ¬æ•°
            'tp': int(tp),
            'fp': int(fp),
            'fn': int(fn),
            'tn': int(tn)
        }
    
    return per_label_metrics


def print_multilabel_report(outputs: torch.Tensor, targets: torch.Tensor, 
                           label_names: list = None, threshold: float = 0.5):
    """æ‰“å°å¤šæ ‡ç­¾åˆ†ç±»çš„è¯¦ç»†æŠ¥å‘Š
    
    Args:
        outputs (torch.Tensor): æ¨¡å‹è¾“å‡ºçš„logits
        targets (torch.Tensor): çœŸå®æ ‡ç­¾
        label_names (list): æ ‡ç­¾åç§°åˆ—è¡¨
        threshold (float): äºŒå€¼åŒ–é˜ˆå€¼
    """
    # è®¡ç®—æ€»ä½“æŒ‡æ ‡
    overall_metrics = calculate_multilabel_metrics(outputs, targets, threshold)
    
    print("ğŸ·ï¸  å¤šæ ‡ç­¾åˆ†ç±»è¯„ä¼°æŠ¥å‘Š")
    print("=" * 60)
    
    print("ğŸ“Š æ€»ä½“æŒ‡æ ‡:")
    print(f"   æ±‰æ˜å‡†ç¡®ç‡: {overall_metrics['hamming_accuracy']:.1%}")
    print(f"   å®Œå…¨åŒ¹é…å‡†ç¡®ç‡: {overall_metrics['exact_match_accuracy']:.1%}")
    print(f"   F1åˆ†æ•°: {overall_metrics['f1_score']:.1%}")
    print(f"   ç²¾ç¡®ç‡: {overall_metrics['precision']:.1%}")
    print(f"   å¬å›ç‡: {overall_metrics['recall']:.1%}")
    
    print(f"\nğŸ“ˆ æ ‡ç­¾çº§æŒ‡æ ‡:")
    print(f"   å®å¹³å‡ç²¾ç¡®ç‡: {overall_metrics['macro_precision']:.1%}")
    print(f"   å®å¹³å‡å¬å›ç‡: {overall_metrics['macro_recall']:.1%}")
    print(f"   å®å¹³å‡F1: {overall_metrics['macro_f1']:.1%}")
    
    print(f"\nğŸ“‹ è¦†ç›–ç‡ç»Ÿè®¡:")
    print(f"   å¹³å‡é¢„æµ‹æ ‡ç­¾æ•°: {overall_metrics['avg_predicted_labels']:.1f}")
    print(f"   å¹³å‡çœŸå®æ ‡ç­¾æ•°: {overall_metrics['avg_true_labels']:.1f}")
    print(f"   æ ‡ç­¾è¦†ç›–ç‡: {overall_metrics['label_coverage']:.1%}")
    
    # è®¡ç®—æ¯ä¸ªæ ‡ç­¾çš„æŒ‡æ ‡
    if label_names is not None:
        per_label_metrics = calculate_per_label_metrics(outputs, targets, label_names, threshold)
        
        print(f"\nğŸ·ï¸  æ¯ä¸ªæ ‡ç­¾çš„è¯¦ç»†æŒ‡æ ‡:")
        print(f"{'æ ‡ç­¾å':<12} {'ç²¾ç¡®ç‡':<8} {'å¬å›ç‡':<8} {'F1åˆ†æ•°':<8} {'æ”¯æŒæ•°':<6}")
        print("-" * 50)
        
        for label_name, metrics in per_label_metrics.items():
            print(f"{label_name:<12} {metrics['precision']:<8.1%} {metrics['recall']:<8.1%} "
                  f"{metrics['f1_score']:<8.1%} {metrics['support']:<6}")


def get_best_threshold(outputs: torch.Tensor, targets: torch.Tensor, 
                      metric: str = 'f1_score', thresholds: np.ndarray = None) -> Tuple[float, float]:
    """å¯»æ‰¾æœ€ä½³é˜ˆå€¼
    
    Args:
        outputs (torch.Tensor): æ¨¡å‹è¾“å‡ºçš„logits
        targets (torch.Tensor): çœŸå®æ ‡ç­¾
        metric (str): ä¼˜åŒ–çš„æŒ‡æ ‡åç§°
        thresholds (np.ndarray): è¦æµ‹è¯•çš„é˜ˆå€¼æ•°ç»„
        
    Returns:
        Tuple[float, float]: (æœ€ä½³é˜ˆå€¼, æœ€ä½³æŒ‡æ ‡å€¼)
    """
    if thresholds is None:
        thresholds = np.arange(0.1, 1.0, 0.05)
    
    best_threshold = 0.5
    best_score = 0.0
    
    for threshold in thresholds:
        metrics = calculate_multilabel_metrics(outputs, targets, threshold)
        score = metrics.get(metric, 0.0)
        
        if score > best_score:
            best_score = score
            best_threshold = threshold
    
    return best_threshold, best_score
