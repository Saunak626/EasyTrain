"""

æ ¸å¿ƒä¼˜åŒ–ï¼š
- ä¸€æ¬¡å¯åŠ¨ï¼Œä¸²è¡Œæ‰§è¡Œæ‰€æœ‰å®éªŒ
- æ‰€æœ‰å®éªŒåœ¨åŒä¸€è¿›ç¨‹ä¸­é¡ºåºè¿è¡Œ
- æ— éœ€ä¸ºæ¯ä¸ªå®éªŒåˆ›å»ºå­è¿›ç¨‹

å¯åŠ¨æ–¹å¼ï¼š
- å•å¡ï¼špython scripts/grid_search_unified.py --config ...
- å¤šå¡ï¼šaccelerate launch scripts/grid_search_unified.py --config ...

"""
import itertools
import yaml
import os
import sys
import csv
import json
import fcntl

from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.utils.config_parser import parse_arguments

# ======================
# æ¨¡å—çº§å¸¸é‡é…ç½®
# ======================

# ç½‘æ ¼æœç´¢ç›¸å…³å¸¸é‡
GRID_SEARCH_CONSTANTS = {
    'model_type_key': 'model.type',
    'batch_size_key': 'hp.batch_size',
    'group_key': 'group',
    'excluded_params': ['model.type', 'hp.batch_size'],
    'csv_base_columns': [
        'exp_name', 'model.type', 'group', 'success', 'trained_epochs',
        # ğŸ¯ å¤šæ ‡ç­¾åˆ†ç±»å…³é”®æŒ‡æ ‡ï¼ˆä¼˜å…ˆæ˜¾ç¤ºï¼‰
        'best_weighted_f1', 'best_weighted_accuracy', 'best_macro_accuracy', 'best_micro_accuracy',
        'best_macro_f1', 'best_micro_f1', 'best_macro_precision', 'best_macro_recall',
        'final_weighted_f1', 'final_weighted_accuracy', 'final_macro_accuracy', 'final_micro_accuracy',
        'final_macro_f1', 'final_micro_f1',
        # ä¼ ç»Ÿå­—æ®µï¼ˆå‘åå…¼å®¹ï¼‰
        'best_accuracy', 'final_accuracy'
    ],
    'common_runtime_params': [
        'data_percentage',
        'optimizer.name', 'scheduler.name', 'loss.name'
    ],
    'excluded_csv_params': ['epochs', 'batch_size', 'learning_rate']
}

# ======================
# å‚æ•°ç»„åˆç”Ÿæˆå™¨ç±»
# ======================

class ParameterCombinationGenerator:
    """å‚æ•°ç»„åˆç”Ÿæˆå™¨
    
    è´Ÿè´£å¤„ç†ç½‘æ ¼æœç´¢çš„å‚æ•°ç»„åˆç”Ÿæˆé€»è¾‘ï¼Œæ”¯æŒåˆ†ç»„å¼é…ç½®å’Œæ¨¡å‹-batch_sizeæ™ºèƒ½é…å¯¹ã€‚
    """
    
    def __init__(self, config: Dict[str, Any]):
        """åˆå§‹åŒ–å‚æ•°ç»„åˆç”Ÿæˆå™¨
        
        Args:
            config: ç½‘æ ¼æœç´¢é…ç½®å­—å…¸
        """
        self.config = config
        self.constants = GRID_SEARCH_CONSTANTS
    
    def generate_combinations(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆå‚æ•°ç»„åˆçš„ä¸»å…¥å£å‡½æ•°
        
        Returns:
            å‚æ•°ç»„åˆåˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸ä»£è¡¨ä¸€ç»„å®éªŒå‚æ•°
        """
        gs = (self.config or {}).get("grid_search", {}) or {}
        fixed = gs.get("fixed", {}) or {}
        models_to_train = self.config.get("models_to_train", [])
        
        # åˆ†ç»„å¼é…ç½®å¤„ç†
        if "groups" in gs and gs["groups"]:
            print(f"ğŸ“‹ ä½¿ç”¨åˆ†ç»„å¼ç½‘æ ¼æœç´¢é…ç½®")
            return self._generate_combinations_by_groups(gs["groups"], fixed, models_to_train)
        
        # è¾¹ç•Œæƒ…å†µï¼šæ— æœç´¢å‚æ•°ï¼Œä»åŸºç¡€é…ç½®ä¸­æå–ä¿¡æ¯
        else:
            print(f"âš ï¸  æœªæ‰¾åˆ°groupsé…ç½®ï¼Œä»åŸºç¡€é…ç½®ä¸­æå–å‚æ•°")
            base_params = {}
            
            # ä»åŸºç¡€é…ç½®ä¸­æå–æ¨¡å‹ç±»å‹
            if 'model' in self.config and 'type' in self.config['model']:
                base_params[self.constants['model_type_key']] = self.config['model']['type']
            
            # ä»åŸºç¡€é…ç½®ä¸­æå–å…¶ä»–å‚æ•°
            if 'optimizer' in self.config and 'name' in self.config['optimizer']:
                base_params['optimizer.name'] = self.config['optimizer']['name']
            
            if 'scheduler' in self.config and 'name' in self.config['scheduler']:
                base_params['scheduler.name'] = self.config['scheduler']['name']
            
            if 'loss' in self.config and 'name' in self.config['loss']:
                base_params['loss.name'] = self.config['loss']['name']
            
            # åˆå¹¶å›ºå®šå‚æ•°
            base_params.update(fixed)
            
            return [base_params] if base_params else []
    
    def _generate_combinations_by_groups(self, groups: Dict[str, Dict], fixed: Dict[str, Any],
                                        models_to_train: List[str]) -> List[Dict[str, Any]]:
        """åˆ†ç»„å¼å‚æ•°ç»„åˆç”Ÿæˆ
        
        Args:
            groups: åˆ†ç»„é…ç½®å­—å…¸
            fixed: å›ºå®šå‚æ•°å­—å…¸
            models_to_train: è¦è®­ç»ƒçš„æ¨¡å‹åˆ—è¡¨
        
        Returns:
            å‚æ•°ç»„åˆåˆ—è¡¨
        """
        all_combinations = []
        print(f"ğŸ¯ å‘ç° {len(groups)} ä¸ªæ¨¡å‹ç»„:")
        
        for group_name, group_config in groups.items():
            print(f"   - {group_name}: {group_config.get(self.constants['model_type_key'], [])}")
        
        print()  # ç©ºè¡Œåˆ†éš”
        
        for group_name, group_config in groups.items():
            print(f"ğŸ”§ å¤„ç†æ¨¡å‹ç»„: {group_name}")
            group_combinations = self._process_single_group(
                group_name, group_config, fixed, models_to_train
            )

            if group_combinations:
                all_combinations.extend(group_combinations)
                print(f"   âœ… ç»„ {group_name} ç”Ÿæˆ {len(group_combinations)} ä¸ªç»„åˆ "
                      f"({len(group_config.get(self.constants['model_type_key'], []))}æ¨¡å‹ Ã— "
                      f"{len(group_combinations) // max(1, len(group_config.get(self.constants['model_type_key'], [])))}å‚æ•°ç»„åˆ)")
            else:
                print(f"   â­ï¸  è·³è¿‡ç»„ {group_name}ï¼šæ— å¯ç”¨çš„æ¨¡å‹")
            print()  # ç©ºè¡Œåˆ†éš”

        print(f"ğŸ‰ åˆ†ç»„å¼æœç´¢æ€»è®¡ç”Ÿæˆ {len(all_combinations)} ä¸ªç»„åˆ")
        return all_combinations

    def _process_single_group(self, group_name: str, group_config: Dict[str, Any],
                             fixed: Dict[str, Any], models_to_train: List[str]) -> List[Dict[str, Any]]:
        """å¤„ç†å•ä¸ªæ¨¡å‹ç»„çš„å‚æ•°ç»„åˆ

        Args:
            group_name: ç»„å
            group_config: ç»„é…ç½®
            fixed: å›ºå®šå‚æ•°
            models_to_train: è¦è®­ç»ƒçš„æ¨¡å‹åˆ—è¡¨

        Returns:
            è¯¥ç»„çš„å‚æ•°ç»„åˆåˆ—è¡¨
        """
        model_type_key = self.constants['model_type_key']
        batch_size_key = self.constants['batch_size_key']

        # æå–æ¨¡å‹åˆ—è¡¨å’Œbatch_sizeåˆ—è¡¨
        models = group_config.get(model_type_key, [])
        batch_sizes = group_config.get(batch_size_key, [])

        # è¿‡æ»¤ï¼šåªä¿ç•™åœ¨ models_to_train ä¸­çš„æ¨¡å‹
        if models_to_train:
            enabled_models = [m for m in models if m in models_to_train]
            if not enabled_models:
                return []
            models = enabled_models

        # æ‰“å°ç»„å†…é…ç½®
        print(f"   ğŸ“‹ ç»„å†…é…ç½®:")
        print(f"      {model_type_key}: {models} (é•¿åº¦: {len(models)})")
        print(f"      {batch_size_key}: {batch_sizes} (é•¿åº¦: {len(batch_sizes)})")

        # æ™ºèƒ½é…å¯¹ï¼šæ¨¡å‹å’Œbatch_size
        model_batch_pairs = self._pair_models_with_batch_sizes(models, batch_sizes)

        # æå–å…¶ä»–å‚æ•°ï¼ˆæ’é™¤model.typeå’Œhp.batch_sizeï¼‰
        other_params = {
            k: v for k, v in group_config.items()
            if k not in self.constants['excluded_params']
        }

        # ç”Ÿæˆå…¶ä»–å‚æ•°çš„ç¬›å¡å°”ç§¯
        if other_params:
            other_keys = list(other_params.keys())
            other_values = [other_params[k] if isinstance(other_params[k], list) else [other_params[k]]
                          for k in other_keys]
            other_combinations = [dict(zip(other_keys, combo)) for combo in itertools.product(*other_values)]
        else:
            other_combinations = [{}]

        # ç»„åˆï¼š(æ¨¡å‹, batch_size) Ã— å…¶ä»–å‚æ•°
        group_combinations = []
        for (model, batch_size), other_combo in itertools.product(model_batch_pairs, other_combinations):
            combo = {
                model_type_key: model,
                batch_size_key: batch_size,
                self.constants['group_key']: group_name
            }
            combo.update(other_combo)
            combo.update(fixed)
            group_combinations.append(combo)

        return group_combinations

    def _pair_models_with_batch_sizes(self, models: List[str], batch_sizes: List[int]) -> List[Tuple[str, int]]:
        """æ™ºèƒ½é…å¯¹æ¨¡å‹å’Œbatch_size

        Args:
            models: æ¨¡å‹åˆ—è¡¨
            batch_sizes: batch_sizeåˆ—è¡¨

        Returns:
            (æ¨¡å‹, batch_size) é…å¯¹åˆ—è¡¨
        """
        if len(batch_sizes) == 1:
            # æƒ…å†µ1ï¼šåªæœ‰ä¸€ä¸ªbatch_sizeï¼Œæ‰€æœ‰æ¨¡å‹ä½¿ç”¨ç›¸åŒçš„batch_size
            return [(m, batch_sizes[0]) for m in models]

        elif len(batch_sizes) == len(models):
            # æƒ…å†µ2ï¼šbatch_sizeæ•°é‡ä¸æ¨¡å‹æ•°é‡ç›¸åŒï¼ŒæŒ‰é¡ºåºé…å¯¹
            print(f"   âœ… é•¿åº¦åŒ¹é…ï¼Œå°†æŒ‰é¡ºåºé…å¯¹")
            return list(zip(models, batch_sizes))

        else:
            # æƒ…å†µ3ï¼šé•¿åº¦ä¸åŒ¹é…ï¼Œæ‰©å……batch_sizeåˆ—è¡¨
            print(f"   ğŸ”„ æ‰©å……batch_size: {batch_sizes} (æ‰©å……åˆ°ä¸model.typeé•¿åº¦ä¸€è‡´)")
            expanded_batch_sizes = []
            for i in range(len(models)):
                expanded_batch_sizes.append(batch_sizes[i % len(batch_sizes)])
            print(f"   ğŸ”„ æ‰©å……å: {expanded_batch_sizes}")

            # æ‰“å°é…å¯¹ç»“æœ
            pairs = list(zip(models, expanded_batch_sizes))
            print(f"   ğŸ¯ å¯ç”¨çš„æ¨¡å‹é…å¯¹: {dict(pairs)}")
            return pairs


# ======================
# å®éªŒç»“æœç®¡ç†å™¨ç±»
# ======================

class ExperimentResultsManager:
    """å®éªŒç»“æœç®¡ç†å™¨

    è´Ÿè´£ç®¡ç†ç½‘æ ¼æœç´¢çš„ç»“æœæ–‡ä»¶ï¼ŒåŒ…æ‹¬ä¸»ç»“æœCSVã€è¯¦æƒ…CSVå’Œå•å®éªŒæ–‡ä»¶ã€‚
    """

    def __init__(self, csv_filepath: str, details_filepath: str, grid_search_dir: str):
        """åˆå§‹åŒ–ç»“æœç®¡ç†å™¨

        Args:
            csv_filepath: ä¸»ç»“æœCSVæ–‡ä»¶è·¯å¾„
            details_filepath: è¯¦æƒ…CSVæ–‡ä»¶è·¯å¾„
            grid_search_dir: ç½‘æ ¼æœç´¢ç›®å½•
        """
        self.csv_filepath = csv_filepath
        self.details_filepath = details_filepath
        self.grid_search_dir = grid_search_dir
        self.experiments_dir = os.path.join(grid_search_dir, "experiments")
        self.fieldnames = []

        # åˆ›å»ºå®éªŒç›®å½•
        os.makedirs(self.experiments_dir, exist_ok=True)
        print(f"ğŸ“ åˆ›å»ºå®éªŒæ–‡ä»¶å¤¹ç»“æ„: {self.experiments_dir}")

    def initialize_csv_file(self, fieldnames: List[str]) -> None:
        """åˆå§‹åŒ–CSVæ–‡ä»¶

        Args:
            fieldnames: CSVå­—æ®µååˆ—è¡¨
        """
        self.fieldnames = fieldnames

        # åˆå§‹åŒ–ä¸»ç»“æœCSV
        os.makedirs(os.path.dirname(self.csv_filepath), exist_ok=True)
        with open(self.csv_filepath, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()

        # åˆå§‹åŒ–è¯¦æƒ…CSV
        print(f"ğŸ“‹ åˆå§‹åŒ–è¯¦æƒ…è¡¨: {self.details_filepath}")
        with open(self.details_filepath, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()

    def append_result_to_csv(self, result: Dict[str, Any]) -> None:
        """è¿½åŠ ç»“æœåˆ°CSVæ–‡ä»¶

        Args:
            result: å®éªŒç»“æœå­—å…¸
        """
        # å†™å…¥ä¸»ç»“æœCSV
        with open(self.csv_filepath, 'a', newline='', encoding='utf-8') as f:
            fcntl.flock(f.fileno(), fcntl.LOCK_EX)
            try:
                writer = csv.DictWriter(f, fieldnames=self.fieldnames)
                row = self._prepare_csv_row(result)
                writer.writerow(row)
            finally:
                fcntl.flock(f.fileno(), fcntl.LOCK_UN)

        # å†™å…¥è¯¦æƒ…CSV
        with open(self.details_filepath, 'a', newline='', encoding='utf-8') as f:
            fcntl.flock(f.fileno(), fcntl.LOCK_EX)
            try:
                writer = csv.DictWriter(f, fieldnames=self.fieldnames)
                writer.writerow(row)
            finally:
                fcntl.flock(f.fileno(), fcntl.LOCK_UN)

        # ä¿å­˜å•å®éªŒJSONæ–‡ä»¶
        self._save_single_experiment_file(result)

    def _prepare_csv_row(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """å‡†å¤‡CSVè¡Œæ•°æ®

        Args:
            result: å®éªŒç»“æœå­—å…¸

        Returns:
            CSVè¡Œå­—å…¸
        """
        row = {}
        params = result.get("params", {})

        for field in self.fieldnames:
            if field in result:
                row[field] = result[field]
            elif field in params:
                row[field] = params[field]
            else:
                row[field] = ""

        return row

    def _save_single_experiment_file(self, result: Dict[str, Any]) -> None:
        """ä¿å­˜å•ä¸ªå®éªŒçš„JSONæ–‡ä»¶

        Args:
            result: å®éªŒç»“æœå­—å…¸
        """
        exp_name = result.get('exp_name', 'unknown')
        exp_dir = os.path.join(self.experiments_dir, exp_name)
        os.makedirs(exp_dir, exist_ok=True)

        # ä¿å­˜å®Œæ•´ç»“æœ
        result_file = os.path.join(exp_dir, 'result.json')
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)


# ======================
# è¾…åŠ©å‡½æ•°
# ======================

def generate_combinations(config: Dict[str, Any]) -> List[Dict[str, Any]]:
    """ç”Ÿæˆå‚æ•°ç»„åˆï¼ˆä¾¿æ·å‡½æ•°ï¼‰

    Args:
        config: é…ç½®å­—å…¸

    Returns:
        å‚æ•°ç»„åˆåˆ—è¡¨
    """
    generator = ParameterCombinationGenerator(config)
    return generator.generate_combinations()


def get_csv_fieldnames(all_params: List[Dict[str, Any]]) -> List[str]:
    """è·å–CSVå­—æ®µå

    Args:
        all_params: æ‰€æœ‰å‚æ•°ç»„åˆåˆ—è¡¨

    Returns:
        å­—æ®µååˆ—è¡¨
    """
    base_columns = GRID_SEARCH_CONSTANTS['csv_base_columns']
    common_runtime_params = GRID_SEARCH_CONSTANTS['common_runtime_params']
    excluded_csv_params = GRID_SEARCH_CONSTANTS['excluded_csv_params']

    # æ”¶é›†æ‰€æœ‰å‚æ•°é”®
    param_keys = set()
    for params in all_params:
        param_keys.update(params.keys())

    # è¿‡æ»¤æ‰æ’é™¤çš„å‚æ•°
    param_keys = [k for k in param_keys if k not in excluded_csv_params]

    # ç»„åˆå­—æ®µåï¼šåŸºç¡€åˆ— + è¿è¡Œæ—¶å‚æ•° + å…¶ä»–å‚æ•°
    fieldnames = base_columns.copy()

    # æ·»åŠ å¸¸è§è¿è¡Œæ—¶å‚æ•°
    for param in common_runtime_params:
        if param not in fieldnames:
            fieldnames.append(param)

    # æ·»åŠ å…¶ä»–å‚æ•°
    for key in sorted(param_keys):
        if key not in fieldnames and key != GRID_SEARCH_CONSTANTS['group_key']:
            fieldnames.append(key)

    return fieldnames


def load_grid_config(config_path: str) -> Dict[str, Any]:
    """åŠ è½½ç½‘æ ¼æœç´¢é…ç½®æ–‡ä»¶

    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„

    Returns:
        é…ç½®å­—å…¸
    """
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


def apply_param_overrides(config: Dict[str, Any], params: Dict[str, Any]) -> Dict[str, Any]:
    """åº”ç”¨å‚æ•°è¦†ç›–åˆ°é…ç½®

    Args:
        config: åŸºç¡€é…ç½®å­—å…¸
        params: å‚æ•°è¦†ç›–å­—å…¸

    Returns:
        æ›´æ–°åçš„é…ç½®å­—å…¸
    """
    for key, value in params.items():
        if '.' in key:
            # å¤„ç†åµŒå¥—å‚æ•°ï¼ˆå¦‚ model.type, hp.batch_sizeï¼‰
            parts = key.split('.')
            current = config
            for part in parts[:-1]:
                if part not in current:
                    current[part] = {}
                current = current[part]
            current[parts[-1]] = value
        else:
            # ç›´æ¥å‚æ•°
            config[key] = value

    return config


# ======================
# æ ¸å¿ƒå®éªŒæ‰§è¡Œå‡½æ•°ï¼ˆç»Ÿä¸€ç‰ˆæœ¬ï¼‰
# ======================

def run_single_experiment(params: Dict[str, Any], exp_id: str, config_path: str,
                         grid_search_dir: Optional[str] = None) -> Dict[str, Any]:
    """è¿è¡Œå•ä¸ªå®éªŒï¼ˆç»Ÿä¸€ç‰ˆæœ¬ - è‡ªåŠ¨é€‚é…å•å¡/å¤šå¡ï¼‰

    Args:
        params: å®éªŒå‚æ•°è¦†ç›–å­—å…¸
        exp_id: å®éªŒIDï¼ˆå¦‚ "001", "002"ï¼‰
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        grid_search_dir: ç½‘æ ¼æœç´¢ç›®å½•ï¼ˆç”¨äºä¿å­˜è§†é¢‘çº§åˆ«æŒ‡æ ‡ç­‰æ–‡ä»¶ï¼‰

    Returns:
        å®éªŒç»“æœå­—å…¸ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
        - success: æ˜¯å¦æˆåŠŸ
        - exp_name: å®éªŒåç§°
        - params: å®éªŒå‚æ•°
        - best_accuracy: æœ€ä½³å‡†ç¡®ç‡
        - final_accuracy: æœ€ç»ˆå‡†ç¡®ç‡
        - trained_epochs: è®­ç»ƒè½®æ•°
        - error: é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰
        - error_type: é”™è¯¯ç±»å‹ï¼ˆå¦‚æœå¤±è´¥ï¼‰
    """
    exp_name = f"grid_{exp_id}"

    try:
        # å¯¼å…¥è®­ç»ƒå‡½æ•°å’ŒGPUé…ç½®å‡½æ•°
        from src.trainers.base_trainer import run_training
        from src.utils.config_parser import setup_gpu_config

        # åŠ è½½åŸºç¡€é…ç½®
        config = load_grid_config(config_path)

        # åº”ç”¨å‚æ•°è¦†ç›–
        config = apply_param_overrides(config, params)

        # å°† grid_search_dir æ·»åŠ åˆ°é…ç½®ä¸­
        if grid_search_dir:
            config['grid_search_dir'] = grid_search_dir

        # é…ç½®GPUç¯å¢ƒ
        setup_gpu_config(config)

        # ç›´æ¥è°ƒç”¨è®­ç»ƒå‡½æ•°ï¼ˆAccelerator ä¼šè‡ªåŠ¨å¤„ç†å•å¡/å¤šå¡ï¼‰
        result = run_training(config, exp_name)

        # æ·»åŠ å‚æ•°ä¿¡æ¯åˆ°ç»“æœä¸­
        result["params"] = params

        return result

    except Exception as e:
        print(f"âŒ å®éªŒ {exp_name} å¤±è´¥: {type(e).__name__}: {str(e)}")
        return {
            "success": False,
            "exp_name": exp_name,
            "params": params,
            "best_accuracy": 0.0,
            "final_accuracy": 0.0,
            "trained_epochs": 0,
            "error": str(e),
            "error_type": type(e).__name__
        }


# ======================
# ä¸»ç½‘æ ¼æœç´¢å‡½æ•°
# ======================

def run_grid_search(args):
    """è¿è¡Œç½‘æ ¼æœç´¢ï¼ˆä¸²è¡Œæ‰§è¡Œæ‰€æœ‰å®éªŒï¼‰

    Args:
        args: å‘½ä»¤è¡Œå‚æ•°

    Returns:
        é€€å‡ºç ï¼ˆ0è¡¨ç¤ºæˆåŠŸï¼Œ1è¡¨ç¤ºå¤±è´¥ï¼‰
    """
    # åŠ è½½é…ç½®å¹¶ç”Ÿæˆå‚æ•°ç»„åˆ
    config = load_grid_config(args.config)
    combinations = generate_combinations(config)

    if len(combinations) > args.max_experiments:
        combinations = combinations[:args.max_experiments]

    # å‡†å¤‡CSVæ–‡ä»¶ - æ ¹æ®ä»»åŠ¡ç±»å‹åˆ›å»ºå¯¹åº”ç›®å½•
    task_tag = config.get('task', {}).get('tag', '')
    dataset_type = config.get('data', {}).get('type', '')

    # æ ¹æ®ä»»åŠ¡ç±»å‹ç¡®å®šå­ç›®å½•å
    if 'multilabel' in task_tag.lower() or 'multilabel' in dataset_type.lower():
        if 'neonatal' in dataset_type.lower():
            task_subdir = "neonatal_multilabel"
        else:
            task_subdir = "multilabel_classification"
    elif 'video' in task_tag.lower():
        task_subdir = "video_classification"
    elif 'image' in task_tag.lower():
        task_subdir = "image_classification"
    elif 'text' in task_tag.lower():
        task_subdir = "text_classification"
    else:
        task_subdir = dataset_type.replace('_', '_').lower() or "general"

    results_dir = os.path.join("runs", task_subdir)

    # åˆ›å»ºå¢å¼ºçš„ç½‘æ ¼æœç´¢æ–‡ä»¶å¤¹ç»“æ„
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    grid_search_dir = os.path.join(results_dir, f"grid_search_{timestamp}")

    # ä¸»ç»“æœæ–‡ä»¶è·¯å¾„
    csv_filepath = os.path.join(grid_search_dir, "grid_search_results.csv")
    details_filepath = os.path.join(grid_search_dir, "grid_search_details.csv")

    # è·å–CSVå­—æ®µå
    all_params = [params for params in combinations]
    fieldnames = get_csv_fieldnames(all_params)

    # åˆ›å»ºå¢å¼ºçš„ç»“æœç®¡ç†å™¨
    results_manager = ExperimentResultsManager(
        csv_filepath=csv_filepath,
        details_filepath=details_filepath,
        grid_search_dir=grid_search_dir
    )

    # åˆå§‹åŒ–CSVæ–‡ä»¶
    if args.save_results:
        os.makedirs(grid_search_dir, exist_ok=True)
        results_manager.initialize_csv_file(fieldnames)
    else:
        results_manager.initialize_csv_file(fieldnames)

    print(f"ğŸš€ å¼€å§‹ç½‘æ ¼æœç´¢ï¼Œå…± {len(combinations)} ä¸ªå®éªŒ")
    print(f"ğŸ“Š ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
    print(f"ğŸ“ ç½‘æ ¼æœç´¢ç›®å½•: {grid_search_dir}")
    print(f"ğŸ’¾ ä¸»ç»“æœæ–‡ä»¶: {csv_filepath}")
    print(f"ğŸ“‹ è¯¦æƒ…è¡¨æ–‡ä»¶: {details_filepath}")

    # å¤„ç†data_percentageå‚æ•°
    data_percentage = args.data_percentage if args.data_percentage is not None else 1.0

    if args.data_percentage is not None:
        print(f"ğŸ¯ å…¨å±€å‚æ•°è¦†ç›–: data_percentage={args.data_percentage}")
    else:
        print(f"ğŸ¯ ä½¿ç”¨é»˜è®¤data_percentage: {data_percentage}")

    print("=" * 60)

    # ä¸²è¡Œæ‰§è¡Œæ‰€æœ‰å®éªŒ
    results = []
    successful = 0

    for i, params in enumerate(combinations, 1):
        # å°†å‘½ä»¤è¡Œå‚æ•°æ·»åŠ åˆ°å®éªŒå‚æ•°ä¸­
        experiment_params = params.copy()
        experiment_params['hp.data_percentage'] = data_percentage
        experiment_params['data_percentage'] = data_percentage

        # è¿è¡Œå•ä¸ªå®éªŒï¼ˆç»Ÿä¸€ç‰ˆæœ¬ - è‡ªåŠ¨é€‚é…å•å¡/å¤šå¡ï¼‰
        result = run_single_experiment(
            experiment_params,
            f"{i:03d}",
            args.config,
            grid_search_dir
        )

        results.append(result)
        if result["success"]:
            successful += 1

        # å®æ—¶å†™å…¥CSV
        if args.save_results:
            print(f"ğŸ’¾ å†™å…¥å®éªŒç»“æœåˆ°CSV: {result.get('exp_name', 'unknown')}")
            results_manager.append_result_to_csv(result)

        # å®æ—¶æ˜¾ç¤ºæœ€ä½³ç»“æœ
        if successful > 0:
            current_best = max([r for r in results if r["success"]], key=lambda x: x["best_accuracy"])
            print(f"ğŸ† å½“å‰æœ€ä½³: {current_best['exp_name']} - {current_best['best_accuracy']:.2f}%")

    # æ€»ç»“
    print("=" * 60)
    print(f"ğŸ“ˆ ç½‘æ ¼æœç´¢å®Œæˆï¼")
    print(f"âœ… æˆåŠŸå®éªŒæ•°é‡: {successful}/{len(combinations)}")

    if successful > 0:
        successful_results = [r for r in results if r["success"]]
        best_result = max(successful_results, key=lambda x: x["best_accuracy"])

        print(f"ğŸ† æœ€ä½³å®éªŒç»“æœ:")
        print(f"å®éªŒåç§°: {best_result['exp_name']}, æœ€ä½³å‡†ç¡®ç‡: {best_result['best_accuracy']:.2f}%, "
              f"æœ€ç»ˆå‡†ç¡®ç‡: {best_result['final_accuracy']:.2f}%")

        # æŒ‰æœ€ä½³ç²¾åº¦æ’åºå‰nç»„ç»“æœ
        top_results = sorted(successful_results, key=lambda x: x["best_accuracy"], reverse=True)[:args.top_n]

        print(f"ğŸ“Š å‰{args.top_n}åå®éªŒç»“æœ:")
        for i, r in enumerate(top_results, 1):
            print(f"{i}. {r['exp_name']} - {r['best_accuracy']:.2f}% - {r['params']}")

    if args.save_results:
        print(f"ğŸ’¾ ä¸»ç»“æœå·²å®æ—¶ä¿å­˜åˆ°: {csv_filepath}")
        print(f"ğŸ“‹ è¯¦æƒ…è¡¨å·²å®æ—¶ä¿å­˜åˆ°: {details_filepath}")
        print(f"ğŸ“ å•å®éªŒæ–‡ä»¶å·²ä¿å­˜åˆ°: {results_manager.experiments_dir}")

    return 0 if successful > 0 else 1


# ======================
# ä¸»å‡½æ•°
# ======================

def main():
    """ä¸»å‡½æ•°

    å¯åŠ¨æ–¹å¼ï¼š
    - å•å¡æ¨¡å¼ï¼špython scripts/grid_search_unified.py --config ...
    - å¤šå¡æ¨¡å¼ï¼šaccelerate launch scripts/grid_search_unified.py --config ...

    Accelerator ä¼šè‡ªåŠ¨æ£€æµ‹å¯åŠ¨æ–¹å¼å¹¶é€‚é…å•å¡/å¤šå¡æ¨¡å¼ã€‚
    """
    args, _ = parse_arguments(mode="grid_search")
    return run_grid_search(args)


if __name__ == "__main__":
    sys.exit(main())

