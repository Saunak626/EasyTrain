"""åŸºç¡€è®­ç»ƒå™¨æ¨¡å—

è¿™ä¸ªæ¨¡å—æä¾›äº†æ·±åº¦å­¦ä¹ è®­ç»ƒçš„æ ¸å¿ƒåŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- train_epoch: å•ä¸ªè®­ç»ƒè½®æ¬¡çš„æ‰§è¡Œ
- test_epoch: å•ä¸ªæµ‹è¯•è½®æ¬¡çš„æ‰§è¡Œ  
- run_training: å®Œæ•´è®­ç»ƒæµç¨‹çš„ä¸»å‡½æ•°

æ”¯æŒå¤šGPUè®­ç»ƒã€å®éªŒè¿½è¸ªå’Œå„ç§æ•°æ®é›†ç±»å‹
"""

import torch
import torch.nn as nn
from accelerate import Accelerator
from tqdm import tqdm
import sys
import os
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„ï¼Œç¡®ä¿å¯ä»¥æ­£ç¡®å¯¼å…¥é¡¹ç›®å†…çš„æ¨¡å—
# è¿™æ˜¯ä¸€ç§å¸¸è§çš„åšæ³•ï¼Œç”¨äºè§£å†³Pythonæ¨¡å—å¯¼å…¥è·¯å¾„é—®é¢˜
# é€šè¿‡os.path.dirnameçš„ä¸‰å±‚åµŒå¥—è°ƒç”¨ï¼Œè·å–åˆ°é¡¹ç›®æ ¹ç›®å½•
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

# å¯¼å…¥é¡¹ç›®å†…éƒ¨æ¨¡å—
from src.models.image_net import get_model                     # å›¾åƒæ¨¡å‹å·¥å‚å‡½æ•°
from src.models.video_net import get_video_model               # è§†é¢‘æ¨¡å‹å·¥å‚å‡½æ•°
from src.losses.image_loss import get_loss_function            # æŸå¤±å‡½æ•°å·¥å‚å‡½æ•°
from src.optimizers.optim import get_optimizer                 # ä¼˜åŒ–å™¨å·¥å‚å‡½æ•°
from src.schedules.scheduler import get_scheduler              # å­¦ä¹ ç‡è°ƒåº¦å™¨å·¥å‚å‡½æ•°
from src.datasets import create_dataloaders, get_dataset_info  # ç»Ÿä¸€æ•°æ®åŠ è½½å™¨å·¥å‚
from src.utils.data_utils import set_seed                      # éšæœºç§å­è®¾ç½®å·¥å…·


def write_epoch_metrics(result_dir, epoch_data, accelerator):
    """å†™å…¥epochçº§åˆ«çš„æŒ‡æ ‡æ•°æ®åˆ°JSONLæ–‡ä»¶"""
    if not accelerator.is_main_process:
        return

    os.makedirs(result_dir, exist_ok=True)
    jsonl_path = os.path.join(result_dir, "metrics.jsonl")

    with open(jsonl_path, "a", encoding="utf-8") as f:
        f.write(json.dumps(epoch_data, ensure_ascii=False) + "\n")


def write_final_result(result_dir, result_data, accelerator):
    """å†™å…¥æœ€ç»ˆè®­ç»ƒç»“æœåˆ°JSONæ–‡ä»¶"""
    if not accelerator.is_main_process:
        return

    os.makedirs(result_dir, exist_ok=True)
    final_path = os.path.join(result_dir, "result.json")

    with open(final_path, "w", encoding="utf-8") as f:
        json.dump(result_data, f, ensure_ascii=False, indent=2)


def train_epoch(dataloader, model, loss_fn, optimizer, lr_scheduler, accelerator, epoch):
    """
    æ‰§è¡Œå•ä¸ªè®­ç»ƒè½®æ¬¡

    è¯¥å‡½æ•°è´Ÿè´£ä¸€ä¸ªå®Œæ•´epochçš„è®­ç»ƒè¿‡ç¨‹ï¼ŒåŒ…æ‹¬å‰å‘ä¼ æ’­ã€æŸå¤±è®¡ç®—ã€
    åå‘ä¼ æ’­ã€å‚æ•°æ›´æ–°å’Œå­¦ä¹ ç‡è°ƒæ•´ã€‚æ”¯æŒå¤šGPUåˆ†å¸ƒå¼è®­ç»ƒã€‚

    Args:
        dataloader (torch.utils.data.DataLoader): è®­ç»ƒæ•°æ®åŠ è½½å™¨ï¼Œæä¾›æ‰¹æ¬¡æ•°æ®
        model (torch.nn.Module): ç¥ç»ç½‘ç»œæ¨¡å‹
        loss_fn (torch.nn.Module): æŸå¤±å‡½æ•°ï¼Œç”¨äºè®¡ç®—é¢„æµ‹ä¸çœŸå®æ ‡ç­¾çš„å·®å¼‚
        optimizer (torch.optim.Optimizer): ä¼˜åŒ–å™¨ï¼Œç”¨äºæ›´æ–°æ¨¡å‹å‚æ•°
        lr_scheduler (torch.optim.lr_scheduler._LRScheduler): å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ŒåŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡
        accelerator (accelerate.Accelerator): Acceleratorå®ä¾‹ï¼Œå¤„ç†å¤šGPUå’Œæ··åˆç²¾åº¦è®­ç»ƒ
        epoch (int): å½“å‰è®­ç»ƒè½®æ¬¡ç¼–å·

    Returns:
        float: å¹³å‡è®­ç»ƒæŸå¤±
    """
    # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼ï¼Œå¯ç”¨dropoutå’Œbatch normalizationçš„è®­ç»ƒè¡Œä¸º
    model.train()

    # åˆå§‹åŒ–è®­ç»ƒæŒ‡æ ‡
    total_loss = 0.0
    num_batches = 0

    # åªåœ¨ä¸»è¿›ç¨‹æ˜¾ç¤ºè¿›åº¦æ¡ï¼Œé¿å…å¤šGPUæ—¶é‡å¤æ˜¾ç¤º
    if accelerator.is_main_process:
        progress_bar = tqdm(
            total=len(dataloader),
            desc=f"Epoch {epoch} Training",
            unit="batch",
            dynamic_ncols=True,
            leave=False,
        )

    # éå†è®­ç»ƒæ•°æ®çš„æ¯ä¸ªæ‰¹æ¬¡
    for batch_idx, (inputs, targets) in enumerate(dataloader):
        # å‰å‘ä¼ æ’­ï¼šå°†è¾“å…¥æ•°æ®é€šè¿‡æ¨¡å‹å¾—åˆ°é¢„æµ‹ç»“æœ
        outputs = model(inputs)
        # è®¡ç®—æŸå¤±ï¼šæ¯”è¾ƒé¢„æµ‹ç»“æœä¸çœŸå®æ ‡ç­¾
        loss = loss_fn(outputs, targets)

        # åå‘ä¼ æ’­ï¼šä½¿ç”¨acceleratorå¤„ç†æ¢¯åº¦è®¡ç®—ï¼Œæ”¯æŒæ··åˆç²¾åº¦å’Œå¤šGPU
        accelerator.backward(loss)
        # æ›´æ–°æ¨¡å‹å‚æ•°ï¼šæ ¹æ®è®¡ç®—çš„æ¢¯åº¦è°ƒæ•´æƒé‡
        optimizer.step()
        # æ¸…é›¶æ¢¯åº¦ï¼šä¸ºä¸‹ä¸€æ¬¡è¿­ä»£å‡†å¤‡
        optimizer.zero_grad()
        # æ›´æ–°å­¦ä¹ ç‡ï¼šæ ¹æ®è°ƒåº¦ç­–ç•¥è°ƒæ•´å­¦ä¹ ç‡
        lr_scheduler.step()

        # ç´¯è®¡æŸå¤±ç»Ÿè®¡
        total_loss += loss.item()
        num_batches += 1

        # è®°å½•è®­ç»ƒæŒ‡æ ‡åˆ°å®éªŒè¿½è¸ªç³»ç»Ÿï¼ˆå¦‚SwanLabï¼‰
        accelerator.log({"train/loss": loss.item(), "epoch_num": epoch})

        # å®šæœŸæ›´æ–°è¿›åº¦æ¡æ˜¾ç¤ºï¼Œé¿å…è¿‡äºé¢‘ç¹çš„æ›´æ–°å½±å“æ€§èƒ½
        if accelerator.is_main_process and batch_idx % 10 == 0:
            current_lr = optimizer.param_groups[0]['lr']
            avg_loss = total_loss / num_batches
            progress_bar.set_postfix(
                loss=f"{avg_loss:.4f}",
                lr=f"{current_lr:.2e}"
            )

        if accelerator.is_main_process:
            progress_bar.update(1)

    # å…³é—­è¿›åº¦æ¡
    if accelerator.is_main_process:
        progress_bar.close()

    # è¿”å›å¹³å‡è®­ç»ƒæŸå¤±
    avg_train_loss = total_loss / num_batches if num_batches > 0 else 0.0
    return avg_train_loss


def test_epoch(dataloader, model, loss_fn, accelerator, epoch):
    """
    æ‰§è¡Œå•ä¸ªæµ‹è¯•è½®æ¬¡
    
    è¯¥å‡½æ•°åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œè®¡ç®—å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡ã€‚
    æ”¯æŒå¤šGPUç¯å¢ƒä¸‹çš„æŒ‡æ ‡èšåˆï¼Œç¡®ä¿ç»“æœçš„å‡†ç¡®æ€§ã€‚

    Args:
        dataloader (torch.utils.data.DataLoader): æµ‹è¯•æ•°æ®åŠ è½½å™¨ï¼Œæä¾›æµ‹è¯•æ‰¹æ¬¡æ•°æ®
        model (torch.nn.Module): ç¥ç»ç½‘ç»œæ¨¡å‹
        loss_fn (torch.nn.Module): æŸå¤±å‡½æ•°ï¼Œç”¨äºè®¡ç®—æµ‹è¯•æŸå¤±
        accelerator (accelerate.Accelerator): Acceleratorå®ä¾‹ï¼Œå¤„ç†å¤šGPUæŒ‡æ ‡èšåˆ
        epoch (int): å½“å‰æµ‹è¯•è½®æ¬¡ç¼–å·

    Returns:
        tuple: (å¹³å‡æŸå¤±, å‡†ç¡®ç‡ç™¾åˆ†æ¯”) æˆ– (None, None) å¦‚æœä¸æ˜¯ä¸»è¿›ç¨‹
    """
    # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œç¦ç”¨dropoutå’Œbatch normalizationçš„è®­ç»ƒè¡Œä¸º
    model.eval()
    device = accelerator.device

    # åˆå§‹åŒ–ç´¯è®¡æŒ‡æ ‡å¼ é‡ï¼Œç”¨äºè·¨GPUèšåˆ
    local_loss_sum = torch.tensor(0.0, device=device)  # å½“å‰GPUçš„æ€»æŸå¤±
    local_correct = torch.tensor(0, device=device)     # å½“å‰GPUçš„æ­£ç¡®é¢„æµ‹æ•°
    local_samples = torch.tensor(0, device=device)     # å½“å‰GPUçš„æ ·æœ¬æ€»æ•°

    # åªåœ¨ä¸»è¿›ç¨‹æ˜¾ç¤ºè¿›åº¦æ¡
    if accelerator.is_main_process:
        progress_bar = tqdm(
            total=len(dataloader),
            desc=f"Epoch {epoch} Testing",
            unit="batch",
            dynamic_ncols=True,
            leave=False,
        )

    # ç¦ç”¨æ¢¯åº¦è®¡ç®—ä»¥èŠ‚çœå†…å­˜å’ŒåŠ é€Ÿæ¨ç†
    with torch.no_grad():
        for inputs, targets in dataloader:
            # å‰å‘ä¼ æ’­è·å–é¢„æµ‹ç»“æœ
            outputs = model(inputs)
            # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„æŸå¤±
            loss = loss_fn(outputs, targets)

            # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„ç»Ÿè®¡ä¿¡æ¯
            batch_size = targets.size(0)
            # è·å–é¢„æµ‹ç±»åˆ«å¹¶è®¡ç®—æ­£ç¡®é¢„æµ‹æ•°é‡
            correct = outputs.argmax(dim=1).eq(targets).sum()

            # ç´¯åŠ åˆ°æœ¬åœ°ç»Ÿè®¡é‡ï¼ˆè€ƒè™‘æ‰¹æ¬¡å¤§å°æƒé‡ï¼‰
            local_loss_sum += loss * batch_size
            local_correct += correct
            local_samples += batch_size

            # æ›´æ–°è¿›åº¦æ¡
            if accelerator.is_main_process:
                progress_bar.update(1)

    # å…³é—­è¿›åº¦æ¡
    if accelerator.is_main_process:
        progress_bar.close()

    # è·¨æ‰€æœ‰GPUè¿›ç¨‹èšåˆç»Ÿè®¡æŒ‡æ ‡
    total_loss = accelerator.reduce(local_loss_sum, reduction="sum")
    total_correct = accelerator.reduce(local_correct, reduction="sum")
    total_samples = accelerator.reduce(local_samples, reduction="sum")

    # åªåœ¨ä¸»è¿›ç¨‹è®¡ç®—æœ€ç»ˆæŒ‡æ ‡å¹¶è®°å½•
    if accelerator.is_main_process:
        # è®¡ç®—å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
        avg_loss = (total_loss / total_samples).item()
        accuracy = 100. * total_correct.item() / total_samples.item()

        # ä½¿ç”¨tqdm.write()è¾“å‡ºæ‘˜è¦ï¼Œä¸ç ´åè¿›åº¦æ¡æ˜¾ç¤º
        tqdm.write(f'Epoch {epoch:03d} | val_loss={avg_loss:.4f} | val_acc={accuracy:.2f}%')

        # è®°å½•æµ‹è¯•æŒ‡æ ‡åˆ°å®éªŒè¿½è¸ªç³»ç»Ÿ
        accelerator.log({"test/loss": avg_loss, "test/accuracy": accuracy}, step=epoch)
        return avg_loss, accuracy

    # éä¸»è¿›ç¨‹è¿”å›None
    return None, None


def run_training(config, experiment_name=None):
    """
    è®­ç»ƒçš„ä¸»å…¥å£å‡½æ•°ï¼Œè´Ÿè´£æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹çš„åè°ƒï¼ŒåŒ…æ‹¬ï¼š
    - ç¯å¢ƒåˆå§‹åŒ–ï¼ˆéšæœºç§å­ã€å®éªŒè¿½è¸ªï¼‰
    - æ•°æ®åŠ è½½å™¨åˆ›å»º
    - æ¨¡å‹ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨åˆå§‹åŒ–
    - å¤šGPUç¯å¢ƒé…ç½®
    - è®­ç»ƒå¾ªç¯æ‰§è¡Œ
    - ç»“æœè®°å½•å’Œè¿”å›

    Args:
        config (dict): åŒ…å«æ‰€æœ‰è®­ç»ƒé…ç½®çš„å­—å…¸ï¼ŒåŒ…æ‹¬æ¨¡å‹ã€æ•°æ®ã€è¶…å‚æ•°ç­‰è®¾ç½®
        experiment_name (str, optional): å®éªŒåç§°ï¼Œç”¨äºè¿½è¸ªå’Œæ—¥å¿—è®°å½•

    Returns:
        dict: è®­ç»ƒç»“æœå­—å…¸ï¼ŒåŒ…å«å®éªŒåç§°ã€æœ€ä½³å‡†ç¡®ç‡å’Œé…ç½®ä¿¡æ¯
    """
    # è®¾ç½®éšæœºç§å­ç¡®ä¿å®éªŒå¯é‡ç°æ€§
    set_seed(42)

    # ç¡®å®šå®éªŒåç§°ï¼Œä¼˜å…ˆä½¿ç”¨ä¼ å…¥å‚æ•°
    if experiment_name is None:
        experiment_name = config['training']['experiment_name']

    # åˆå§‹åŒ–Acceleratorï¼Œè‡ªåŠ¨å¤„ç†å¤šGPUå’Œæ··åˆç²¾åº¦è®­ç»ƒ
    accelerator = Accelerator(log_with="swanlab")

    # å‡†å¤‡å®éªŒè¿½è¸ªé…ç½®
    hyperparams = config['hyperparameters']
    tracker_config = {**hyperparams, "experiment_name": experiment_name}

    # åˆå§‹åŒ–SwanLabå®éªŒè¿½è¸ªå™¨
    accelerator.init_trackers(
        project_name=config['swanlab']['project_name'],
        config=tracker_config,
        init_kwargs={"swanlab": {
            "experiment_name": experiment_name,
            "description": config['swanlab']['description']
        }}
    )

    # è§£ææ•°æ®é…ç½®
    data_config = config.get('data', {})
    dataset_type = data_config.get('type', 'cifar10')

    # ä½¿ç”¨ç®€åŒ–çš„æ•°æ®åŠ è½½å™¨åˆ›å»ºå‡½æ•°
    train_dataloader, test_dataloader, num_classes = create_dataloaders(
        dataset_name=dataset_type,
        data_dir=data_config.get('root', './data'),
        batch_size=hyperparams['batch_size'],
        num_workers=data_config.get('num_workers', 4),
        **data_config.get('params', {})
    )
    
    # è·å–æ•°æ®é›†ä¿¡æ¯
    dataset_info = get_dataset_info(dataset_type)
    dataset_info['num_classes'] = num_classes or dataset_info['num_classes']

    # è§£ææ¨¡å‹é…ç½®
    model_config = config.get('model', {})
    model_name = model_config.get('type', model_config.get('name', 'resnet18'))

    # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©å¯¹åº”çš„å·¥å‚å‡½æ•°
    video_model_prefixes = ['r3d_', 'mc3_', 'r2plus1d_', 's3d']
    is_video_model = any(model_name.startswith(prefix) for prefix in video_model_prefixes)
    
    if is_video_model:
        # ä½¿ç”¨è§†é¢‘æ¨¡å‹å·¥å‚å‡½æ•°
        video_params = model_config.get('params', {}).copy()
        # ç¡®ä¿ä½¿ç”¨æ•°æ®é›†çš„å®é™…ç±»åˆ«æ•°
        video_params['num_classes'] = dataset_info['num_classes']
        model = get_video_model(
            model_name=model_name,
            **video_params
        )
    else:
        # ä½¿ç”¨å›¾åƒæ¨¡å‹å·¥å‚å‡½æ•°
        model = get_model(
            model_name=model_name,
            num_classes=dataset_info['num_classes'],
            **model_config.get('params', {})
        )

    # åˆ›å»ºæŸå¤±å‡½æ•°
    loss_config = config.get('loss', {})
    loss_fn = get_loss_function(
        loss_config.get('name', 'crossentropy'),
        **loss_config.get('params', {})
    )

    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer_config = config.get('optimizer', {})
    optimizer = get_optimizer(
        model,
        optimizer_config.get('name', 'adam'),
        hyperparams['learning_rate'],
        **optimizer_config.get('params', {})
    )

    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler_config = config.get('scheduler', {})
    lr_scheduler = get_scheduler(
        optimizer,
        scheduler_config.get('name', 'onecycle'),
        max_lr=5 * hyperparams['learning_rate'],
        epochs=hyperparams['epochs'],
        steps_per_epoch=len(train_dataloader),
        **scheduler_config.get('params', {})
    )

    # ä½¿ç”¨AcceleratoråŒ…è£…æ‰€æœ‰è®­ç»ƒç»„ä»¶ï¼Œè‡ªåŠ¨å¤„ç†å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒ
    try:
        # æ¸…ç†GPUç¼“å­˜ï¼Œé‡Šæ”¾æœªä½¿ç”¨çš„å†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        # ä½¿ç”¨AcceleratoråŒ…è£…è®­ç»ƒç»„ä»¶ï¼Œè‡ªåŠ¨å¤„ç†åˆ†å¸ƒå¼è®­ç»ƒ
        model, optimizer, lr_scheduler, train_dataloader, test_dataloader = accelerator.prepare(
            model, optimizer, lr_scheduler, train_dataloader, test_dataloader
        )

    except RuntimeError as e:
        # å¤„ç†å¸¸è§çš„GPUå†…å­˜ä¸è¶³é”™è¯¯
        if "out of memory" in str(e):
            print(f"âŒ GPUå†…å­˜ä¸è¶³: {e}")
            print("ğŸ’¡ å»ºè®®è§£å†³æ–¹æ¡ˆ:")
            print("  1. å‡å°‘batch_size")
            print("  2. ä½¿ç”¨æ›´å°çš„æ¨¡å‹")
            print("  3. ä½¿ç”¨CPUè®­ç»ƒ: --use_cpu")
            raise e
        else:
            raise e

    # æ‰“å°è®­ç»ƒé…ç½®ä¿¡æ¯ï¼ˆä»…åœ¨ä¸»è¿›ç¨‹ï¼‰
    if accelerator.is_main_process:
        print(f"\n=== è®­ç»ƒå®éªŒ: {experiment_name} ===")
        print(f"æ•°æ®é›†: {dataset_type}")
        print(f"æ¨¡å‹: {model_name}")
        print(f"å‚æ•°: {hyperparams}")
        print("=" * 50)

    # è®¾ç½®ç»“æœç›®å½•
    result_dir = os.path.join("runs", experiment_name) if experiment_name else None

    # åˆå§‹åŒ–æœ€ä½³å‡†ç¡®ç‡è¿½è¸ª
    best_accuracy = 0.0

    # ä¸»è®­ç»ƒå¾ªç¯ï¼šæ‰§è¡ŒæŒ‡å®šè½®æ•°çš„è®­ç»ƒ
    for epoch in range(1, hyperparams['epochs'] + 1):
        if accelerator.is_main_process:
            tqdm.write(f"\nEpoch {epoch}/{hyperparams['epochs']}")

        # æ‰§è¡Œä¸€è½®è®­ç»ƒå’Œæµ‹è¯•
        train_loss = train_epoch(train_dataloader, model, loss_fn, optimizer, lr_scheduler, accelerator, epoch)
        
        val_loss, val_accuracy = test_epoch(test_dataloader, model, loss_fn, accelerator, epoch)

        # æ›´æ–°å¹¶è®°å½•æœ€ä½³å‡†ç¡®ç‡
        if accelerator.is_main_process and val_accuracy and val_accuracy > best_accuracy:
            best_accuracy = val_accuracy
            tqdm.write(f"æ–°æœ€ä½³å‡†ç¡®ç‡: {best_accuracy:.2f}%")

        # å†™å…¥epochçº§åˆ«çš„ç»“æ„åŒ–æ•°æ®
        if accelerator.is_main_process and result_dir and val_accuracy is not None:
            epoch_data = {
                "event": "epoch_end",
                "epoch": epoch,
                "train_loss": train_loss,
                "val_loss": val_loss,
                "val_acc": val_accuracy,
                "best_acc": best_accuracy,
                "timestamp": datetime.now().isoformat()
            }
            write_epoch_metrics(result_dir, epoch_data, accelerator)

    # ç»“æŸå®éªŒè¿½è¸ªï¼Œä¿å­˜æ—¥å¿—å’Œç»“æœ
    accelerator.end_training()

    # å†™å…¥æœ€ç»ˆç»“æœ
    if accelerator.is_main_process:
        tqdm.write(f"\nè®­ç»ƒå®Œæˆ! æœ€ä½³å‡†ç¡®ç‡: {best_accuracy:.2f}%")

        # è¾“å‡ºæœºå™¨å¯è¯»çš„ç»“æœè¡Œ
        result_json = {"best_accuracy": best_accuracy, "final_accuracy": best_accuracy}
        print("##RESULT## " + json.dumps(result_json))

        # å†™å…¥æœ€ç»ˆç»“æœæ–‡ä»¶
        if result_dir:
            final_result = {
                "experiment_name": experiment_name,
                "best_accuracy": best_accuracy,
                "final_accuracy": best_accuracy,
                "total_epochs": hyperparams['epochs'],
                "config": tracker_config,
                "timestamp": datetime.now().isoformat()
            }
            write_final_result(result_dir, final_result, accelerator)

    # è¿”å›è®­ç»ƒç»“æœæ‘˜è¦
    return {
        "experiment_name": experiment_name,    # å®éªŒåç§°
        "best_accuracy": best_accuracy,        # æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡
        "config": tracker_config               # å®Œæ•´çš„è®­ç»ƒé…ç½®
    }