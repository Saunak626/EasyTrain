#!/usr/bin/env python3
"""
Focal Lossè¿‡åº¦é¢„æµ‹é—®é¢˜è¯Šæ–­è„šæœ¬

åˆ†æFocal Lossåœ¨å¤šæ ‡ç­¾åˆ†ç±»ä¸­çš„è¡¨ç°ï¼Œè¯†åˆ«è¿‡åº¦é¢„æµ‹é—®é¢˜å¹¶æä¾›ä¿®å¤å»ºè®®ã€‚
"""

import sys
import os
import argparse
import json
import pandas as pd
import numpy as np
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def analyze_metrics_file(metrics_file):
    """åˆ†æå•ä¸ªæŒ‡æ ‡æ–‡ä»¶"""
    try:
        with open(metrics_file, 'r', encoding='utf-8') as f:
            metrics = json.load(f)
        
        epoch = metrics.get('epoch', 0)
        class_metrics = metrics.get('class_metrics', {})
        
        # åˆ†ææ¯ä¸ªç±»åˆ«çš„ç²¾ç¡®ç‡å’Œå¬å›ç‡
        analysis = {
            'epoch': epoch,
            'classes': {},
            'overall': {}
        }
        
        precisions = []
        recalls = []
        f1_scores = []
        
        for class_name, class_data in class_metrics.items():
            precision = class_data.get('precision', 0)
            recall = class_data.get('recall', 0)
            f1 = class_data.get('f1', 0)
            
            precisions.append(precision)
            recalls.append(recall)
            f1_scores.append(f1)
            
            analysis['classes'][class_name] = {
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'imbalance_ratio': recall / max(precision, 0.001)  # å¬å›ç‡/ç²¾ç¡®ç‡æ¯”ä¾‹
            }
        
        # æ•´ä½“åˆ†æ
        analysis['overall'] = {
            'avg_precision': np.mean(precisions),
            'avg_recall': np.mean(recalls),
            'avg_f1': np.mean(f1_scores),
            'precision_recall_ratio': np.mean(recalls) / max(np.mean(precisions), 0.001),
            'min_precision': np.min(precisions),
            'max_recall': np.max(recalls)
        }
        
        return analysis
        
    except Exception as e:
        print(f"âŒ åˆ†ææ–‡ä»¶ {metrics_file} å¤±è´¥: {e}")
        return None

def diagnose_overprediction(analysis):
    """è¯Šæ–­è¿‡åº¦é¢„æµ‹é—®é¢˜"""
    overall = analysis['overall']
    classes = analysis['classes']
    
    # è¿‡åº¦é¢„æµ‹çš„åˆ¤æ–­æ ‡å‡†
    issues = []
    severity = "æ­£å¸¸"
    
    # 1. ç²¾ç¡®ç‡vså¬å›ç‡å¤±è¡¡
    pr_ratio = overall['precision_recall_ratio']
    if pr_ratio > 2.0:
        issues.append(f"ä¸¥é‡çš„ç²¾ç¡®ç‡-å¬å›ç‡å¤±è¡¡ (æ¯”ä¾‹: {pr_ratio:.2f})")
        severity = "ä¸¥é‡"
    elif pr_ratio > 1.5:
        issues.append(f"ä¸­ç­‰çš„ç²¾ç¡®ç‡-å¬å›ç‡å¤±è¡¡ (æ¯”ä¾‹: {pr_ratio:.2f})")
        severity = "ä¸­ç­‰" if severity == "æ­£å¸¸" else severity
    
    # 2. æ•´ä½“ç²¾ç¡®ç‡è¿‡ä½
    if overall['avg_precision'] < 0.3:
        issues.append(f"æ•´ä½“ç²¾ç¡®ç‡è¿‡ä½ ({overall['avg_precision']:.3f})")
        severity = "ä¸¥é‡"
    elif overall['avg_precision'] < 0.5:
        issues.append(f"æ•´ä½“ç²¾ç¡®ç‡åä½ ({overall['avg_precision']:.3f})")
        severity = "ä¸­ç­‰" if severity == "æ­£å¸¸" else severity
    
    # 3. æœ€ä½ç²¾ç¡®ç‡è¿‡ä½
    if overall['min_precision'] < 0.1:
        issues.append(f"å­˜åœ¨æä½ç²¾ç¡®ç‡ç±»åˆ« ({overall['min_precision']:.3f})")
        severity = "ä¸¥é‡"
    
    # 4. å¬å›ç‡è¿‡é«˜
    if overall['max_recall'] > 0.95:
        issues.append(f"å­˜åœ¨è¿‡é«˜å¬å›ç‡ç±»åˆ« ({overall['max_recall']:.3f})")
        severity = "ä¸­ç­‰" if severity == "æ­£å¸¸" else severity
    
    # 5. ç±»åˆ«çº§åˆ«åˆ†æ
    problematic_classes = []
    for class_name, class_data in classes.items():
        if class_data['imbalance_ratio'] > 3.0:
            problematic_classes.append(f"{class_name} (æ¯”ä¾‹: {class_data['imbalance_ratio']:.2f})")
    
    if problematic_classes:
        issues.append(f"é—®é¢˜ç±»åˆ«: {', '.join(problematic_classes)}")
    
    return {
        'severity': severity,
        'issues': issues,
        'metrics': overall
    }

def generate_recommendations(diagnosis, current_config=None):
    """ç”Ÿæˆä¿®å¤å»ºè®®"""
    severity = diagnosis['severity']
    recommendations = []
    
    if severity == "ä¸¥é‡":
        recommendations.extend([
            "ğŸš¨ ç«‹å³åœæ­¢å½“å‰è®­ç»ƒï¼Œé—®é¢˜ä¸¥é‡",
            "ğŸ“‰ ä½¿ç”¨ä¿å®ˆå‚æ•°é‡æ–°å¼€å§‹ï¼šgamma=1.0, alpha=1.0, pos_weight=3.0",
            "ğŸ”„ è€ƒè™‘åˆ‡æ¢åˆ° focal_multilabel_balanced æŸå¤±å‡½æ•°",
            "ğŸ“š é™ä½å­¦ä¹ ç‡åˆ° 0.0005",
            "â±ï¸ å¢åŠ è®­ç»ƒepochæ•°ï¼Œç»™æ¨¡å‹æ›´å¤šæ—¶é—´æ”¶æ•›"
        ])
    elif severity == "ä¸­ç­‰":
        recommendations.extend([
            "âš ï¸ éœ€è¦è°ƒæ•´å‚æ•°ï¼Œä½†å¯ä»¥ç»§ç»­è§‚å¯Ÿ",
            "ğŸ“Š é™ä½ gamma å‚æ•° (å½“å‰å€¼ â†’ å½“å‰å€¼*0.7)",
            "âš–ï¸ å¦‚æœä½¿ç”¨äº†alphaæƒé‡ï¼Œè€ƒè™‘è®¾ç½®ä¸º1.0",
            "ğŸ“‰ é€‚å½“é™ä½ pos_weight (å½“å‰å€¼ â†’ å½“å‰å€¼*0.8)",
            "ğŸ“ˆ ç›‘æ§åç»­å‡ ä¸ªepochçš„è¡¨ç°"
        ])
    else:
        recommendations.extend([
            "âœ… å½“å‰è¡¨ç°æ­£å¸¸ï¼Œç»§ç»­è®­ç»ƒ",
            "ğŸ“Š å¯ä»¥è€ƒè™‘è½»å¾®è°ƒä¼˜å‚æ•°ä»¥è·å¾—æ›´å¥½æ•ˆæœ",
            "ğŸ“ˆ æŒç»­ç›‘æ§ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„å¹³è¡¡"
        ])
    
    # åŸºäºå½“å‰é…ç½®çš„å…·ä½“å»ºè®®
    if current_config:
        loss_params = current_config.get('loss', {}).get('params', {})
        gamma = loss_params.get('gamma', 2.0)
        alpha = loss_params.get('alpha', 1.0)
        pos_weight = loss_params.get('pos_weight', 1.0)
        
        recommendations.append("\nğŸ”§ å…·ä½“å‚æ•°è°ƒæ•´å»ºè®®:")
        
        if severity == "ä¸¥é‡":
            recommendations.append(f"  gamma: {gamma} â†’ 1.0")
            recommendations.append(f"  alpha: {alpha} â†’ 1.0")
            recommendations.append(f"  pos_weight: {pos_weight} â†’ 3.0")
        elif severity == "ä¸­ç­‰":
            new_gamma = max(0.5, gamma * 0.7)
            new_pos_weight = max(1.0, pos_weight * 0.8)
            recommendations.append(f"  gamma: {gamma} â†’ {new_gamma:.1f}")
            if alpha != 1.0:
                recommendations.append(f"  alpha: {alpha} â†’ 1.0")
            recommendations.append(f"  pos_weight: {pos_weight} â†’ {new_pos_weight:.1f}")
    
    return recommendations

def main():
    parser = argparse.ArgumentParser(description='è¯Šæ–­Focal Lossè¿‡åº¦é¢„æµ‹é—®é¢˜')
    parser.add_argument('--metrics_dir', type=str, default='runs/neonatal_multilabel',
                       help='æŒ‡æ ‡æ–‡ä»¶ç›®å½•')
    parser.add_argument('--config', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--latest_only', action='store_true',
                       help='åªåˆ†ææœ€æ–°çš„æŒ‡æ ‡æ–‡ä»¶')
    
    args = parser.parse_args()
    
    print("ğŸ” Focal Lossè¿‡åº¦é¢„æµ‹é—®é¢˜è¯Šæ–­")
    print("=" * 60)
    
    # æŸ¥æ‰¾æŒ‡æ ‡æ–‡ä»¶
    metrics_dir = Path(args.metrics_dir)
    if not metrics_dir.exists():
        print(f"âŒ æŒ‡æ ‡ç›®å½•ä¸å­˜åœ¨: {metrics_dir}")
        return 1
    
    # æŸ¥æ‰¾epochæŒ‡æ ‡æ–‡ä»¶
    epoch_files = list(metrics_dir.glob("epoch_*_metrics.json"))
    if not epoch_files:
        print(f"âŒ åœ¨ {metrics_dir} ä¸­æœªæ‰¾åˆ°epochæŒ‡æ ‡æ–‡ä»¶")
        return 1
    
    # æŒ‰epochæ’åº
    epoch_files.sort(key=lambda x: int(x.stem.split('_')[1]))
    
    if args.latest_only:
        epoch_files = [epoch_files[-1]]
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(epoch_files)} ä¸ªæŒ‡æ ‡æ–‡ä»¶")
    
    # åŠ è½½é…ç½®æ–‡ä»¶ï¼ˆå¦‚æœæä¾›ï¼‰
    current_config = None
    if args.config:
        try:
            import yaml
            with open(args.config, 'r', encoding='utf-8') as f:
                current_config = yaml.safe_load(f)
            print(f"ğŸ“‹ å·²åŠ è½½é…ç½®æ–‡ä»¶: {args.config}")
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶: {e}")
    
    # åˆ†ææ¯ä¸ªepoch
    all_diagnoses = []
    
    for epoch_file in epoch_files:
        print(f"\nğŸ“ˆ åˆ†æ {epoch_file.name}")
        print("-" * 40)
        
        analysis = analyze_metrics_file(epoch_file)
        if analysis is None:
            continue
        
        diagnosis = diagnose_overprediction(analysis)
        all_diagnoses.append((analysis['epoch'], diagnosis))
        
        # æ˜¾ç¤ºè¯Šæ–­ç»“æœ
        epoch = analysis['epoch']
        severity = diagnosis['severity']
        metrics = diagnosis['metrics']
        
        # æ ¹æ®ä¸¥é‡ç¨‹åº¦é€‰æ‹©emoji
        severity_emoji = {"æ­£å¸¸": "âœ…", "ä¸­ç­‰": "âš ï¸", "ä¸¥é‡": "ğŸš¨"}
        
        print(f"Epoch {epoch} - {severity_emoji[severity]} {severity}")
        print(f"  å¹³å‡ç²¾ç¡®ç‡: {metrics['avg_precision']:.3f}")
        print(f"  å¹³å‡å¬å›ç‡: {metrics['avg_recall']:.3f}")
        print(f"  å¹³å‡F1åˆ†æ•°: {metrics['avg_f1']:.3f}")
        print(f"  ç²¾ç¡®ç‡/å¬å›ç‡æ¯”ä¾‹: {1/metrics['precision_recall_ratio']:.3f}")
        
        if diagnosis['issues']:
            print("  é—®é¢˜:")
            for issue in diagnosis['issues']:
                print(f"    â€¢ {issue}")
    
    # ç”Ÿæˆæ€»ä½“å»ºè®®
    if all_diagnoses:
        print("\n" + "="*60)
        print("ğŸ“‹ æ€»ä½“è¯Šæ–­å’Œå»ºè®®")
        print("="*60)
        
        # åˆ†æè¶‹åŠ¿
        latest_diagnosis = all_diagnoses[-1][1]
        latest_severity = latest_diagnosis['severity']
        
        print(f"ğŸ¯ å½“å‰çŠ¶æ€: {latest_severity}")
        
        # ç”Ÿæˆå»ºè®®
        recommendations = generate_recommendations(latest_diagnosis, current_config)
        
        print("\nğŸ’¡ ä¿®å¤å»ºè®®:")
        for rec in recommendations:
            print(f"  {rec}")
        
        # è¶‹åŠ¿åˆ†æ
        if len(all_diagnoses) > 1:
            print(f"\nğŸ“ˆ è¶‹åŠ¿åˆ†æ:")
            severities = [d[1]['severity'] for d in all_diagnoses[-3:]]  # æœ€è¿‘3ä¸ªepoch
            if all(s == "ä¸¥é‡" for s in severities[-2:]):
                print("  ğŸ“‰ é—®é¢˜æŒç»­æ¶åŒ–ï¼Œå»ºè®®ç«‹å³åœæ­¢è®­ç»ƒ")
            elif latest_severity == "æ­£å¸¸":
                print("  ğŸ“ˆ è¡¨ç°è‰¯å¥½ï¼Œå¯ä»¥ç»§ç»­è®­ç»ƒ")
            else:
                print("  ğŸ“Š è¡¨ç°ä¸ç¨³å®šï¼Œéœ€è¦å¯†åˆ‡ç›‘æ§")
        
        # è¿”å›çŠ¶æ€ç 
        if latest_severity == "ä¸¥é‡":
            return 2
        elif latest_severity == "ä¸­ç­‰":
            return 1
        else:
            return 0
    
    return 0

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
