"""
å¤šæ ‡ç­¾åˆ†ç±»è¯„ä¼°æŒ‡æ ‡æ¨¡å— (å·²ä½¿ç”¨ sklearn.metrics ä¼˜åŒ–)

æä¾›è¯¦ç»†çš„å¤šæ ‡ç­¾åˆ†ç±»æ€§èƒ½è¯„ä¼°ï¼ŒåŒ…æ‹¬ï¼š
- æ¯ä¸ªç±»åˆ«çš„ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°
- ç±»åˆ«ä¸å¹³è¡¡åˆ†æ
- å®æ—¶è®­ç»ƒç›‘æ§
- ç»“æœä¿å­˜å’Œå¯è§†åŒ–
"""

import os
import torch
import numpy as np
import pandas as pd
import logging
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime
# ä¼˜åŒ–ï¼šå¼•å…¥ classification_report å’Œ accuracy_score
from sklearn.metrics import (precision_score, recall_score, f1_score,
                             classification_report, accuracy_score)

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


class MultilabelMetricsCalculator:
    """å¤šæ ‡ç­¾åˆ†ç±»æŒ‡æ ‡è®¡ç®—å™¨
    
    è´Ÿè´£è®¡ç®—å’Œç®¡ç†å¤šæ ‡ç­¾åˆ†ç±»çš„è¯¦ç»†æŒ‡æ ‡ï¼ŒåŒ…æ‹¬æ¯ä¸ªç±»åˆ«çš„æ€§èƒ½æŒ‡æ ‡ã€‚
    """
    
    def __init__(self, class_names: List[str], output_dir: str = "runs/neonatal",
                 dataset=None, model_type: str = None, exp_name: str = None):
        """åˆå§‹åŒ–æŒ‡æ ‡è®¡ç®—å™¨

        Args:
            class_names: ç±»åˆ«åç§°åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            dataset: æ•°æ®é›†å¯¹è±¡ï¼ˆç”¨äºè·å–æ ·æœ¬çš„session_nameç­‰å…ƒæ•°æ®ï¼‰
            model_type: æ¨¡å‹ç±»å‹ï¼ˆå¦‚r3d_18ï¼‰
            exp_name: å®éªŒåç§°ï¼ˆå¦‚grid_001ï¼‰
        """
        self.class_names = class_names
        self.num_classes = len(class_names)
        self.output_dir = output_dir
        self.dataset = dataset  # ä¿å­˜datasetå¼•ç”¨ï¼Œç”¨äºè·å–æ ·æœ¬å…ƒæ•°æ®
        self.model_type = model_type or "unknown"
        self.exp_name = exp_name or "unknown"

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)

        # åˆå§‹åŒ–æœ€ä½³æŒ‡æ ‡è¿½è¸ª
        self.best_metrics = {
            'epoch': 0,
            'macro_avg_f1': 0.0,
            'macro_avg_accuracy': 0.0,
            'micro_avg_f1': 0.0,
            'weighted_avg_f1': 0.0,
            'class_metrics': {},
            'macro_avg': {},
            'micro_avg': {},
            'weighted_avg': {}
        }

        # ç”¨äºæ”¶é›†æ ·æœ¬çº§åˆ«çš„é¢„æµ‹ç»“æœï¼ˆç”¨äºè§†é¢‘çº§åˆ«èšåˆï¼‰
        self.sample_predictions = []  # æ¯ä¸ªå…ƒç´ : {'session_name', 'predictions', 'targets', 'metrics'}

        # ä¸ºæ¯ä¸ªç±»åˆ«å•ç‹¬è¿½è¸ªæœ€ä½³æŒ‡æ ‡
        self.best_class_metrics = {}
        for class_name in class_names:
            self.best_class_metrics[class_name] = {
                'best_precision': {'value': 0.0, 'epoch': 0},
                'best_recall': {'value': 0.0, 'epoch': 0},
                'best_f1': {'value': 0.0, 'epoch': 0},
                'best_accuracy': {'value': 0.0, 'epoch': 0}
            }

        # å†å²æŒ‡æ ‡è®°å½•
        self.metrics_history = []
    
    def calculate_detailed_metrics(self, predictions: np.ndarray, targets: np.ndarray, 
                                     threshold: float = 0.5) -> Dict[str, Any]:
        """
        (å·²ä¼˜åŒ–) è®¡ç®—è¯¦ç»†çš„å¤šæ ‡ç­¾åˆ†ç±»æŒ‡æ ‡
        
        æ­¤ç‰ˆæœ¬ä½¿ç”¨ sklearn.metrics.classification_report å’Œå‘é‡åŒ–æ“ä½œè¿›è¡Œä¼˜åŒ–ï¼Œ
        æ›¿ä»£äº†åŸæœ‰çš„ for å¾ªç¯ï¼Œæé«˜äº†è®¡ç®—æ•ˆç‡å’Œä»£ç ç®€æ´æ€§ï¼ŒåŒæ—¶ä¿æŒè¾“å‡ºç»“æ„ä¸å˜ã€‚
        
        Args:
            predictions: æ¨¡å‹é¢„æµ‹æ¦‚ç‡ï¼Œå½¢çŠ¶ä¸º (N, num_classes)
            targets: çœŸå®æ ‡ç­¾ï¼Œå½¢çŠ¶ä¸º (N, num_classes)
            threshold: äºŒå€¼åŒ–é˜ˆå€¼
            
        Returns:
            åŒ…å«è¯¦ç»†æŒ‡æ ‡çš„å­—å…¸
        """
        # äºŒå€¼åŒ–é¢„æµ‹
        pred_binary = (predictions > threshold).astype(int)
        targets_binary = targets.astype(int)
        
        # --- ä¼˜åŒ–æ ¸å¿ƒ ---
        # 1. ä½¿ç”¨ classification_report ä¸€æ¬¡æ€§è®¡ç®—å¤§å¤šæ•°æŒ‡æ ‡
        report = classification_report(
            targets_binary, 
            pred_binary, 
            target_names=self.class_names, 
            zero_division=0,
            output_dict=True
        )
        
        # 2. å‘é‡åŒ–è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡ (classification_report ä¸æä¾›æ­¤é¡¹)
        per_class_accuracy = (pred_binary == targets_binary).mean(axis=0)
        
        # 3. æŒ‰ç…§åŸå‡½æ•°æ¥å£è¦æ±‚ï¼Œé‡æ–°ç»„ç»‡ class_metrics å­—å…¸
        class_metrics = {}
        for i, class_name in enumerate(self.class_names):
            class_report = report[class_name]
            support = int(class_report['support'])
            class_metrics[class_name] = {
                'precision': float(class_report['precision']),
                'recall': float(class_report['recall']),
                'f1': float(class_report['f1-score']), # é”®åæ˜ å°„
                'accuracy': float(per_class_accuracy[i]),
                'pos_samples': support,
                'neg_samples': len(targets) - support,
                'total_samples': len(targets)
            }
            
        # 4. æå–æˆ–è®¡ç®—å¹³å‡æŒ‡æ ‡
        macro_avg_report = report['macro avg']
        macro_avg = {
            'precision': float(macro_avg_report['precision']),
            'recall': float(macro_avg_report['recall']),
            'f1': float(macro_avg_report['f1-score']),
            'accuracy': float(np.mean(per_class_accuracy))
        }
        
        # ğŸ”§ ä¿®å¤ï¼šmicro_accuracyä½¿ç”¨å…¨å±€accuracyï¼Œä¸microçš„precision/recallä¸€è‡´
        # åŸæ¥ä½¿ç”¨accuracy_scoreè®¡ç®—çš„æ˜¯subset accuracyï¼ˆæ‰€æœ‰ç±»åˆ«éƒ½æ­£ç¡®æ‰ç®—æ­£ç¡®ï¼‰
        # ç°åœ¨æ”¹ä¸ºå…¨å±€accuracyï¼ˆæ‰€æœ‰é¢„æµ‹ä¸­æ­£ç¡®çš„æ¯”ä¾‹ï¼‰
        micro_accuracy = float((pred_binary == targets_binary).mean())

        # å¯é€‰ï¼šä¿ç•™subset accuracyä½œä¸ºé¢å¤–æŒ‡æ ‡
        subset_accuracy = float(accuracy_score(targets_binary, pred_binary))

        micro_avg = {
            'precision': float(precision_score(targets_binary, pred_binary, average='micro', zero_division=0)),
            'recall': float(recall_score(targets_binary, pred_binary, average='micro', zero_division=0)),
            'f1': float(f1_score(targets_binary, pred_binary, average='micro', zero_division=0)),
            'accuracy': micro_accuracy,  # å…¨å±€å‡†ç¡®ç‡ï¼ˆä¸microçš„precision/recallä¸€è‡´ï¼‰
            'subset_accuracy': subset_accuracy  # å­é›†å‡†ç¡®ç‡ï¼ˆæ‰€æœ‰ç±»åˆ«éƒ½æ­£ç¡®ï¼‰
        }
        
        weighted_avg_report = report['weighted avg']
        class_supports = np.array([m['pos_samples'] for m in class_metrics.values()])
        
        # ä¿®æ­£åŠ æƒå‡†ç¡®ç‡çš„è®¡ç®—
        if np.sum(class_supports) > 0:
            weighted_accuracy = np.average(per_class_accuracy, weights=class_supports)
        else:
            weighted_accuracy = macro_avg['accuracy']

        weighted_avg = {
            'precision': float(weighted_avg_report['precision']),
            'recall': float(weighted_avg_report['recall']),
            'f1': float(weighted_avg_report['f1-score']),
            'accuracy': float(weighted_accuracy)
        }

        # 5. ç»„è£…æˆä¸åŸå‡½æ•°å®Œå…¨ç›¸åŒçš„è¿”å›ç»“æ„
        return {
            'class_metrics': class_metrics,
            'macro_avg': macro_avg,
            'micro_avg': micro_avg,
            'weighted_avg': weighted_avg,
            'threshold': threshold,
            'total_samples': len(targets)
        }
        
    def format_metrics_display(self, metrics: Dict[str, Any], epoch: int,
                               val_loss: float, train_batches: int) -> str:
        """æ ¼å¼åŒ–æŒ‡æ ‡æ˜¾ç¤ºï¼ˆçªå‡ºæ˜¾ç¤ºåŠ æƒå¹³å‡æŒ‡æ ‡ï¼‰

        (æ— éœ€ä¿®æ”¹)
        """
        macro_acc = metrics['macro_avg']['accuracy'] * 100
        macro_f1 = metrics['macro_avg']['f1'] * 100
        
        micro_acc = metrics['micro_avg']['accuracy'] * 100
        weighted_acc = metrics['weighted_avg']['accuracy'] * 100

        main_line = (f"Epoch {epoch:03d} | val_loss={val_loss:.4f} | "
                     f"macro_acc={macro_acc:.2f}% | micro_acc={micro_acc:.2f}% | weighted_acc={weighted_acc:.2f}% | "
                     f"val_f1={macro_f1:.2f}% | train_batches={train_batches}")
        
        detail_lines = ["\nå„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡:"]
        detail_lines.append("ç±»åˆ«åç§°          ç²¾ç¡®ç‡   å¬å›ç‡   F1åˆ†æ•°   å‡†ç¡®ç‡   æ­£æ ·æœ¬   è´Ÿæ ·æœ¬")
        detail_lines.append("-" * 75)
        
        for class_name, class_metric in metrics['class_metrics'].items():
            line = (f"{class_name:<12} "
                    f"{class_metric['precision']:>7.3f}  "
                    f"{class_metric['recall']:>7.3f}  "
                    f"{class_metric['f1']:>7.3f}  "
                    f"{class_metric['accuracy']:>7.3f}  "
                    f"{class_metric['pos_samples']:>6d}  "
                    f"{class_metric['neg_samples']:>6d}")
            detail_lines.append(line)
        
        detail_lines.append("-" * 75)
        detail_lines.append(f"å®å¹³å‡            "
                            f"{metrics['macro_avg']['precision']:>7.3f}  "
                            f"{metrics['macro_avg']['recall']:>7.3f}  "
                            f"{metrics['macro_avg']['f1']:>7.3f}  "
                            f"{metrics['macro_avg']['accuracy']:>7.3f}  "
                            f"{'':>6s}  {'':>6s}")
        
        detail_lines.append(f"å¾®å¹³å‡            "
                            f"{metrics['micro_avg']['precision']:>7.3f}  "
                            f"{metrics['micro_avg']['recall']:>7.3f}  "
                            f"{metrics['micro_avg']['f1']:>7.3f}  "
                            f"{metrics['micro_avg']['accuracy']:>7.3f}  "
                            f"{'':>6s}  {'':>6s}")

        detail_lines.append(f"ğŸ¯åŠ æƒå¹³å‡        "
                            f"{metrics['weighted_avg']['precision']:>7.3f}  "
                            f"{metrics['weighted_avg']['recall']:>7.3f}  "
                            f"{metrics['weighted_avg']['f1']:>7.3f}  "
                            f"{metrics['weighted_avg']['accuracy']:>7.3f}  "
                            f"{'':>6s}  {'':>6s}")

        detail_lines.append("")
        detail_lines.append("ğŸ“Š æŒ‡æ ‡è¯´æ˜:")
        detail_lines.append("  â€¢ å®å¹³å‡: æ¯ä¸ªç±»åˆ«æƒé‡ç›¸ç­‰ï¼Œå¯¹ç¨€æœ‰ç±»åˆ«æ•æ„Ÿ")
        detail_lines.append("  â€¢ å¾®å¹³å‡: æŒ‰æ ·æœ¬æ•°é‡åŠ æƒï¼Œå¯¹ä¸»è¦ç±»åˆ«æ•æ„Ÿ")
        detail_lines.append("  â€¢ ğŸ¯åŠ æƒå¹³å‡: æŒ‰ç±»åˆ«æ ·æœ¬æ•°åŠ æƒï¼Œé€‚åˆä¸å¹³è¡¡æ•°æ®ï¼ˆæ¨èï¼‰")

        return main_line + "\n" + "\n".join(detail_lines)
    
    def update_best_metrics(self, metrics: Dict[str, Any], epoch: int,
                           predictions: np.ndarray = None, targets: np.ndarray = None) -> bool:
        """æ›´æ–°æœ€ä½³æŒ‡æ ‡è®°å½•ï¼ˆåŒ…æ‹¬æ¯ä¸ªç±»åˆ«çš„æœ€ä½³æŒ‡æ ‡ï¼‰

        Args:
            metrics: è¯¦ç»†æŒ‡æ ‡å­—å…¸
            epoch: å½“å‰epochç¼–å·
            predictions: é¢„æµ‹æ¦‚ç‡æ•°ç»„ (n_samples, n_classes)
            targets: çœŸå®æ ‡ç­¾æ•°ç»„ (n_samples, n_classes)

        Returns:
            æ˜¯å¦ä¸ºæ•´ä½“æœ€ä½³æŒ‡æ ‡
        """
        current_f1 = metrics['macro_avg']['f1']
        current_accuracy = metrics['macro_avg']['accuracy']
        is_best_overall = False
        is_best_f1 = False
        is_best_accuracy = False

        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³F1åˆ†æ•°
        if current_f1 > self.best_metrics['macro_avg_f1']:
            self.best_metrics = {
                'epoch': epoch,
                'macro_avg_f1': current_f1,
                'macro_avg_accuracy': metrics['macro_avg']['accuracy'],
                'micro_avg_f1': metrics['micro_avg']['f1'],
                'weighted_avg_f1': metrics['weighted_avg']['f1'],
                'class_metrics': metrics['class_metrics'].copy(),
                'macro_avg': metrics['macro_avg'].copy(),
                'micro_avg': metrics['micro_avg'].copy(),
                'weighted_avg': metrics['weighted_avg'].copy()
            }
            is_best_overall = True
            is_best_f1 = True

            # ç”Ÿæˆæœ€ä½³F1æ—¶çš„è§†é¢‘çº§åˆ«æŠ¥å‘Š
            if predictions is not None and targets is not None:
                self.save_video_metrics_files(predictions, targets, epoch, metric_type='best_f1')

        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³å‡†ç¡®ç‡ï¼ˆç‹¬ç«‹äºF1ï¼‰
        if current_accuracy > self.best_metrics.get('best_accuracy_value', 0.0):
            self.best_metrics['best_accuracy_value'] = current_accuracy
            self.best_metrics['best_accuracy_epoch'] = epoch
            is_best_accuracy = True

            # ç”Ÿæˆæœ€ä½³å‡†ç¡®ç‡æ—¶çš„è§†é¢‘çº§åˆ«æŠ¥å‘Š
            if predictions is not None and targets is not None:
                self.save_video_metrics_files(predictions, targets, epoch, metric_type='best_accuracy')

        for class_name, class_metric in metrics['class_metrics'].items():
            if class_name in self.best_class_metrics:
                if class_metric['precision'] > self.best_class_metrics[class_name]['best_precision']['value']:
                    self.best_class_metrics[class_name]['best_precision'] = {'value': class_metric['precision'], 'epoch': epoch}
                if class_metric['recall'] > self.best_class_metrics[class_name]['best_recall']['value']:
                    self.best_class_metrics[class_name]['best_recall'] = {'value': class_metric['recall'], 'epoch': epoch}
                if class_metric['f1'] > self.best_class_metrics[class_name]['best_f1']['value']:
                    self.best_class_metrics[class_name]['best_f1'] = {'value': class_metric['f1'], 'epoch': epoch}
                if class_metric['accuracy'] > self.best_class_metrics[class_name]['best_accuracy']['value']:
                    self.best_class_metrics[class_name]['best_accuracy'] = {'value': class_metric['accuracy'], 'epoch': epoch}

        self.save_best_metrics_files()
        return is_best_overall

    def aggregate_video_metrics(self, predictions: np.ndarray, targets: np.ndarray) -> pd.DataFrame:
        """æŒ‰è§†é¢‘åç§°èšåˆç‰‡æ®µçº§åˆ«çš„æŒ‡æ ‡

        Args:
            predictions: é¢„æµ‹æ¦‚ç‡æ•°ç»„ (n_samples, n_classes)
            targets: çœŸå®æ ‡ç­¾æ•°ç»„ (n_samples, n_classes)

        Returns:
            åŒ…å«æ¯ä¸ªè§†é¢‘èšåˆæŒ‡æ ‡çš„DataFrame
        """
        # æ£€æŸ¥æ˜¯å¦æœ‰datasetå¼•ç”¨
        if self.dataset is None:
            logger.warning("æœªæä¾›datasetå¼•ç”¨ï¼Œæ— æ³•ç”Ÿæˆè§†é¢‘çº§åˆ«æŠ¥å‘Š")
            return None

        # æ£€æŸ¥datasetæ˜¯å¦æœ‰sampleså±æ€§
        if not hasattr(self.dataset, 'samples'):
            logger.warning("datasetæ²¡æœ‰sampleså±æ€§ï¼Œæ— æ³•ç”Ÿæˆè§†é¢‘çº§åˆ«æŠ¥å‘Š")
            return None

        # æ£€æŸ¥æ ·æœ¬æ•°é‡æ˜¯å¦åŒ¹é…
        if len(self.dataset.samples) != len(predictions):
            logger.warning(f"æ ·æœ¬æ•°é‡ä¸åŒ¹é…: dataset={len(self.dataset.samples)}, predictions={len(predictions)}")
            return None

        # æ”¶é›†æ¯ä¸ªæ ·æœ¬çš„session_nameå’ŒæŒ‡æ ‡
        video_data = {}  # {session_name: {'clips': [], 'predictions': [], 'targets': []}}

        for idx in range(len(predictions)):
            sample = self.dataset.samples[idx]

            # è·å–session_nameï¼ˆå‘åå…¼å®¹ï¼‰
            session_name = sample.get('session_name', sample.get('video_name', f'unknown_{idx}'))

            if session_name not in video_data:
                video_data[session_name] = {
                    'clips': [],
                    'predictions': [],
                    'targets': []
                }

            video_data[session_name]['clips'].append(idx)
            video_data[session_name]['predictions'].append(predictions[idx])
            video_data[session_name]['targets'].append(targets[idx])

        # è®¡ç®—æ¯ä¸ªè§†é¢‘çš„èšåˆæŒ‡æ ‡
        video_metrics = []

        for session_name, data in video_data.items():
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            video_preds = np.array(data['predictions'])  # (n_clips, n_classes)
            video_targets = np.array(data['targets'])    # (n_clips, n_classes)

            # è®¡ç®—è¯¥è§†é¢‘æ‰€æœ‰ç‰‡æ®µçš„å¹³å‡æŒ‡æ ‡ï¼ˆå®å¹³å‡ï¼‰
            # å¯¹æ¯ä¸ªç‰‡æ®µè®¡ç®—æŒ‡æ ‡ï¼Œç„¶åå¹³å‡
            clip_precisions = []
            clip_recalls = []
            clip_f1s = []
            clip_accuracies = []

            for clip_pred, clip_target in zip(video_preds, video_targets):
                # äºŒå€¼åŒ–é¢„æµ‹
                clip_pred_binary = (clip_pred > 0.5).astype(int)
                clip_target_binary = clip_target.astype(int)

                # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡ï¼Œç„¶åå®å¹³å‡
                precision = precision_score(clip_target_binary, clip_pred_binary,
                                          average='macro', zero_division=0)
                recall = recall_score(clip_target_binary, clip_pred_binary,
                                    average='macro', zero_division=0)
                f1 = f1_score(clip_target_binary, clip_pred_binary,
                            average='macro', zero_division=0)
                accuracy = accuracy_score(clip_target_binary, clip_pred_binary)

                clip_precisions.append(precision)
                clip_recalls.append(recall)
                clip_f1s.append(f1)
                clip_accuracies.append(accuracy)

            # è®¡ç®—è¯¥è§†é¢‘çš„å¹³å‡æŒ‡æ ‡
            video_metrics.append({
                'session_name': session_name,
                'total_clips': len(data['clips']),
                'avg_precision': np.mean(clip_precisions),
                'avg_recall': np.mean(clip_recalls),
                'avg_f1': np.mean(clip_f1s),
                'avg_accuracy': np.mean(clip_accuracies)
            })

        # è½¬æ¢ä¸ºDataFrameå¹¶æŒ‰avg_f1æ’åºï¼ˆä»ä½åˆ°é«˜ï¼Œæ–¹ä¾¿è¯†åˆ«è¡¨ç°å·®çš„è§†é¢‘ï¼‰
        df = pd.DataFrame(video_metrics)
        df = df.sort_values('avg_f1', ascending=True)

        return df

    def save_video_metrics_files(self, predictions: np.ndarray, targets: np.ndarray,
                                 epoch: int, metric_type: str = 'best_f1'):
        """ä¿å­˜è§†é¢‘çº§åˆ«çš„æŒ‡æ ‡åˆ°CSVæ–‡ä»¶

        Args:
            predictions: é¢„æµ‹æ¦‚ç‡æ•°ç»„ (n_samples, n_classes)
            targets: çœŸå®æ ‡ç­¾æ•°ç»„ (n_samples, n_classes)
            epoch: å½“å‰epochç¼–å·
            metric_type: æŒ‡æ ‡ç±»å‹ ('best_f1' æˆ– 'best_accuracy')
        """
        # èšåˆè§†é¢‘çº§åˆ«çš„æŒ‡æ ‡
        df = self.aggregate_video_metrics(predictions, targets)

        if df is None or df.empty:
            return

        # æ·»åŠ é¢å¤–çš„å…ƒæ•°æ®åˆ—
        df['best_epoch'] = epoch
        df['model_type'] = self.model_type
        df['exp_name'] = self.exp_name

        # é‡æ–°æ’åˆ—åˆ—çš„é¡ºåº
        columns_order = ['session_name', 'total_clips', 'avg_precision', 'avg_recall',
                        'avg_f1', 'avg_accuracy', 'best_epoch', 'model_type', 'exp_name']
        df = df[columns_order]

        # ç”Ÿæˆæ–‡ä»¶å
        filename = f"video_metrics_{self.model_type}_{self.exp_name}_{metric_type}.csv"
        filepath = os.path.join(self.output_dir, filename)

        # ä¿å­˜åˆ°CSV
        df.to_csv(filepath, index=False, encoding='utf-8-sig')
        logger.info(f"ğŸ“Š è§†é¢‘çº§åˆ«æŒ‡æ ‡å·²ä¿å­˜: {filename}")
        logger.info(f"   å…± {len(df)} ä¸ªè§†é¢‘ï¼Œå¹³å‡F1åˆ†æ•°: {df['avg_f1'].mean():.4f}")

        # æ˜¾ç¤ºè¡¨ç°æœ€å·®çš„5ä¸ªè§†é¢‘
        worst_videos = df.head(5)
        logger.info(f"   è¡¨ç°æœ€å·®çš„5ä¸ªè§†é¢‘:")
        for idx, row in worst_videos.iterrows():
            logger.info(f"      {row['session_name']}: F1={row['avg_f1']:.4f}, "
                       f"clips={row['total_clips']}, acc={row['avg_accuracy']:.4f}")

    def save_best_metrics_files(self):
        """ä¿å­˜æœ€ä½³æŒ‡æ ‡åˆ°æ–‡ä»¶

        (æ— éœ€ä¿®æ”¹)
        """
        csv_data = []
        for class_name in self.class_names:
            if class_name in self.best_class_metrics:
                class_best = self.best_class_metrics[class_name]
                csv_data.append({
                    'ç±»åˆ«åç§°': class_name,
                    'æœ€ä½³ç²¾ç¡®ç‡': f"{class_best['best_precision']['value']:.4f}",
                    'æœ€ä½³ç²¾ç¡®ç‡Epoch': class_best['best_precision']['epoch'],
                    'æœ€ä½³å¬å›ç‡': f"{class_best['best_recall']['value']:.4f}",
                    'æœ€ä½³å¬å›ç‡Epoch': class_best['best_recall']['epoch'],
                    'æœ€ä½³F1åˆ†æ•°': f"{class_best['best_f1']['value']:.4f}",
                    'æœ€ä½³F1åˆ†æ•°Epoch': class_best['best_f1']['epoch'],
                    'æœ€ä½³å‡†ç¡®ç‡': f"{class_best['best_accuracy']['value']:.4f}",
                    'æœ€ä½³å‡†ç¡®ç‡Epoch': class_best['best_accuracy']['epoch']
                })

        if 'macro_avg' in self.best_metrics and self.best_metrics['macro_avg']:
            csv_data.append({
                'ç±»åˆ«åç§°': 'ğŸ†æ•´ä½“æœ€ä½³',
                'æœ€ä½³ç²¾ç¡®ç‡': f"{self.best_metrics['macro_avg']['precision']:.4f}",
                'æœ€ä½³ç²¾ç¡®ç‡Epoch': self.best_metrics['epoch'],
                'æœ€ä½³å¬å›ç‡': f"{self.best_metrics['macro_avg']['recall']:.4f}",
                'æœ€ä½³å¬å›ç‡Epoch': self.best_metrics['epoch'],
                'æœ€ä½³F1åˆ†æ•°': f"{self.best_metrics['macro_avg_f1']:.4f}",
                'æœ€ä½³F1åˆ†æ•°Epoch': self.best_metrics['epoch'],
                'æœ€ä½³å‡†ç¡®ç‡': f"{self.best_metrics['macro_avg_accuracy']:.4f}",
                'æœ€ä½³å‡†ç¡®ç‡Epoch': self.best_metrics['epoch']
            })
        else:
            csv_data.append({
                'ç±»åˆ«åç§°': 'ğŸ†æ•´ä½“æœ€ä½³', 'æœ€ä½³ç²¾ç¡®ç‡': 'å¾…æ›´æ–°', 'æœ€ä½³ç²¾ç¡®ç‡Epoch': 0,
                'æœ€ä½³å¬å›ç‡': 'å¾…æ›´æ–°', 'æœ€ä½³å¬å›ç‡Epoch': 0, 'æœ€ä½³F1åˆ†æ•°': 'å¾…æ›´æ–°',
                'æœ€ä½³F1åˆ†æ•°Epoch': 0, 'æœ€ä½³å‡†ç¡®ç‡': 'å¾…æ›´æ–°', 'æœ€ä½³å‡†ç¡®ç‡Epoch': 0
            })

        df = pd.DataFrame(csv_data)
        best_metrics_csv = os.path.join(self.output_dir, "best_metrics_summary.csv")
        df.to_csv(best_metrics_csv, index=False, encoding='utf-8-sig')

    def save_metrics(self, metrics: Dict[str, Any], epoch: int,
                     val_loss: float, is_best: bool = False):
        """ä¿å­˜æŒ‡æ ‡åˆ°æ–‡ä»¶

        (æ— éœ€ä¿®æ”¹)
        """
        record = {
            'epoch': epoch,
            'val_loss': val_loss,
            'timestamp': datetime.now().isoformat(),
            'is_best': is_best,
            **metrics
        }
        self.metrics_history.append(record)
        self._save_class_metrics_csv(metrics, epoch)
    
    def _save_class_metrics_csv(self, metrics: Dict[str, Any], epoch: int):
        """ä¿å­˜ç±»åˆ«æŒ‡æ ‡åˆ°CSVæ–‡ä»¶

        (æ— éœ€ä¿®æ”¹)
        """
        csv_file = os.path.join(self.output_dir, 'class_metrics_history.csv')
        rows = []
        for class_name, class_metric in metrics['class_metrics'].items():
            row = {
                'epoch': epoch, 'class_name': class_name,
                'precision': class_metric['precision'], 'recall': class_metric['recall'],
                'f1': class_metric['f1'], 'accuracy': class_metric['accuracy'],
                'pos_samples': class_metric['pos_samples'], 'neg_samples': class_metric['neg_samples']
            }
            rows.append(row)
        df = pd.DataFrame(rows)
        if os.path.exists(csv_file):
            df.to_csv(csv_file, mode='a', header=False, index=False, encoding='utf-8')
        else:
            df.to_csv(csv_file, index=False, encoding='utf-8')

    def save_train_metrics(self, metrics: Dict[str, Any], epoch: int, train_loss: float):
        """ä¿å­˜è®­ç»ƒé›†æŒ‡æ ‡åˆ°å•ç‹¬çš„CSVæ–‡ä»¶

        (æ— éœ€ä¿®æ”¹)
        """
        csv_file = os.path.join(self.output_dir, 'train_metrics_history.csv')
        rows = []
        for class_name, class_metric in metrics['class_metrics'].items():
            row = {
                'epoch': epoch, 'class_name': class_name,
                'precision': class_metric['precision'], 'recall': class_metric['recall'],
                'f1': class_metric['f1'], 'accuracy': class_metric['accuracy'],
                'pos_samples': class_metric['pos_samples'], 'neg_samples': class_metric['neg_samples']
            }
            rows.append(row)
        df = pd.DataFrame(rows)
        if os.path.exists(csv_file):
            df.to_csv(csv_file, mode='a', header=False, index=False, encoding='utf-8')
        else:
            df.to_csv(csv_file, index=False, encoding='utf-8')

    def save_test_metrics(self, metrics: Dict[str, Any], epoch: int, test_loss: float):
        """ä¿å­˜æµ‹è¯•é›†æŒ‡æ ‡åˆ°å•ç‹¬çš„CSVæ–‡ä»¶

        (æ— éœ€ä¿®æ”¹)
        """
        csv_file = os.path.join(self.output_dir, 'test_metrics_history.csv')
        rows = []
        for class_name, class_metric in metrics['class_metrics'].items():
            row = {
                'epoch': epoch, 'class_name': class_name,
                'precision': class_metric['precision'], 'recall': class_metric['recall'],
                'f1': class_metric['f1'], 'accuracy': class_metric['accuracy'],
                'pos_samples': class_metric['pos_samples'], 'neg_samples': class_metric['neg_samples']
            }
            rows.append(row)
        df = pd.DataFrame(rows)
        if os.path.exists(csv_file):
            df.to_csv(csv_file, mode='a', header=False, index=False, encoding='utf-8')
        else:
            df.to_csv(csv_file, index=False, encoding='utf-8')
    
    def get_summary_report(self) -> str:
        """è·å–è®­ç»ƒæ€»ç»“æŠ¥å‘Š
        
        (æ— éœ€ä¿®æ”¹)
        """
        if not self.best_metrics['class_metrics']:
            return "æš‚æ— æœ€ä½³æŒ‡æ ‡è®°å½•"
        
        lines = [
            f"\nğŸ† è®­ç»ƒæ€»ç»“æŠ¥å‘Š (æœ€ä½³epoch: {self.best_metrics['epoch']})",
            "=" * 80,
            f"æœ€ä½³å®å¹³å‡F1åˆ†æ•°: {self.best_metrics['macro_avg_f1']:.4f}",
            f"æœ€ä½³å®å¹³å‡å‡†ç¡®ç‡: {self.best_metrics['macro_avg']['accuracy']:.4f}",
            "",
            "å„ç±»åˆ«æœ€ä½³æŒ‡æ ‡:",
            "ç±»åˆ«åç§°          ç²¾ç¡®ç‡   å¬å›ç‡   F1åˆ†æ•°   å‡†ç¡®ç‡   æ­£æ ·æœ¬   è´Ÿæ ·æœ¬",
            "-" * 75
        ]
        
        for class_name, class_metric in self.best_metrics['class_metrics'].items():
            line = (f"{class_name:<12} "
                    f"{class_metric['precision']:>7.3f}  "
                    f"{class_metric['recall']:>7.3f}  "
                    f"{class_metric['f1']:>7.3f}  "
                    f"{class_metric['accuracy']:>7.3f}  "
                    f"{class_metric['pos_samples']:>6d}  "
                    f"{class_metric['neg_samples']:>6d}")
            lines.append(line)
        
        lines.extend([
            "-" * 75,
            f"ğŸ“Š æŒ‡æ ‡æ–‡ä»¶ä¿å­˜ä½ç½®: {self.output_dir}",
            f"ğŸ“ˆ å†å²è®°å½•: {len(self.metrics_history)} ä¸ªepoch",
            "=" * 80
        ])
        
        return "\n".join(lines)