"""ç»Ÿä¸€æ•°æ®åŠ è½½å™¨å·¥å‚

è¯¥æ¨¡å—æä¾›ç»Ÿä¸€çš„æ•°æ®åŠ è½½å™¨åˆ›å»ºæ¥å£ï¼Œä½¿ç”¨src/datasetsä¸­å®šä¹‰çš„æ•°æ®é›†ç±»ã€‚
"""

import os
import torch
import numpy as np
from torch.utils.data import DataLoader, Subset, WeightedRandomSampler
from collections import Counter
from .cifar10_dataset import CIFAR10Dataset
from .custom_dataset import CustomDatasetWrapper
from .video_dataset import VideoDataset, CombinedVideoDataset
from .neonatal_multilabel_dataset import NeonatalMultilabelDataset


def is_main_process():
    """æ£€æŸ¥æ˜¯å¦ä¸ºä¸»è¿›ç¨‹ï¼ˆç”¨äºé¿å…é‡å¤è¾“å‡ºï¼‰"""
    return int(os.environ.get("LOCAL_RANK", 0)) == 0


def calculate_sample_weights(dataset, mode='inverse_frequency', verbose=True):
    """ä¸ºå¤šæ ‡ç­¾æ•°æ®é›†è®¡ç®—æ ·æœ¬æƒé‡

    Args:
        dataset: æ•°æ®é›†å¯¹è±¡ï¼ˆæ”¯æŒSubsetåŒ…è£…ï¼‰
        mode (str): æƒé‡è®¡ç®—æ¨¡å¼
            - 'inverse_frequency': åŸºäºç±»åˆ«é€†é¢‘ç‡çš„æƒé‡
            - 'label_combination': åŸºäºæ ‡ç­¾ç»„åˆç¨€æœ‰æ€§çš„æƒé‡
        verbose (bool): æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯

    Returns:
        torch.Tensor: æ¯ä¸ªæ ·æœ¬çš„æƒé‡å‘é‡
    """
    # å¤„ç†SubsetåŒ…è£…çš„æƒ…å†µ
    actual_dataset = dataset.dataset if isinstance(dataset, Subset) else dataset
    indices = dataset.indices if isinstance(dataset, Subset) else range(len(dataset))

    # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦æ”¯æŒå¤šæ ‡ç­¾
    if not hasattr(actual_dataset, 'get_num_classes'):
        raise ValueError("æ•°æ®é›†ä¸æ”¯æŒå¤šæ ‡ç­¾æƒé‡è®¡ç®—")

    num_classes = actual_dataset.get_num_classes()
    class_names = actual_dataset.get_class_names() if hasattr(actual_dataset, 'get_class_names') else None

    # ğŸ”§ ä¼˜åŒ–ï¼šç›´æ¥ä»æ•°æ®é›†çš„sampleså±æ€§è¯»å–æ ‡ç­¾ï¼Œé¿å…åŠ è½½å›¾åƒæ•°æ®
    # æ”¶é›†æ‰€æœ‰æ ‡ç­¾
    all_labels = []

    # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦æœ‰sampleså±æ€§ï¼ˆNeonatalMultilabelDatasetæœ‰ï¼‰
    if hasattr(actual_dataset, 'samples'):
        # ç›´æ¥ä»samplesè¯»å–æ ‡ç­¾ï¼Œé¿å…åŠ è½½å›¾åƒ
        for idx in indices:
            sample = actual_dataset.samples[idx]
            labels = sample['labels']
            if isinstance(labels, torch.Tensor):
                labels = labels.numpy()
            all_labels.append(labels)
    else:
        # é™çº§æ–¹æ¡ˆï¼šé€šè¿‡__getitem__è·å–æ ‡ç­¾ï¼ˆä¼šåŠ è½½å›¾åƒï¼Œè¾ƒæ…¢ï¼‰
        if verbose and is_main_process():
            print(f"   âš ï¸  æ•°æ®é›†æ²¡æœ‰sampleså±æ€§ï¼Œä½¿ç”¨__getitem__æ–¹æ³•ï¼ˆè¾ƒæ…¢ï¼‰...")
        for idx in indices:
            _, labels = actual_dataset[idx]
            if isinstance(labels, torch.Tensor):
                labels = labels.numpy()
            all_labels.append(labels)

    all_labels = np.array(all_labels)  # (n_samples, n_classes)

    if mode == 'inverse_frequency':
        # ğŸ”§ æ¨¡å¼1: åŸºäºç±»åˆ«é€†é¢‘ç‡çš„æƒé‡
        # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æ­£æ ·æœ¬æ•°
        class_counts = all_labels.sum(axis=0)  # (n_classes,)

        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„é€†é¢‘ç‡æƒé‡
        # ä½¿ç”¨å¹³æ»‘å› å­é¿å…é™¤é›¶å’Œæç«¯å€¼
        class_weights = 1.0 / (class_counts + 1.0)
        class_weights = class_weights / class_weights.sum() * num_classes  # å½’ä¸€åŒ–

        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æƒé‡ï¼ˆæ‰€æœ‰æ­£æ ‡ç­¾æƒé‡çš„å¹³å‡ï¼‰
        sample_weights = []
        for labels in all_labels:
            if labels.sum() > 0:
                # æ ·æœ¬æƒé‡ = å…¶æ‰€æœ‰æ­£æ ‡ç­¾æƒé‡çš„å¹³å‡å€¼
                weight = (class_weights * labels).sum() / labels.sum()
            else:
                # æ— æ ‡ç­¾æ ·æœ¬ä½¿ç”¨å¹³å‡æƒé‡
                weight = 1.0
            sample_weights.append(weight)

        sample_weights = np.array(sample_weights)

        if verbose and is_main_process():
            print(f"\nğŸ“Š åŠ æƒé‡‡æ ·ç»Ÿè®¡ (æ¨¡å¼: {mode}):")
            print(f"   ç±»åˆ«æƒé‡:")
            for i in range(num_classes):
                class_name = class_names[i] if class_names else f"ç±»åˆ«{i}"
                print(f"     {class_name}: æ ·æœ¬æ•°={int(class_counts[i])}, æƒé‡={class_weights[i]:.4f}")

    elif mode == 'label_combination':
        # ğŸ”§ æ¨¡å¼2: åŸºäºæ ‡ç­¾ç»„åˆç¨€æœ‰æ€§çš„æƒé‡
        # å°†æ¯ä¸ªæ ‡ç­¾ç»„åˆè½¬æ¢ä¸ºå­—ç¬¦ä¸²ä½œä¸ºé”®
        label_combinations = Counter()
        label_to_indices = {}

        for idx, labels in enumerate(all_labels):
            label_key = tuple(labels)
            label_combinations[label_key] += 1
            if label_key not in label_to_indices:
                label_to_indices[label_key] = []
            label_to_indices[label_key].append(idx)

        # è®¡ç®—æ¯ä¸ªæ ‡ç­¾ç»„åˆçš„æƒé‡ï¼ˆé€†é¢‘ç‡ï¼‰
        total_samples = len(all_labels)
        combination_weights = {
            label_key: total_samples / count
            for label_key, count in label_combinations.items()
        }

        # å½’ä¸€åŒ–æƒé‡
        max_weight = max(combination_weights.values())
        combination_weights = {
            label_key: weight / max_weight
            for label_key, weight in combination_weights.items()
        }

        # ä¸ºæ¯ä¸ªæ ·æœ¬åˆ†é…æƒé‡
        sample_weights = np.array([
            combination_weights[tuple(labels)]
            for labels in all_labels
        ])

        if verbose and is_main_process():
            print(f"\nğŸ“Š åŠ æƒé‡‡æ ·ç»Ÿè®¡ (æ¨¡å¼: {mode}):")
            print(f"   æ ‡ç­¾ç»„åˆæ•°é‡: {len(label_combinations)}")
            print(f"   å‰10ä¸ªæœ€ç¨€æœ‰çš„æ ‡ç­¾ç»„åˆ:")
            sorted_combinations = sorted(
                label_combinations.items(),
                key=lambda x: x[1]
            )[:10]
            for label_key, count in sorted_combinations:
                label_str = ','.join([
                    class_names[i] if class_names else str(i)
                    for i, val in enumerate(label_key) if val > 0
                ])
                weight = combination_weights[label_key]
                print(f"     [{label_str}]: æ ·æœ¬æ•°={count}, æƒé‡={weight:.4f}")

    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æƒé‡è®¡ç®—æ¨¡å¼: {mode}")

    # è¾“å‡ºæƒé‡ç»Ÿè®¡
    if verbose and is_main_process():
        print(f"   æ ·æœ¬æƒé‡ç»Ÿè®¡:")
        print(f"     æœ€å°å€¼: {sample_weights.min():.4f}")
        print(f"     æœ€å¤§å€¼: {sample_weights.max():.4f}")
        print(f"     å¹³å‡å€¼: {sample_weights.mean():.4f}")
        print(f"     ä¸­ä½æ•°: {np.median(sample_weights):.4f}")
        print(f"     æ ‡å‡†å·®: {sample_weights.std():.4f}")

    return torch.from_numpy(sample_weights).float()


def create_dataloaders(dataset_name, data_dir, batch_size, num_workers=4, model_type=None, **kwargs):
    """
    ç»Ÿä¸€çš„æ•°æ®åŠ è½½å™¨åˆ›å»ºå‡½æ•°

    Args:
        dataset_name (str): æ•°æ®é›†åç§°ï¼Œæ”¯æŒ'cifar10'ã€'custom'æˆ–'ucf101'
        data_dir (str): æ•°æ®å­˜å‚¨æ ¹ç›®å½•è·¯å¾„
        batch_size (int): æ‰¹å¤§å°
        num_workers (int, optional): æ•°æ®åŠ è½½çš„å·¥ä½œè¿›ç¨‹æ•°ï¼Œé»˜è®¤ä¸º4
        model_type (str, optional): æ¨¡å‹ç±»å‹ï¼Œç”¨äºè§†é¢‘æ•°æ®é›†çš„åŠ¨æ€transforms
        **kwargs: å…¶ä»–æ•°æ®é›†ç‰¹å®šå‚æ•°ï¼Œå¦‚augment, download, csv_fileç­‰

    Returns:
        tuple: (train_loader, test_loader, num_classes) è®­ç»ƒå’Œæµ‹è¯•æ•°æ®åŠ è½½å™¨åŠç±»åˆ«æ•°

    Raises:
        ValueError: å½“æŒ‡å®šçš„æ•°æ®é›†åç§°ä¸æ”¯æŒæ—¶
    """
    dataset_name = dataset_name.lower()
    # æ•°æ®å­é‡‡æ ·æ¯”ä¾‹ï¼ˆ0-1ï¼‰ï¼Œ1.0è¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æ•°æ®
    data_percentage = float(kwargs.get('data_percentage', 1.0))

    if dataset_name == "cifar10":
        # åˆ›å»ºCIFAR-10æ•°æ®é›†
        cifar10_dataset = CIFAR10Dataset(
            data_dir=data_dir,
            augment=kwargs.get('augment', True),
            download=kwargs.get('download', True)
        )
        
        train_dataset, test_dataset = cifar10_dataset.get_datasets()
        num_classes = cifar10_dataset.num_classes

    elif dataset_name == "custom":
        # åˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›†
        custom_dataset = CustomDatasetWrapper(
            data_dir=data_dir,
            csv_file=kwargs.get('csv_file', None),
            image_size=kwargs.get('image_size', 224),
            augment=kwargs.get('augment', True),
            train_split=kwargs.get('train_split', 0.8)
        )
        
        train_dataset, test_dataset = custom_dataset.get_datasets()
        num_classes = custom_dataset.num_classes

    elif dataset_name in ["ucf101", "ucf101_video"]:
        # ç»Ÿä¸€ä½¿ç”¨VideoDatasetå¤„ç†UCF-101è§†é¢‘æ•°æ®ï¼ˆä»é¢„å¤„ç†å¸§å›¾åƒåŠ è½½ï¼‰
        clip_len = kwargs.get('clip_len', kwargs.get('frames_per_clip', 16))  # å…¼å®¹ä¸¤ç§å‚æ•°å

        # FPSé‡‡æ ·ç›¸å…³å‚æ•°
        sampling_mode = kwargs.get('sampling_mode', 'random')
        target_fps = kwargs.get('target_fps', None)
        original_fps = kwargs.get('original_fps', 16)

        train_dataset = VideoDataset(
            dataset_path=data_dir,
            images_path='train',
            clip_len=clip_len,
            model_type=model_type,  # ä¼ é€’æ¨¡å‹ç±»å‹ç”¨äºåŠ¨æ€transforms
            sampling_mode=sampling_mode,
            target_fps=target_fps,
            original_fps=original_fps
        )

        # å°†valå’Œteståˆå¹¶ä½œä¸ºæµ‹è¯•é›†
        test_dataset = CombinedVideoDataset(
            dataset_path=data_dir,
            clip_len=clip_len,
            model_type=model_type,  # ä¼ é€’æ¨¡å‹ç±»å‹ç”¨äºåŠ¨æ€transforms
            sampling_mode=sampling_mode,
            target_fps=target_fps,
            original_fps=original_fps
        )

        num_classes = 101  # UCF-101å›ºå®šä¸º101ä¸ªç±»åˆ«

    elif dataset_name == "neonatal_multilabel":
        # æ–°ç”Ÿå„¿å¤šæ ‡ç­¾è¡Œä¸ºè¯†åˆ«æ•°æ®é›†
        clip_len = kwargs.get('clip_len', kwargs.get('frames_per_clip', 16))
        top_n_classes = kwargs.get('top_n_classes', None)
        stratified_split = kwargs.get('stratified_split', True)
        min_samples_per_class = kwargs.get('min_samples_per_class', 10)

        # FPSé‡‡æ ·ç›¸å…³å‚æ•°
        sampling_mode = kwargs.get('sampling_mode', 'random')
        target_fps = kwargs.get('target_fps', None)
        original_fps = kwargs.get('original_fps', 16)

        # æ•°æ®è·¯å¾„ï¼šä¼˜å…ˆä½¿ç”¨configä¸­çš„root/paramsï¼Œæœªæä¾›æ—¶å›é€€åˆ°é»˜è®¤è·¯å¾„ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰
        import os
        # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆå‡è®¾dataloader_factory.pyåœ¨src/datasets/ä¸‹ï¼‰
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        default_frames = os.path.join(project_root, "../Neonate-Feeding-Assessment/data/cpu_processed_627/frames_segments")
        default_labels = os.path.join(project_root, "../Neonate-Feeding-Assessment/result_xlsx/shanghai/multi_hot_labels.xlsx")

        # å¦‚æœdata_diræ˜¯ç›¸å¯¹è·¯å¾„ï¼Œåˆ™ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•è§£æ
        if data_dir:
            if not os.path.isabs(data_dir):
                frames_dir = os.path.join(project_root, data_dir)
            else:
                frames_dir = data_dir
        else:
            frames_dir = default_frames

        # æ ‡ç­¾æ–‡ä»¶è·¯å¾„å¤„ç†
        labels_file_param = (
            kwargs.get('labels_file') or
            kwargs.get('label_file') or
            kwargs.get('labels_path')
        )
        if labels_file_param:
            if not os.path.isabs(labels_file_param):
                labels_file = os.path.join(project_root, labels_file_param)
            else:
                labels_file = labels_file_param
        else:
            labels_file = default_labels

        train_dataset = NeonatalMultilabelDataset(
            frames_dir=frames_dir,
            labels_file=labels_file,
            split='train',
            clip_len=clip_len,
            model_type=model_type,
            top_n_classes=top_n_classes,
            stratified_split=stratified_split,
            min_samples_per_class=min_samples_per_class,
            sampling_mode=sampling_mode,
            target_fps=target_fps,
            original_fps=original_fps
        )

        test_dataset = NeonatalMultilabelDataset(
            frames_dir=frames_dir,
            labels_file=labels_file,
            split='test',
            clip_len=clip_len,
            model_type=model_type,
            top_n_classes=top_n_classes,
            stratified_split=stratified_split,
            min_samples_per_class=min_samples_per_class,
            sampling_mode=sampling_mode,
            target_fps=target_fps,
            original_fps=original_fps
        )

        num_classes = train_dataset.get_num_classes()

    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}ã€‚æ”¯æŒçš„æ•°æ®é›†: cifar10, custom, ucf101, ucf101_video, neonatal_multilabel")

    # æŒ‰æ¯”ä¾‹éšæœºæŠ½æ ·æ•°æ®å­é›†ï¼ˆæ”¯æŒå¿«é€Ÿå®éªŒï¼‰
    if 0 < data_percentage < 1.0:
        def _sample_subset(dataset, split_name):
            total = len(dataset)
            sample_size = max(1, int(total * data_percentage))
            indices = torch.randperm(total)[:sample_size]
            # æ•°æ®å­é‡‡æ ·ä¿¡æ¯å°†åœ¨è®­ç»ƒå™¨ä¸­ç»Ÿä¸€æ˜¾ç¤º
            # if is_main_process():
            #     print(f"ğŸ“Š æ•°æ®å­é‡‡æ · - {split_name}: {total} -> {sample_size} æ ·æœ¬ (æ¯”ä¾‹: {data_percentage:.1%})")
            return Subset(dataset, indices)
        
        original_train_size = len(train_dataset)
        original_test_size = len(test_dataset)
        
        train_dataset = _sample_subset(train_dataset, "è®­ç»ƒé›†")
        test_dataset = _sample_subset(test_dataset, "æµ‹è¯•é›†")
        
        # æ•°æ®é‡‡æ ·ä¿¡æ¯å°†åœ¨è®­ç»ƒå™¨ä¸­ç»Ÿä¸€æ˜¾ç¤º
        # if is_main_process():
        #     print(f"ğŸ¯ æ•°æ®é‡‡æ ·å®Œæˆ - è®­ç»ƒé›†: {original_train_size} -> {len(train_dataset)}, æµ‹è¯•é›†: {original_test_size} -> {len(test_dataset)}")
    # else:
        # if is_main_process():
        #     print(f"ğŸ“Š ä½¿ç”¨å®Œæ•´æ•°æ®é›† - è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬, æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")

    # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦ä¸ºç©º
    train_size = len(train_dataset)
    test_size = len(test_dataset)

    if train_size == 0:
        raise ValueError(
            f"è®­ç»ƒé›†ä¸ºç©ºï¼è¯·æ£€æŸ¥ä»¥ä¸‹å¯èƒ½çš„åŸå› :\n"
            f"  1. data_percentageå‚æ•°è®¾ç½®è¿‡å° (å½“å‰: {data_percentage:.1%})\n"
            f"  2. æ•°æ®é›†è·¯å¾„ä¸æ­£ç¡®: {data_dir}\n"
            f"  3. æ•°æ®è¿‡æ»¤æ¡ä»¶è¿‡äºä¸¥æ ¼\n"
            f"  å»ºè®®: å¢å¤§data_percentageæˆ–æ£€æŸ¥æ•°æ®é›†é…ç½®"
        )

    if test_size == 0:
        raise ValueError(
            f"æµ‹è¯•é›†ä¸ºç©ºï¼è¯·æ£€æŸ¥ä»¥ä¸‹å¯èƒ½çš„åŸå› :\n"
            f"  1. data_percentageå‚æ•°è®¾ç½®è¿‡å° (å½“å‰: {data_percentage:.1%})\n"
            f"  2. æ•°æ®é›†è·¯å¾„ä¸æ­£ç¡®: {data_dir}\n"
            f"  3. æ•°æ®åˆ’åˆ†æ¯”ä¾‹ä¸åˆç†\n"
            f"  å»ºè®®: å¢å¤§data_percentageæˆ–æ£€æŸ¥æ•°æ®é›†é…ç½®"
        )

    # ğŸ”§ æ–°å¢ï¼šæ”¯æŒåŠ æƒéšæœºé‡‡æ ·ï¼ˆä»…ç”¨äºå¤šæ ‡ç­¾æ•°æ®é›†çš„è®­ç»ƒé›†ï¼‰
    use_weighted_sampling = kwargs.get('use_weighted_sampling', False)
    weighted_sampling_mode = kwargs.get('weighted_sampling_mode', 'inverse_frequency')

    train_sampler = None
    train_shuffle = True

    if use_weighted_sampling and dataset_name == "neonatal_multilabel":
        if is_main_process():
            print(f"\nğŸ¯ å¯ç”¨åŠ æƒéšæœºé‡‡æ · (æ¨¡å¼: {weighted_sampling_mode})")

        try:
            # è®¡ç®—æ ·æœ¬æƒé‡
            sample_weights = calculate_sample_weights(
                train_dataset,
                mode=weighted_sampling_mode,
                verbose=True
            )

            # åˆ›å»ºåŠ æƒé‡‡æ ·å™¨
            train_sampler = WeightedRandomSampler(
                weights=sample_weights,
                num_samples=len(sample_weights),
                replacement=True  # å…è®¸é‡å¤é‡‡æ ·
            )

            # ä½¿ç”¨sampleræ—¶ä¸èƒ½åŒæ—¶ä½¿ç”¨shuffle
            train_shuffle = False

            if is_main_process():
                print(f"âœ… åŠ æƒé‡‡æ ·å™¨åˆ›å»ºæˆåŠŸ")

        except Exception as e:
            if is_main_process():
                print(f"âš ï¸  åŠ æƒé‡‡æ ·å™¨åˆ›å»ºå¤±è´¥: {e}")
                print(f"   å›é€€åˆ°æ™®é€šéšæœºé‡‡æ ·")
            train_sampler = None
            train_shuffle = True

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=train_shuffle,
        sampler=train_sampler,
        num_workers=num_workers,
        pin_memory=True
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )

    return train_loader, test_loader, num_classes


def get_dataset_info(dataset_name):
    """
    è·å–æ•°æ®é›†åŸºæœ¬ä¿¡æ¯
    
    Args:
        dataset_name (str): æ•°æ®é›†åç§°ï¼Œæ”¯æŒ'cifar10'ã€'custom'æˆ–'ucf101'
        
    Returns:
        dict: åŒ…å«æ•°æ®é›†åç§°ã€ç±»åˆ«æ•°ã€è¾“å…¥å°ºå¯¸å’Œç±»åˆ«åˆ—è¡¨çš„å­—å…¸
        
    Raises:
        ValueError: å½“æŒ‡å®šçš„æ•°æ®é›†åç§°ä¸æ”¯æŒæ—¶
    """
    dataset_name = dataset_name.lower()
    
    if dataset_name == "cifar10":
        return {
            "name": "CIFAR-10",
            "num_classes": 10,
            "input_size": (3, 32, 32),
            "classes": ['airplane', 'automobile', 'bird', 'cat', 'deer', 
                       'dog', 'frog', 'horse', 'ship', 'truck']
        }
    elif dataset_name == "custom":
        return {
            "name": "Custom Dataset",
            "num_classes": None,  # éœ€è¦è¿è¡Œæ—¶ç¡®å®š
            "input_size": (3, 224, 224),  # é»˜è®¤å¤§å°
            "classes": None  # éœ€è¦è¿è¡Œæ—¶ç¡®å®š
        }
    elif dataset_name in ["ucf101", "ucf101_video"]:
        return {
            "name": "UCF-101 Video",
            "num_classes": 101,
            "input_size": (3, 16, 112, 112),  # (C, T, H, W)
            "classes": None  # éœ€è¦è¿è¡Œæ—¶ç¡®å®š
        }
    elif dataset_name == "neonatal_multilabel":
        # æ³¨æ„ï¼šå®é™…çš„ç±»åˆ«æ•°é‡å’Œç±»åˆ«åç§°éœ€è¦åœ¨è¿è¡Œæ—¶ä»æ•°æ®é›†å®ä¾‹è·å–
        return {
            "name": "Neonatal Multilabel Behavior Recognition",
            "num_classes": None,  # éœ€è¦è¿è¡Œæ—¶ç¡®å®šï¼ˆå–å†³äºtop_n_classeså‚æ•°ï¼‰
            "input_size": (3, 16, 112, 112),  # (C, T, H, W)
            "classes": None  # éœ€è¦è¿è¡Œæ—¶ç¡®å®šï¼ˆå–å†³äºç±»åˆ«ç­›é€‰ç»“æœï¼‰
        }
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}")
