"""
å¤šæ ‡ç­¾åˆ†ç±»è¯„ä¼°æŒ‡æ ‡æ¨¡å—

æä¾›è¯¦ç»†çš„å¤šæ ‡ç­¾åˆ†ç±»æ€§èƒ½è¯„ä¼°ï¼ŒåŒ…æ‹¬ï¼š
- æ¯ä¸ªç±»åˆ«çš„ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°
- ç±»åˆ«ä¸å¹³è¡¡åˆ†æ
- å®æ—¶è®­ç»ƒç›‘æ§
- ç»“æœä¿å­˜å’Œå¯è§†åŒ–
"""

import os
import json
import torch
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime
from sklearn.metrics import precision_score, recall_score, f1_score, classification_report


class MultilabelMetricsCalculator:
    """å¤šæ ‡ç­¾åˆ†ç±»æŒ‡æ ‡è®¡ç®—å™¨
    
    è´Ÿè´£è®¡ç®—å’Œç®¡ç†å¤šæ ‡ç­¾åˆ†ç±»çš„è¯¦ç»†æŒ‡æ ‡ï¼ŒåŒ…æ‹¬æ¯ä¸ªç±»åˆ«çš„æ€§èƒ½æŒ‡æ ‡ã€‚
    """
    
    def __init__(self, class_names: List[str], output_dir: str = "runs/neonatal"):
        """åˆå§‹åŒ–æŒ‡æ ‡è®¡ç®—å™¨
        
        Args:
            class_names: ç±»åˆ«åç§°åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
        """
        self.class_names = class_names
        self.num_classes = len(class_names)
        self.output_dir = output_dir
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # åˆå§‹åŒ–æœ€ä½³æŒ‡æ ‡è¿½è¸ª
        self.best_metrics = {
            'epoch': 0,
            'macro_avg_f1': 0.0,
            'class_metrics': {}
        }
        
        # å†å²æŒ‡æ ‡è®°å½•
        self.metrics_history = []
        
    def calculate_detailed_metrics(self, predictions: np.ndarray, targets: np.ndarray, 
                                 threshold: float = 0.5) -> Dict[str, Any]:
        """è®¡ç®—è¯¦ç»†çš„å¤šæ ‡ç­¾åˆ†ç±»æŒ‡æ ‡
        
        Args:
            predictions: æ¨¡å‹é¢„æµ‹æ¦‚ç‡ï¼Œå½¢çŠ¶ä¸º (N, num_classes)
            targets: çœŸå®æ ‡ç­¾ï¼Œå½¢çŠ¶ä¸º (N, num_classes)
            threshold: äºŒå€¼åŒ–é˜ˆå€¼
            
        Returns:
            åŒ…å«è¯¦ç»†æŒ‡æ ‡çš„å­—å…¸
        """
        # äºŒå€¼åŒ–é¢„æµ‹
        pred_binary = (predictions > threshold).astype(int)
        targets_binary = targets.astype(int)
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
        class_metrics = {}
        for i, class_name in enumerate(self.class_names):
            # æå–å½“å‰ç±»åˆ«çš„é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾
            class_pred = pred_binary[:, i]
            class_true = targets_binary[:, i]
            
            # è®¡ç®—åŸºæœ¬æŒ‡æ ‡
            precision = precision_score(class_true, class_pred, zero_division=0)
            recall = recall_score(class_true, class_pred, zero_division=0)
            f1 = f1_score(class_true, class_pred, zero_division=0)
            
            # è®¡ç®—æ ·æœ¬æ•°é‡
            pos_samples = int(np.sum(class_true))
            neg_samples = int(len(class_true) - pos_samples)
            
            # è®¡ç®—å‡†ç¡®ç‡ï¼ˆæ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹ï¼‰
            accuracy = np.mean(class_pred == class_true)
            
            class_metrics[class_name] = {
                'precision': float(precision),
                'recall': float(recall),
                'f1': float(f1),
                'accuracy': float(accuracy),
                'pos_samples': pos_samples,
                'neg_samples': neg_samples,
                'total_samples': pos_samples + neg_samples
            }
        
        # è®¡ç®—å®å¹³å‡æŒ‡æ ‡
        macro_precision = np.mean([m['precision'] for m in class_metrics.values()])
        macro_recall = np.mean([m['recall'] for m in class_metrics.values()])
        macro_f1 = np.mean([m['f1'] for m in class_metrics.values()])
        macro_accuracy = np.mean([m['accuracy'] for m in class_metrics.values()])
        
        # è®¡ç®—å¾®å¹³å‡æŒ‡æ ‡
        micro_precision = precision_score(targets_binary, pred_binary, average='micro', zero_division=0)
        micro_recall = recall_score(targets_binary, pred_binary, average='micro', zero_division=0)
        micro_f1 = f1_score(targets_binary, pred_binary, average='micro', zero_division=0)
        
        # è®¡ç®—åŠ æƒå¹³å‡æŒ‡æ ‡
        weighted_precision = precision_score(targets_binary, pred_binary, average='weighted', zero_division=0)
        weighted_recall = recall_score(targets_binary, pred_binary, average='weighted', zero_division=0)
        weighted_f1 = f1_score(targets_binary, pred_binary, average='weighted', zero_division=0)
        
        return {
            'class_metrics': class_metrics,
            'macro_avg': {
                'precision': float(macro_precision),
                'recall': float(macro_recall),
                'f1': float(macro_f1),
                'accuracy': float(macro_accuracy)
            },
            'micro_avg': {
                'precision': float(micro_precision),
                'recall': float(micro_recall),
                'f1': float(micro_f1)
            },
            'weighted_avg': {
                'precision': float(weighted_precision),
                'recall': float(weighted_recall),
                'f1': float(weighted_f1)
            },
            'threshold': threshold,
            'total_samples': len(targets)
        }
    
    def format_metrics_display(self, metrics: Dict[str, Any], epoch: int, 
                             val_loss: float, train_batches: int) -> str:
        """æ ¼å¼åŒ–æŒ‡æ ‡æ˜¾ç¤º
        
        Args:
            metrics: è¯¦ç»†æŒ‡æ ‡å­—å…¸
            epoch: å½“å‰epoch
            val_loss: éªŒè¯æŸå¤±
            train_batches: è®­ç»ƒæ‰¹æ¬¡æ•°
            
        Returns:
            æ ¼å¼åŒ–çš„æ˜¾ç¤ºå­—ç¬¦ä¸²
        """
        macro_acc = metrics['macro_avg']['accuracy'] * 100
        macro_f1 = metrics['macro_avg']['f1'] * 100
        
        # ä¸»è¦æŒ‡æ ‡è¡Œ
        main_line = (f"Epoch {epoch:03d} | val_loss={val_loss:.4f} | "
                    f"val_acc={macro_acc:.2f}% (å®å¹³å‡) | val_f1={macro_f1:.2f}% | "
                    f"train_batches={train_batches}")
        
        # è¯¦ç»†ç±»åˆ«æŒ‡æ ‡è¡¨æ ¼
        detail_lines = ["\nå„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡:"]
        detail_lines.append("ç±»åˆ«åç§°          ç²¾ç¡®ç‡   å¬å›ç‡   F1åˆ†æ•°   å‡†ç¡®ç‡   æ­£æ ·æœ¬   è´Ÿæ ·æœ¬")
        detail_lines.append("-" * 75)
        
        for class_name, class_metric in metrics['class_metrics'].items():
            line = (f"{class_name:<12} "
                   f"{class_metric['precision']:>7.3f}  "
                   f"{class_metric['recall']:>7.3f}  "
                   f"{class_metric['f1']:>7.3f}  "
                   f"{class_metric['accuracy']:>7.3f}  "
                   f"{class_metric['pos_samples']:>6d}  "
                   f"{class_metric['neg_samples']:>6d}")
            detail_lines.append(line)
        
        # å¹³å‡æŒ‡æ ‡æ±‡æ€»
        detail_lines.append("-" * 75)
        detail_lines.append(f"å®å¹³å‡           "
                           f"{metrics['macro_avg']['precision']:>7.3f}  "
                           f"{metrics['macro_avg']['recall']:>7.3f}  "
                           f"{metrics['macro_avg']['f1']:>7.3f}  "
                           f"{metrics['macro_avg']['accuracy']:>7.3f}  "
                           f"{'':>6s}  {'':>6s}")
        
        detail_lines.append(f"å¾®å¹³å‡           "
                           f"{metrics['micro_avg']['precision']:>7.3f}  "
                           f"{metrics['micro_avg']['recall']:>7.3f}  "
                           f"{metrics['micro_avg']['f1']:>7.3f}  "
                           f"{'':>7s}  {'':>6s}  {'':>6s}")
        
        return main_line + "\n" + "\n".join(detail_lines)
    
    def update_best_metrics(self, metrics: Dict[str, Any], epoch: int) -> bool:
        """æ›´æ–°æœ€ä½³æŒ‡æ ‡è®°å½•
        
        Args:
            metrics: å½“å‰æŒ‡æ ‡
            epoch: å½“å‰epoch
            
        Returns:
            æ˜¯å¦æ›´æ–°äº†æœ€ä½³æŒ‡æ ‡
        """
        current_f1 = metrics['macro_avg']['f1']
        
        if current_f1 > self.best_metrics['macro_avg_f1']:
            self.best_metrics = {
                'epoch': epoch,
                'macro_avg_f1': current_f1,
                'class_metrics': metrics['class_metrics'].copy(),
                'macro_avg': metrics['macro_avg'].copy(),
                'micro_avg': metrics['micro_avg'].copy(),
                'weighted_avg': metrics['weighted_avg'].copy()
            }
            return True
        return False
    
    def save_metrics(self, metrics: Dict[str, Any], epoch: int, 
                    val_loss: float, is_best: bool = False):
        """ä¿å­˜æŒ‡æ ‡åˆ°æ–‡ä»¶
        
        Args:
            metrics: æŒ‡æ ‡å­—å…¸
            epoch: å½“å‰epoch
            val_loss: éªŒè¯æŸå¤±
            is_best: æ˜¯å¦ä¸ºæœ€ä½³æŒ‡æ ‡
        """
        # æ·»åŠ åˆ°å†å²è®°å½•
        record = {
            'epoch': epoch,
            'val_loss': val_loss,
            'timestamp': datetime.now().isoformat(),
            'is_best': is_best,
            **metrics
        }
        self.metrics_history.append(record)
        
        # ä¿å­˜å†å²è®°å½•
        history_file = os.path.join(self.output_dir, 'metrics_history.json')
        with open(history_file, 'w', encoding='utf-8') as f:
            json.dump(self.metrics_history, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜æœ€ä½³æŒ‡æ ‡
        best_file = os.path.join(self.output_dir, 'best_metrics.json')
        with open(best_file, 'w', encoding='utf-8') as f:
            json.dump(self.best_metrics, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜å½“å‰epochçš„è¯¦ç»†æŒ‡æ ‡
        epoch_file = os.path.join(self.output_dir, f'epoch_{epoch:03d}_metrics.json')
        with open(epoch_file, 'w', encoding='utf-8') as f:
            json.dump(record, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜CSVæ ¼å¼çš„ç±»åˆ«æŒ‡æ ‡ï¼ˆä¾¿äºåˆ†æï¼‰
        self._save_class_metrics_csv(metrics, epoch)
    
    def _save_class_metrics_csv(self, metrics: Dict[str, Any], epoch: int):
        """ä¿å­˜ç±»åˆ«æŒ‡æ ‡åˆ°CSVæ–‡ä»¶"""
        csv_file = os.path.join(self.output_dir, 'class_metrics_history.csv')
        
        # å‡†å¤‡æ•°æ®
        rows = []
        for class_name, class_metric in metrics['class_metrics'].items():
            row = {
                'epoch': epoch,
                'class_name': class_name,
                'precision': class_metric['precision'],
                'recall': class_metric['recall'],
                'f1': class_metric['f1'],
                'accuracy': class_metric['accuracy'],
                'pos_samples': class_metric['pos_samples'],
                'neg_samples': class_metric['neg_samples']
            }
            rows.append(row)
        
        # åˆ›å»ºDataFrame
        df = pd.DataFrame(rows)
        
        # è¿½åŠ åˆ°CSVæ–‡ä»¶
        if os.path.exists(csv_file):
            df.to_csv(csv_file, mode='a', header=False, index=False, encoding='utf-8')
        else:
            df.to_csv(csv_file, index=False, encoding='utf-8')
    
    def get_summary_report(self) -> str:
        """è·å–è®­ç»ƒæ€»ç»“æŠ¥å‘Š"""
        if not self.best_metrics['class_metrics']:
            return "æš‚æ— æœ€ä½³æŒ‡æ ‡è®°å½•"
        
        lines = [
            f"\nğŸ† è®­ç»ƒæ€»ç»“æŠ¥å‘Š (æœ€ä½³epoch: {self.best_metrics['epoch']})",
            "=" * 80,
            f"æœ€ä½³å®å¹³å‡F1åˆ†æ•°: {self.best_metrics['macro_avg_f1']:.4f}",
            f"æœ€ä½³å®å¹³å‡å‡†ç¡®ç‡: {self.best_metrics['macro_avg']['accuracy']:.4f}",
            "",
            "å„ç±»åˆ«æœ€ä½³æŒ‡æ ‡:",
            "ç±»åˆ«åç§°          ç²¾ç¡®ç‡   å¬å›ç‡   F1åˆ†æ•°   å‡†ç¡®ç‡   æ­£æ ·æœ¬   è´Ÿæ ·æœ¬",
            "-" * 75
        ]
        
        for class_name, class_metric in self.best_metrics['class_metrics'].items():
            line = (f"{class_name:<12} "
                   f"{class_metric['precision']:>7.3f}  "
                   f"{class_metric['recall']:>7.3f}  "
                   f"{class_metric['f1']:>7.3f}  "
                   f"{class_metric['accuracy']:>7.3f}  "
                   f"{class_metric['pos_samples']:>6d}  "
                   f"{class_metric['neg_samples']:>6d}")
            lines.append(line)
        
        lines.extend([
            "-" * 75,
            f"ğŸ“Š æŒ‡æ ‡æ–‡ä»¶ä¿å­˜ä½ç½®: {self.output_dir}",
            f"ğŸ“ˆ å†å²è®°å½•: {len(self.metrics_history)} ä¸ªepoch",
            "=" * 80
        ])
        
        return "\n".join(lines)
