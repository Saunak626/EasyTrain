"""è®­ç»ƒæ—¥å¿—ç®¡ç†å™¨

ç»Ÿä¸€ç®¡ç†è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ‰€æœ‰ç»ˆç«¯è¾“å‡ºå’Œæ—¥å¿—è®°å½•ã€‚
"""

from typing import Dict, Any, Optional
from accelerate import Accelerator
from tqdm import tqdm


class TrainingLogger:
    """ç»Ÿä¸€çš„è®­ç»ƒæ—¥å¿—ç®¡ç†å™¨
    
    è´Ÿè´£ç®¡ç†è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ‰€æœ‰ç»ˆç«¯è¾“å‡ºï¼Œæ”¯æŒæ—¥å¿—çº§åˆ«æ§åˆ¶å’Œç®€æ´/è¯¦ç»†æ¨¡å¼åˆ‡æ¢ã€‚
    
    ç‰¹æ€§ï¼š
    - æ”¯æŒç®€æ´æ¨¡å¼å’Œè¯¦ç»†æ¨¡å¼
    - è‡ªåŠ¨å¤„ç† accelerator.is_main_process æ£€æŸ¥
    - ç»Ÿä¸€çš„æ—¥å¿—æ ¼å¼
    """
    
    def __init__(self, accelerator: Accelerator, verbose: bool = False):
        """åˆå§‹åŒ–æ—¥å¿—ç®¡ç†å™¨
        
        Args:
            accelerator: Acceleratorå®ä¾‹
            verbose: æ˜¯å¦å¯ç”¨è¯¦ç»†æ¨¡å¼ï¼ˆé»˜è®¤Falseï¼Œä½¿ç”¨ç®€æ´æ¨¡å¼ï¼‰
        """
        self.accelerator = accelerator
        self.verbose = verbose
    
    def info(self, message: str, force: bool = False):
        """æ‰“å°INFOçº§åˆ«æ—¥å¿—ï¼ˆå§‹ç»ˆæ˜¾ç¤ºï¼‰
        
        Args:
            message: æ—¥å¿—æ¶ˆæ¯
            force: æ˜¯å¦å¼ºåˆ¶æ‰“å°ï¼ˆå¿½ç•¥ä¸»è¿›ç¨‹æ£€æŸ¥ï¼‰
        """
        if force or self.accelerator.is_main_process:
            tqdm.write(message)
    
    def debug(self, message: str):
        """æ‰“å°DEBUGçº§åˆ«æ—¥å¿—ï¼ˆåªåœ¨è¯¦ç»†æ¨¡å¼ä¸‹æ˜¾ç¤ºï¼‰
        
        Args:
            message: æ—¥å¿—æ¶ˆæ¯
        """
        if self.verbose and self.accelerator.is_main_process:
            tqdm.write(message)
    
    def print_experiment_config(self, config: Dict[str, Any]):
        """æ‰“å°å®éªŒé…ç½®ï¼ˆæ ¹æ®æ¨¡å¼é€‰æ‹©è¯¦ç»†ç¨‹åº¦ï¼‰
        
        Args:
            config: å®éªŒé…ç½®å­—å…¸
        """
        if self.verbose:
            self._print_detailed_config(config)
        else:
            self._print_compact_config(config)
    
    def _print_compact_config(self, config: Dict[str, Any]):
        """æ‰“å°ç®€æ´çš„å®éªŒé…ç½®ï¼ˆ3-4 è¡Œï¼‰
        
        Args:
            config: å®éªŒé…ç½®å­—å…¸
        """
        if not self.accelerator.is_main_process:
            return
        
        # ç¬¬1è¡Œï¼šå®éªŒåç§°ã€æ¨¡å‹ã€æ•°æ®é›†
        model_params = config.get('model_params_m', 0)
        model_size_mb = config.get('model_size_mb', 0)
        self.info(f"ğŸš€ å®éªŒ: {config['exp_name']} | "
                  f"æ¨¡å‹: {config['model_name']} ({model_params:.1f}M, {model_size_mb:.1f}MB) | "
                  f"æ•°æ®: {config['dataset_type']}")
        
        # ç¬¬2è¡Œï¼šè®­ç»ƒé…ç½®
        train_size = config.get('train_size', 0)
        test_size = config.get('test_size', 0)
        data_pct = config.get('data_percentage', 1.0)
        self.info(f"ğŸ“Š æ•°æ®: è®­ç»ƒ{train_size:,} | æµ‹è¯•{test_size:,} | ä½¿ç”¨{data_pct:.0%} | "
                  f"é…ç½®: {config['epochs']}epÃ—bs{config['batch_size']}Ã—lr{config['learning_rate']}")
        
        # ç¬¬3è¡Œï¼šä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        scheduler_info = config.get('scheduler_info', 'default')
        optimizer_name = config.get('optimizer_name', 'adam')
        weight_decay = config.get('weight_decay', 0)
        self.info(f"âš™ï¸  ä¼˜åŒ–: {optimizer_name}(wd={weight_decay}) | è°ƒåº¦: {scheduler_info} | "
                  f"å¤šå¡: {'æ˜¯' if self.accelerator.num_processes > 1 else 'å¦'}")
        
        self.info("â•" * 80)
    
    def _print_detailed_config(self, config: Dict[str, Any]):
        """æ‰“å°è¯¦ç»†çš„å®éªŒé…ç½®ï¼ˆåŸæœ‰çš„å®Œæ•´æ‰“å°ï¼‰
        
        Args:
            config: å®éªŒé…ç½®å­—å…¸
        """
        if not self.accelerator.is_main_process:
            return
        
        self.info(f"ğŸš€ ========== è®­ç»ƒå®éªŒå¼€å§‹ ==========")
        self.info(f"ğŸ“‹ å®éªŒé…ç½®:")
        self.info(f"  â””â”€ å®éªŒåç§°: {config['exp_name']}")
        self.info(f"  â””â”€ ä»»åŠ¡ç±»å‹: {config.get('task_description', 'Unknown')} ({config['dataset_type'].upper()})")
        
        # æ¨¡å‹ä¿¡æ¯
        model_params = config.get('model_params_m', 0)
        model_size_mb = config.get('model_size_mb', 0)
        self.info(f"  â””â”€ æ¨¡å‹æ¶æ„: {config['model_name']} ({model_params:.1f}Må‚æ•°, {model_size_mb:.1f}MB)")
        
        # æ•°æ®é…ç½®
        train_size = config.get('train_size', 0)
        test_size = config.get('test_size', 0)
        data_pct = config.get('data_percentage', 1.0)
        self.info(f"  â””â”€ æ•°æ®é…ç½®: è®­ç»ƒé›† {train_size:,} | æµ‹è¯•é›† {test_size:,} | ä½¿ç”¨æ¯”ä¾‹ {data_pct:.0%}")
        
        # è®­ç»ƒé…ç½®
        self.info(f"  â””â”€ è®­ç»ƒé…ç½®: {config['epochs']} epochs | batch_size {config['batch_size']} | åˆå§‹LR {config['learning_rate']}")
        
        # è°ƒåº¦å™¨ä¿¡æ¯
        scheduler_info = config.get('scheduler_info', 'default')
        self.info(f"  â””â”€ è°ƒåº¦ç­–ç•¥: {scheduler_info}")
        
        # ä¼˜åŒ–å™¨ä¿¡æ¯
        optimizer_name = config.get('optimizer_name', 'adam')
        weight_decay = config.get('weight_decay', 0)
        self.info(f"  â””â”€ ä¼˜åŒ–å™¨é…ç½®: {optimizer_name} (weight_decay={weight_decay})")
        self.info(f"  â””â”€ å¤šå¡è®­ç»ƒ: {'æ˜¯' if self.accelerator.num_processes > 1 else 'å¦'}")
        
        self.info("â•" * 63)
    
    def print_pos_weight_summary(self, total_samples: int, num_classes: int):
        """æ‰“å°pos_weightè®¡ç®—æ‘˜è¦
        
        Args:
            total_samples: æ€»æ ·æœ¬æ•°
            num_classes: ç±»åˆ«æ•°
        """
        if self.accelerator.is_main_process:
            self.info(f"âœ… pos_weightå·²è®¡ç®— (åŸºäº{total_samples:,}ä¸ªæ ·æœ¬ï¼Œ{num_classes}ä¸ªç±»åˆ«)")
    
    def print_pos_weight_details(self, pos_weight, pos_counts, neg_counts, 
                                 raw_ratio, scale_factor, class_names=None):
        """æ‰“å°pos_weightè¯¦ç»†ä¿¡æ¯ï¼ˆDEBUGçº§åˆ«ï¼‰
        
        Args:
            pos_weight: pos_weightå¼ é‡
            pos_counts: æ­£æ ·æœ¬è®¡æ•°
            neg_counts: è´Ÿæ ·æœ¬è®¡æ•°
            raw_ratio: åŸå§‹æ¯”ä¾‹
            scale_factor: ç¼©æ”¾å› å­
            class_names: ç±»åˆ«åç§°åˆ—è¡¨
        """
        if not self.verbose or not self.accelerator.is_main_process:
            return
        
        num_classes = len(pos_weight)
        for i in range(num_classes):
            class_name = class_names[i] if class_names else f"ç±»åˆ«{i}"
            scale = scale_factor[i].item() if hasattr(scale_factor, 'item') else scale_factor
            self.debug(f"   {class_name}: pos={int(pos_counts[i])}, neg={int(neg_counts[i])}, "
                      f"ratio={raw_ratio[i]:.2f}, scale={scale:.1f}, pos_weight={pos_weight[i]:.2f}")

