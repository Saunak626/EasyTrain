"""å®éªŒç»“æœç®¡ç†å™¨

è´Ÿè´£ç®¡ç†ç½‘æ ¼æœç´¢çš„ç»“æœæ–‡ä»¶ï¼ŒåŒ…æ‹¬ä¸»ç»“æœCSVã€è¯¦æƒ…CSVå’Œå•å®éªŒæ–‡ä»¶ã€‚
"""

import csv
import json
import fcntl
import os
from typing import Dict, List, Any

from .grid_search_generator import GROUP_KEY


# ======================
# CSV é…ç½®å¸¸é‡
# ======================

# CSV åŸºç¡€åˆ—ï¼ˆåŒ…å«æ‰€æœ‰å¯èƒ½çš„æŒ‡æ ‡åˆ—ï¼‰
CSV_BASE_COLUMNS = [
    'exp_name', 'model.type', 'group', 'success', 'trained_epochs',
    # ğŸ¯ å¤šæ ‡ç­¾åˆ†ç±»å…³é”®æŒ‡æ ‡ï¼ˆä¼˜å…ˆæ˜¾ç¤ºï¼‰
    'best_weighted_f1', 'best_weighted_accuracy', 'best_macro_accuracy', 'best_micro_accuracy',
    'best_macro_f1', 'best_micro_f1', 'best_macro_precision', 'best_macro_recall',
    'final_weighted_f1', 'final_weighted_accuracy', 'final_macro_accuracy', 'final_micro_accuracy',
    'final_macro_f1', 'final_micro_f1',
    # ä¼ ç»Ÿå­—æ®µï¼ˆå‘åå…¼å®¹ï¼‰
    'best_accuracy', 'final_accuracy'
]

# å¸¸è§è¿è¡Œæ—¶å‚æ•°ï¼ˆä¼šæ·»åŠ åˆ° CSV åˆ—ä¸­ï¼‰
COMMON_RUNTIME_PARAMS = [
    'data_percentage',
    'optimizer.name', 'scheduler.name', 'loss.name'
]

# CSV ä¸­æ’é™¤çš„å‚æ•°ï¼ˆä¸æ˜¾ç¤ºåœ¨ CSV ä¸­ï¼‰
EXCLUDED_CSV_PARAMS = ['epochs', 'batch_size', 'learning_rate']


# ======================
# å®éªŒç»“æœç®¡ç†å™¨ç±»
# ======================

class ExperimentResultsManager:
    """å®éªŒç»“æœç®¡ç†å™¨

    è´Ÿè´£ç®¡ç†ç½‘æ ¼æœç´¢çš„ç»“æœæ–‡ä»¶ï¼ŒåŒ…æ‹¬ä¸»ç»“æœCSVã€è¯¦æƒ…CSVå’Œå•å®éªŒæ–‡ä»¶ã€‚
    """

    def __init__(self, csv_filepath: str, details_filepath: str, grid_search_dir: str):
        """åˆå§‹åŒ–ç»“æœç®¡ç†å™¨

        Args:
            csv_filepath: ä¸»ç»“æœCSVæ–‡ä»¶è·¯å¾„
            details_filepath: è¯¦æƒ…CSVæ–‡ä»¶è·¯å¾„
            grid_search_dir: ç½‘æ ¼æœç´¢ç›®å½•
        """
        self.csv_filepath = csv_filepath
        self.details_filepath = details_filepath
        self.grid_search_dir = grid_search_dir
        self.experiments_dir = os.path.join(grid_search_dir, "experiments")
        self.fieldnames = []

        # åˆ›å»ºå®éªŒç›®å½•
        os.makedirs(self.experiments_dir, exist_ok=True)
        print(f"ğŸ“ åˆ›å»ºå®éªŒæ–‡ä»¶å¤¹ç»“æ„: {self.experiments_dir}")

    def initialize_csv_file(self, fieldnames: List[str]) -> None:
        """åˆå§‹åŒ–CSVæ–‡ä»¶

        Args:
            fieldnames: CSVå­—æ®µååˆ—è¡¨
        """
        self.fieldnames = fieldnames

        # åˆå§‹åŒ–ä¸»ç»“æœCSV
        os.makedirs(os.path.dirname(self.csv_filepath), exist_ok=True)
        with open(self.csv_filepath, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()

        # åˆå§‹åŒ–è¯¦æƒ…CSV
        print(f"ğŸ“‹ åˆå§‹åŒ–è¯¦æƒ…è¡¨: {self.details_filepath}")
        with open(self.details_filepath, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()

    def append_result_to_csv(self, result: Dict[str, Any]) -> None:
        """è¿½åŠ ç»“æœåˆ°CSVæ–‡ä»¶

        Args:
            result: å®éªŒç»“æœå­—å…¸
        """
        # å†™å…¥ä¸»ç»“æœCSV
        with open(self.csv_filepath, 'a', newline='', encoding='utf-8') as f:
            fcntl.flock(f.fileno(), fcntl.LOCK_EX)
            try:
                writer = csv.DictWriter(f, fieldnames=self.fieldnames)
                row = self._prepare_csv_row(result)
                writer.writerow(row)
            finally:
                fcntl.flock(f.fileno(), fcntl.LOCK_UN)

        # å†™å…¥è¯¦æƒ…CSV
        with open(self.details_filepath, 'a', newline='', encoding='utf-8') as f:
            fcntl.flock(f.fileno(), fcntl.LOCK_EX)
            try:
                writer = csv.DictWriter(f, fieldnames=self.fieldnames)
                writer.writerow(row)
            finally:
                fcntl.flock(f.fileno(), fcntl.LOCK_UN)

        # ä¿å­˜å•å®éªŒJSONæ–‡ä»¶
        self._save_single_experiment_file(result)

    def _prepare_csv_row(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """å‡†å¤‡CSVè¡Œæ•°æ®

        Args:
            result: å®éªŒç»“æœå­—å…¸

        Returns:
            CSVè¡Œå­—å…¸
        """
        row = {}
        params = result.get("params", {})

        for field in self.fieldnames:
            if field in result:
                row[field] = result[field]
            elif field in params:
                row[field] = params[field]
            else:
                row[field] = ""

        return row

    def _save_single_experiment_file(self, result: Dict[str, Any]) -> None:
        """ä¿å­˜å•ä¸ªå®éªŒçš„JSONæ–‡ä»¶

        Args:
            result: å®éªŒç»“æœå­—å…¸
        """
        exp_name = result.get("exp_name", "unknown")
        exp_filepath = os.path.join(self.experiments_dir, f"{exp_name}.json")

        with open(exp_filepath, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)


# ======================
# CSVå­—æ®µåç”Ÿæˆå‡½æ•°
# ======================

def get_csv_fieldnames(all_params: List[Dict[str, Any]]) -> List[str]:
    """è·å–CSVæ–‡ä»¶çš„å­—æ®µååˆ—è¡¨

    Args:
        all_params: æ‰€æœ‰å‚æ•°ç»„åˆåˆ—è¡¨

    Returns:
        CSVå­—æ®µååˆ—è¡¨
    """
    # æ”¶é›†æ‰€æœ‰å‚æ•°é”®
    param_keys = set()
    for params in all_params:
        param_keys.update(params.keys())

    # æ’é™¤ä¸éœ€è¦åœ¨CSVä¸­æ˜¾ç¤ºçš„å‚æ•°
    param_keys = {k for k in param_keys if k not in EXCLUDED_CSV_PARAMS}

    # ç»„åˆå­—æ®µåï¼šåŸºç¡€åˆ— + è¿è¡Œæ—¶å‚æ•° + å…¶ä»–å‚æ•°
    fieldnames = CSV_BASE_COLUMNS.copy()

    # æ·»åŠ å¸¸è§è¿è¡Œæ—¶å‚æ•°
    for param in COMMON_RUNTIME_PARAMS:
        if param not in fieldnames:
            fieldnames.append(param)

    # æ·»åŠ å…¶ä»–å‚æ•°
    for key in sorted(param_keys):
        if key not in fieldnames and key != GROUP_KEY:
            fieldnames.append(key)

    return fieldnames

