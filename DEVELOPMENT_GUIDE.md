# è‡ªå®šä¹‰ç»„ä»¶å¼€å‘æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›äº†å¦‚ä½•æ‰©å±•å’Œè‡ªå®šä¹‰è®­ç»ƒæ¡†æ¶å„ä¸ªç»„ä»¶çš„è¯¦ç»†æŒ‡å—ï¼ŒåŒ…æ‹¬æ•°æ®é›†ã€æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨å’ŒæŸå¤±å‡½æ•°ã€‚

## ğŸ—ï¸ é¡¹ç›®æ¶æ„

### æ ¸å¿ƒè®¾è®¡ç†å¿µ

æœ¬è®­ç»ƒæ¡†æ¶é‡‡ç”¨**æ¨¡å—åŒ–è®¾è®¡**ï¼Œæ¯ä¸ªç»„ä»¶éƒ½æ˜¯ç‹¬ç«‹çš„ã€å¯æ›¿æ¢çš„æ¨¡å—ï¼š

```
è®­ç»ƒæµç¨‹:
é…ç½®è§£æ â†’ æ•°æ®åŠ è½½ â†’ æ¨¡å‹æ„å»º â†’ è®­ç»ƒå™¨æ‰§è¡Œ â†’ ç»“æœè®°å½•
    â†“           â†“         â†“         â†“         â†“
 config/    data_preprocessing/  models/  trainers/  utils/
```

### ç»„ä»¶ä¾èµ–å…³ç³»

```
scripts/train.py (å…¥å£)
    â”œâ”€â”€ utils/config_parser.py (é…ç½®è§£æ)
    â”œâ”€â”€ data_preprocessing/ (æ•°æ®åŠ è½½)
    â”‚   â”œâ”€â”€ cifar10_dataset.py
    â”‚   â””â”€â”€ dataset.py
    â”œâ”€â”€ models/net.py (æ¨¡å‹æ„å»º)
    â”œâ”€â”€ optimizers/optim.py (ä¼˜åŒ–å™¨)
    â”œâ”€â”€ schedules/scheduler.py (è°ƒåº¦å™¨)
    â”œâ”€â”€ losses/image_loss.py (æŸå¤±å‡½æ•°)
    â””â”€â”€ trainers/base_trainer.py (è®­ç»ƒæ‰§è¡Œ)
```

### æ¥å£è®¾è®¡åŸåˆ™

1. **ç»Ÿä¸€çš„å·¥å‚å‡½æ•°**ï¼šæ¯ä¸ªç»„ä»¶éƒ½æœ‰ `get_xxx()` å·¥å‚å‡½æ•°
2. **é…ç½®é©±åŠ¨**ï¼šæ‰€æœ‰ç»„ä»¶éƒ½é€šè¿‡é…ç½®æ–‡ä»¶å‚æ•°åŒ–
3. **å‘åå…¼å®¹**ï¼šæ–°å¢ç»„ä»¶ä¸å½±å“ç°æœ‰åŠŸèƒ½
4. **é”™è¯¯å¤„ç†**ï¼šæä¾›æ¸…æ™°çš„é”™è¯¯ä¿¡æ¯å’Œé™çº§æœºåˆ¶

## ğŸ—‚ï¸ è‡ªå®šä¹‰æ•°æ®é›†

### 1. æ•°æ®é›†å‡†å¤‡

#### æ–¹å¼1: ç›®å½•ç»“æ„åˆ†ç±»ï¼ˆæ¨èï¼‰
```
data/raw/my_dataset/
â”œâ”€â”€ class1/
â”‚   â”œâ”€â”€ img001.jpg
â”‚   â””â”€â”€ img002.jpg
â”œâ”€â”€ class2/
â”‚   â”œâ”€â”€ img003.jpg
â”‚   â””â”€â”€ img004.jpg
â””â”€â”€ class3/
    â””â”€â”€ img005.jpg
```

#### æ–¹å¼2: CSVæ–‡ä»¶ç´¢å¼•
```csv
image_path,label
class1/img001.jpg,0
class2/img003.jpg,1
class3/img005.jpg,2
```

### 2. æ‰©å±•CustomDatasetç±»

```python
# src/data_preprocessing/my_dataset.py
from .dataset import CustomDataset
from PIL import Image
import torch

class MyCustomDataset(CustomDataset):
    """è‡ªå®šä¹‰æ•°æ®é›†ç±»ç¤ºä¾‹"""

    def __init__(self, data_dir, transform=None, **kwargs):
        super().__init__(data_dir, transform=transform, **kwargs)
        # æ·»åŠ è‡ªå®šä¹‰åˆå§‹åŒ–é€»è¾‘
        self.custom_preprocessing = kwargs.get('custom_preprocessing', None)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        label = self.labels[idx]

        # è‡ªå®šä¹‰å›¾åƒåŠ è½½é€»è¾‘
        image = Image.open(img_path).convert('RGB')

        # è‡ªå®šä¹‰é¢„å¤„ç†
        if self.custom_preprocessing:
            image = self.custom_preprocessing(image)

        if self.transform:
            image = self.transform(image)

        return image, label

    def get_class_weights(self):
        """è®¡ç®—ç±»åˆ«æƒé‡ï¼Œç”¨äºå¤„ç†ä¸å¹³è¡¡æ•°æ®é›†"""
        from collections import Counter
        label_counts = Counter(self.labels)
        total = len(self.labels)
        weights = {cls: total / count for cls, count in label_counts.items()}
        return weights
```

### 3. åˆ›å»ºä¸“ç”¨æ•°æ®é›†æ¨¡å—

```python
# src/data_preprocessing/my_special_dataset.py
import torch
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
import pandas as pd
import numpy as np

class MySpecialDataset(Dataset):
    """å®Œå…¨è‡ªå®šä¹‰çš„æ•°æ®é›†ç±»"""

    def __init__(self, data_path, transform=None, mode='train'):
        self.data_path = data_path
        self.transform = transform
        self.mode = mode
        # åŠ è½½æ•°æ®ç´¢å¼•
        self._load_data()

    def _load_data(self):
        """åŠ è½½æ•°æ®ç´¢å¼•å’Œæ ‡ç­¾"""
        # ç¤ºä¾‹ï¼šä»CSVæ–‡ä»¶åŠ è½½
        csv_file = f"{self.data_path}/{self.mode}.csv"
        self.data_df = pd.read_csv(csv_file)
        self.image_paths = self.data_df['image_path'].tolist()
        self.labels = self.data_df['label'].tolist()
        
        # ç±»åˆ«æ˜ å°„
        self.class_to_idx = {cls: idx for idx, cls in enumerate(set(self.labels))}
        self.labels = [self.class_to_idx[label] for label in self.labels]

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = os.path.join(self.data_path, self.image_paths[idx])
        label = self.labels[idx]
        
        # åŠ è½½å›¾åƒ
        image = Image.open(img_path).convert('RGB')
        
        if self.transform:
            image = self.transform(image)
            
        return image, label

def create_my_special_dataloaders(data_path, batch_size=32, image_size=224, **kwargs):
    """åˆ›å»ºä¸“ç”¨æ•°æ®åŠ è½½å™¨"""
    # æ•°æ®å¢å¼º
    train_transform = transforms.Compose([
        transforms.Resize((image_size, image_size)),
        transforms.RandomHorizontalFlip(0.5),
        transforms.RandomRotation(10),
        transforms.ColorJitter(brightness=0.2, contrast=0.2),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    val_transform = transforms.Compose([
        transforms.Resize((image_size, image_size)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # åˆ›å»ºæ•°æ®é›†
    train_dataset = MySpecialDataset(data_path, transform=train_transform, mode='train')
    val_dataset = MySpecialDataset(data_path, transform=val_transform, mode='val')
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True,
        num_workers=kwargs.get('num_workers', 4),
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=batch_size, 
        shuffle=False,
        num_workers=kwargs.get('num_workers', 4),
        pin_memory=True
    )

    return train_loader, val_loader
```

## ğŸ§  è‡ªå®šä¹‰æ¨¡å‹

### 1. æ·»åŠ æ–°æ¨¡å‹åˆ°ç°æœ‰æ¨¡å—

```python
# åœ¨ src/models/net.py ä¸­æ·»åŠ 
import torch.nn as nn
import torch.nn.functional as F

class MyCustomModel(nn.Module):
    """è‡ªå®šä¹‰æ¨¡å‹"""

    def __init__(self, num_classes=10, input_channels=3, **kwargs):
        super(MyCustomModel, self).__init__()
        self.num_classes = num_classes
        
        # å®šä¹‰ç½‘ç»œå±‚
        self.features = nn.Sequential(
            nn.Conv2d(input_channels, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            nn.AdaptiveAvgPool2d((1, 1))
        )
        
        self.classifier = nn.Sequential(
            nn.Dropout(kwargs.get('dropout', 0.5)),
            nn.Linear(128, num_classes)
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

    def get_feature_maps(self, x):
        """è·å–ç‰¹å¾å›¾ï¼Œç”¨äºå¯è§†åŒ–"""
        features = []
        for layer in self.features:
            x = layer(x)
            if isinstance(layer, nn.Conv2d):
                features.append(x)
        return features

# æ›´æ–°get_modelå‡½æ•°
def get_model(model_name="simple_cnn", **kwargs):
    model_name = model_name.lower()

    if model_name == "my_custom_model":
        return MyCustomModel(**kwargs)
    # ... å…¶ä»–æ¨¡å‹
    else:
        raise ValueError(f"Unknown model: {model_name}")
```

### 2. åˆ›å»ºç‹¬ç«‹çš„æ¨¡å‹æ¨¡å—

```python
# src/models/my_models.py
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision.models import resnet18

class AdvancedCNN(nn.Module):
    """é«˜çº§CNNæ¨¡å‹"""

    def __init__(self, num_classes=10, dropout=0.5):
        super(AdvancedCNN, self).__init__()
        
        # ä½¿ç”¨æ®‹å·®å—
        self.conv1 = self._make_conv_block(3, 64)
        self.conv2 = self._make_conv_block(64, 128)
        self.conv3 = self._make_conv_block(128, 256)
        
        # æ³¨æ„åŠ›æœºåˆ¶
        self.attention = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(256, 256//16, 1),
            nn.ReLU(),
            nn.Conv2d(256//16, 256, 1),
            nn.Sigmoid()
        )
        
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Dropout(dropout),
            nn.Linear(256, num_classes)
        )

    def _make_conv_block(self, in_channels, out_channels):
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, 3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2)
        )

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        
        # åº”ç”¨æ³¨æ„åŠ›
        attention = self.attention(x)
        x = x * attention
        
        x = self.classifier(x)
        return x

class TransformerModel(nn.Module):
    """åŸºäºTransformerçš„æ¨¡å‹"""

    def __init__(self, num_classes=10, d_model=512, nhead=8, num_layers=6, **kwargs):
        super(TransformerModel, self).__init__()
        
        # å›¾åƒåˆ°åºåˆ—çš„è½¬æ¢
        self.patch_embed = nn.Conv2d(3, d_model, kernel_size=16, stride=16)
        
        # ä½ç½®ç¼–ç 
        self.pos_embed = nn.Parameter(torch.randn(1, 196, d_model))
        
        # Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, 
            nhead=nhead,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Linear(d_model, num_classes)
        
    def forward(self, x):
        # å›¾åƒåˆ†å—
        x = self.patch_embed(x)  # [B, d_model, H/16, W/16]
        x = x.flatten(2).transpose(1, 2)  # [B, num_patches, d_model]
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        x = x + self.pos_embed
        
        # Transformerç¼–ç 
        x = self.transformer(x)
        
        # å…¨å±€å¹³å‡æ± åŒ– + åˆ†ç±»
        x = x.mean(dim=1)
        x = self.classifier(x)
        
        return x
```

## âš™ï¸ è‡ªå®šä¹‰ä¼˜åŒ–å™¨

### 1. æ‰©å±•ç°æœ‰ä¼˜åŒ–å™¨æ¨¡å—

```python
# åœ¨ src/optimizers/optim.py ä¸­æ·»åŠ 
import torch.optim as optim
import math

def get_optimizer(model, optimizer_name, learning_rate, **kwargs):
    optimizer_name = optimizer_name.lower()

    if optimizer_name == "my_optimizer":
        return MyCustomOptimizer(
            model.parameters(),
            lr=learning_rate,
            **kwargs
        )
    elif optimizer_name == "lion":
        return LionOptimizer(
            model.parameters(),
            lr=learning_rate,
            **kwargs
        )
    # ... å…¶ä»–ä¼˜åŒ–å™¨
    else:
        raise ValueError(f"Unknown optimizer: {optimizer_name}")

class MyCustomOptimizer(optim.Optimizer):
    """è‡ªå®šä¹‰ä¼˜åŒ–å™¨ç¤ºä¾‹"""

    def __init__(self, params, lr=1e-3, momentum=0.9, weight_decay=0, **kwargs):
        defaults = dict(lr=lr, momentum=momentum, weight_decay=weight_decay)
        super(MyCustomOptimizer, self).__init__(params, defaults)

    def step(self, closure=None):
        loss = None
        if closure is not None:
            loss = closure()

        for group in self.param_groups:
            weight_decay = group['weight_decay']
            momentum = group['momentum']

            for p in group['params']:
                if p.grad is None:
                    continue
                    
                grad = p.grad.data
                if weight_decay != 0:
                    grad = grad.add(p.data, alpha=weight_decay)

                param_state = self.state[p]

                if len(param_state) == 0:
                    param_state['momentum_buffer'] = torch.zeros_like(p.data)

                buf = param_state['momentum_buffer']
                buf.mul_(momentum).add_(grad)

                p.data.add_(buf, alpha=-group['lr'])

        return loss

class LionOptimizer(optim.Optimizer):
    """Lionä¼˜åŒ–å™¨å®ç°"""
    
    def __init__(self, params, lr=1e-4, betas=(0.9, 0.99), weight_decay=0.0):
        defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)
        super(LionOptimizer, self).__init__(params, defaults)

    def step(self, closure=None):
        loss = None
        if closure is not None:
            loss = closure()

        for group in self.param_groups:
            for p in group['params']:
                if p.grad is None:
                    continue

                grad = p.grad.data
                if group['weight_decay'] != 0:
                    grad = grad.add(p.data, alpha=group['weight_decay'])

                param_state = self.state[p]
                if len(param_state) == 0:
                    param_state['exp_avg'] = torch.zeros_like(p.data)

                exp_avg = param_state['exp_avg']
                beta1, beta2 = group['betas']

                # Lion update
                update = exp_avg * beta1 + grad * (1 - beta1)
                p.data.mul_(1 - group['lr'] * group['weight_decay'])
                p.data.add_(torch.sign(update), alpha=-group['lr'])
                
                exp_avg.mul_(beta2).add_(grad, alpha=1 - beta2)

        return loss
```

## ğŸ“… è‡ªå®šä¹‰å­¦ä¹ ç‡è°ƒåº¦å™¨

### 1. æ‰©å±•è°ƒåº¦å™¨æ¨¡å—

```python
# åœ¨ src/schedules/scheduler.py ä¸­æ·»åŠ 
import math
from torch.optim.lr_scheduler import _LRScheduler

class MyCustomScheduler(_LRScheduler):
    """è‡ªå®šä¹‰å­¦ä¹ ç‡è°ƒåº¦å™¨"""

    def __init__(self, optimizer, warmup_epochs=5, max_epochs=100, min_lr=1e-6, **kwargs):
        self.warmup_epochs = warmup_epochs
        self.max_epochs = max_epochs
        self.min_lr = min_lr
        super(MyCustomScheduler, self).__init__(optimizer)

    def get_lr(self):
        if self.last_epoch < self.warmup_epochs:
            # é¢„çƒ­é˜¶æ®µ
            return [base_lr * (self.last_epoch + 1) / self.warmup_epochs 
                    for base_lr in self.base_lrs]
        else:
            # ä½™å¼¦é€€ç«
            progress = (self.last_epoch - self.warmup_epochs) / (self.max_epochs - self.warmup_epochs)
            return [self.min_lr + (base_lr - self.min_lr) * 0.5 * (1 + math.cos(math.pi * progress))
                    for base_lr in self.base_lrs]

class CyclicLRScheduler(_LRScheduler):
    """å¾ªç¯å­¦ä¹ ç‡è°ƒåº¦å™¨"""
    
    def __init__(self, optimizer, base_lr, max_lr, step_size_up=2000, mode='triangular', **kwargs):
        self.base_lr = base_lr
        self.max_lr = max_lr
        self.step_size_up = step_size_up
        self.step_size_down = step_size_up
        self.mode = mode
        super(CyclicLRScheduler, self).__init__(optimizer)

    def get_lr(self):
        cycle = math.floor(1 + self.last_epoch / (self.step_size_up + self.step_size_down))
        x = abs(self.last_epoch / self.step_size_up - 2 * cycle + 1)
        
        if self.mode == 'triangular':
            scale_fn = lambda x: 1.
        elif self.mode == 'triangular2':
            scale_fn = lambda x: 1 / (2. ** (cycle - 1))
        else:
            scale_fn = lambda x: 1.
            
        return [self.base_lr + (self.max_lr - self.base_lr) * max(0, (1 - x)) * scale_fn(x)
                for _ in self.base_lrs]

def get_scheduler(optimizer, scheduler_name, **kwargs):
    scheduler_name = scheduler_name.lower()

    if scheduler_name == "my_scheduler":
        return MyCustomScheduler(optimizer, **kwargs)
    elif scheduler_name == "cyclic":
        return CyclicLRScheduler(optimizer, **kwargs)
    # ... å…¶ä»–è°ƒåº¦å™¨
    else:
        raise ValueError(f"Unknown scheduler: {scheduler_name}")
```

## ğŸ’” è‡ªå®šä¹‰æŸå¤±å‡½æ•°

### 1. æ‰©å±•æŸå¤±å‡½æ•°æ¨¡å—

```python
# åœ¨ src/losses/image_loss.py ä¸­æ·»åŠ 
import torch
import torch.nn as nn
import torch.nn.functional as F

class MyCustomLoss(nn.Module):
    """è‡ªå®šä¹‰æŸå¤±å‡½æ•°"""

    def __init__(self, alpha=1.0, beta=1.0, reduction='mean'):
        super(MyCustomLoss, self).__init__()
        self.alpha = alpha
        self.beta = beta
        self.reduction = reduction

    def forward(self, inputs, targets):
        # å®ç°æŸå¤±è®¡ç®—é€»è¾‘
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        
        # æ·»åŠ è‡ªå®šä¹‰é¡¹
        custom_term = self._compute_custom_term(inputs, targets)
        
        loss = self.alpha * ce_loss + self.beta * custom_term

        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:
            return loss

    def _compute_custom_term(self, inputs, targets):
        # å…·ä½“çš„è‡ªå®šä¹‰æŸå¤±è®¡ç®—
        # ä¾‹å¦‚ï¼šç½®ä¿¡åº¦æƒ©ç½š
        probs = F.softmax(inputs, dim=1)
        max_probs = torch.max(probs, dim=1)[0]
        confidence_penalty = -torch.log(max_probs + 1e-8)
        return confidence_penalty

class ContrastiveLoss(nn.Module):
    """å¯¹æ¯”æŸå¤±å‡½æ•°"""
    
    def __init__(self, margin=1.0, reduction='mean'):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin
        self.reduction = reduction
        
    def forward(self, output1, output2, label):
        # label: 1è¡¨ç¤ºç›¸ä¼¼ï¼Œ0è¡¨ç¤ºä¸ç›¸ä¼¼
        euclidean_distance = F.pairwise_distance(output1, output2)
        
        loss_contrastive = torch.mean(
            (1 - label) * torch.pow(euclidean_distance, 2) +
            label * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)
        )
        
        return loss_contrastive

class DiceLoss(nn.Module):
    """DiceæŸå¤±å‡½æ•°ï¼Œå¸¸ç”¨äºåˆ†å‰²ä»»åŠ¡"""
    
    def __init__(self, smooth=1e-6):
        super(DiceLoss, self).__init__()
        self.smooth = smooth
        
    def forward(self, inputs, targets):
        # inputs: [B, C, H, W]
        # targets: [B, H, W]
        
        inputs = F.softmax(inputs, dim=1)
        targets_one_hot = F.one_hot(targets, num_classes=inputs.size(1)).permute(0, 3, 1, 2).float()
        
        intersection = (inputs * targets_one_hot).sum(dim=(2, 3))
        union = inputs.sum(dim=(2, 3)) + targets_one_hot.sum(dim=(2, 3))
        
        dice = (2. * intersection + self.smooth) / (union + self.smooth)
        return 1 - dice.mean()

def get_loss_function(loss_type="cross_entropy", **kwargs):
    loss_type = loss_type.lower()

    if loss_type == "my_custom_loss":
        return MyCustomLoss(**kwargs)
    elif loss_type == "contrastive":
        return ContrastiveLoss(**kwargs)
    elif loss_type == "dice":
        return DiceLoss(**kwargs)
    # ... å…¶ä»–æŸå¤±å‡½æ•°
    else:
        raise ValueError(f"Unknown loss function: {loss_type}")
```

## ğŸ”§ é›†æˆè‡ªå®šä¹‰ç»„ä»¶

### 1. æ›´æ–°è®­ç»ƒå™¨

```python
# åœ¨ scripts/train.py ä¸­é›†æˆè‡ªå®šä¹‰ç»„ä»¶
def run_training(config, experiment_name=None):
    # ... ç°æœ‰ä»£ç 

    # ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†
    dataset_type = data_config.get('type', 'cifar10')
    if dataset_type == 'my_special':
        from src.data_preprocessing.my_special_dataset import create_my_special_dataloaders
        train_dataloader, test_dataloader = create_my_special_dataloaders(
            data_path=data_config.get('data_path'),
            batch_size=hyperparams['batch_size'],
            image_size=data_config.get('image_size', 224),
            num_workers=data_config.get('num_workers', 4)
        )
        num_classes = len(train_dataloader.dataset.class_to_idx)
    
    # ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹
    model_type = model_config.get('type', 'simple_cnn')
    if model_type == 'my_custom_model':
        from src.models.my_models import AdvancedCNN
        model = AdvancedCNN(
            num_classes=num_classes,
            **model_config.get('params', {})
        )
    elif model_type == 'transformer':
        from src.models.my_models import TransformerModel
        model = TransformerModel(
            num_classes=num_classes,
            **model_config.get('params', {})
        )
    else:
        # ä½¿ç”¨ç°æœ‰çš„get_modelå‡½æ•°
        model = get_model(model_type, num_classes=num_classes, **model_config.get('params', {}))

    # ... å…¶ä½™è®­ç»ƒé€»è¾‘
```

### 2. æ›´æ–°é…ç½®æ–‡ä»¶

```yaml
# config/custom_config.yaml
data:
  type: "my_special"  # æ–°çš„æ•°æ®é›†ç±»å‹
  data_path: "./data/raw/my_special_data"
  image_size: 224
  num_workers: 8

model:
  type: "my_custom_model"  # æ–°çš„æ¨¡å‹ç±»å‹
  params:
    dropout: 0.3
    input_channels: 3

optimizer:
  type: "lion"  # æ–°çš„ä¼˜åŒ–å™¨
  params:
    betas: [0.9, 0.99]
    weight_decay: 0.01

scheduler:
  type: "my_scheduler"  # æ–°çš„è°ƒåº¦å™¨
  params:
    warmup_epochs: 10
    max_epochs: 100
    min_lr: 1e-6

loss:
  type: "my_custom_loss"  # æ–°çš„æŸå¤±å‡½æ•°
  params:
    alpha: 1.0
    beta: 0.5

hyperparameters:
  learning_rate: 1e-4
  batch_size: 32
  epochs: 100
```

## ğŸ§ª æµ‹è¯•è‡ªå®šä¹‰ç»„ä»¶

### 1. å•å…ƒæµ‹è¯•

```python
# tests/test_custom_components.py
import unittest
import torch
from src.data_preprocessing.my_dataset import MyCustomDataset
from src.models.my_models import AdvancedCNN, TransformerModel
from src.optimizers.optim import LionOptimizer
from src.losses.image_loss import MyCustomLoss

class TestCustomComponents(unittest.TestCase):

    def test_custom_dataset(self):
        """æµ‹è¯•è‡ªå®šä¹‰æ•°æ®é›†"""
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        dataset = MyCustomDataset('./test_data')
        self.assertGreater(len(dataset), 0)
        
        # æµ‹è¯•æ•°æ®åŠ è½½
        sample = dataset[0]
        self.assertEqual(len(sample), 2)  # image, label

    def test_advanced_cnn(self):
        """æµ‹è¯•é«˜çº§CNNæ¨¡å‹"""
        model = AdvancedCNN(num_classes=5)
        x = torch.randn(2, 3, 224, 224)
        output = model(x)
        self.assertEqual(output.shape, (2, 5))

    def test_transformer_model(self):
        """æµ‹è¯•Transformeræ¨¡å‹"""
        model = TransformerModel(num_classes=10, d_model=256, nhead=8)
        x = torch.randn(1, 3, 224, 224)
        output = model(x)
        self.assertEqual(output.shape, (1, 10))

    def test_lion_optimizer(self):
        """æµ‹è¯•Lionä¼˜åŒ–å™¨"""
        model = AdvancedCNN(num_classes=5)
        optimizer = LionOptimizer(model.parameters(), lr=1e-4)
        
        # æ¨¡æ‹Ÿä¸€æ­¥ä¼˜åŒ–
        x = torch.randn(2, 3, 224, 224)
        y = torch.randint(0, 5, (2,))
        
        output = model(x)
        loss = torch.nn.CrossEntropyLoss()(output, y)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        self.assertTrue(True)  # å¦‚æœæ²¡æœ‰é”™è¯¯ï¼Œæµ‹è¯•é€šè¿‡

    def test_custom_loss(self):
        """æµ‹è¯•è‡ªå®šä¹‰æŸå¤±å‡½æ•°"""
        loss_fn = MyCustomLoss(alpha=1.0, beta=0.5)
        
        inputs = torch.randn(4, 10)
        targets = torch.randint(0, 10, (4,))
        
        loss = loss_fn(inputs, targets)
        self.assertIsInstance(loss, torch.Tensor)
        self.assertEqual(loss.dim(), 0)  # æ ‡é‡

if __name__ == '__main__':
    unittest.main()
```

### 2. é›†æˆæµ‹è¯•

```python
# tests/test_integration.py
import unittest
import tempfile
import os
from scripts.train import run_training

class TestIntegration(unittest.TestCase):
    
    def test_custom_training_pipeline(self):
        """æµ‹è¯•å®Œæ•´çš„è‡ªå®šä¹‰è®­ç»ƒæµç¨‹"""
        config = {
            'data': {
                'type': 'cifar10',  # ä½¿ç”¨CIFAR-10è¿›è¡Œå¿«é€Ÿæµ‹è¯•
                'root': './data',
                'download': True,
                'batch_size': 16
            },
            'model': {
                'type': 'my_custom_model',
                'params': {'dropout': 0.3}
            },
            'optimizer': {
                'type': 'lion',
                'params': {'weight_decay': 0.01}
            },
            'scheduler': {
                'type': 'my_scheduler',
                'params': {'warmup_epochs': 1, 'max_epochs': 2}
            },
            'loss': {
                'type': 'my_custom_loss',
                'params': {'alpha': 1.0, 'beta': 0.1}
            },
            'hyperparameters': {
                'learning_rate': 1e-3,
                'batch_size': 16,
                'epochs': 2  # å¿«é€Ÿæµ‹è¯•
            }
        }

        # è¿è¡Œè®­ç»ƒ
        result = run_training(config, experiment_name='test_custom')
        
        # éªŒè¯ç»“æœ
        self.assertIsNotNone(result)
        self.assertIn('best_accuracy', result)
        self.assertGreater(result['best_accuracy'], 0)

if __name__ == '__main__':
    unittest.main()
```

## ğŸ“š æœ€ä½³å®è·µ

### 1. ä»£ç ç»„ç»‡
- **æ¨¡å—åŒ–è®¾è®¡**ï¼šæ¯ä¸ªè‡ªå®šä¹‰ç»„ä»¶æ”¾åœ¨ç‹¬ç«‹çš„æ–‡ä»¶ä¸­
- **æ¥å£ä¸€è‡´æ€§**ï¼šä¿æŒä¸ç°æœ‰ç»„ä»¶ç›¸åŒçš„æ¥å£è®¾è®¡
- **æ–‡æ¡£å®Œæ•´æ€§**ï¼šæ·»åŠ è¯¦ç»†çš„æ–‡æ¡£å­—ç¬¦ä¸²å’Œç±»å‹æ³¨è§£
- **ä»£ç å¤ç”¨**ï¼šå°½å¯èƒ½å¤ç”¨ç°æœ‰çš„å·¥å…·å‡½æ•°å’ŒåŸºç±»

### 2. é…ç½®ç®¡ç†
- **å‚æ•°éªŒè¯**ï¼šåœ¨ç»„ä»¶åˆå§‹åŒ–æ—¶éªŒè¯é…ç½®å‚æ•°çš„æœ‰æ•ˆæ€§
- **é»˜è®¤å€¼**ï¼šä¸ºæ‰€æœ‰å‚æ•°æä¾›åˆç†çš„é»˜è®¤å€¼
- **å‘åå…¼å®¹**ï¼šç¡®ä¿æ–°å¢å‚æ•°ä¸å½±å“ç°æœ‰é…ç½®çš„ä½¿ç”¨
- **é…ç½®æ–‡æ¡£**ï¼šåœ¨é…ç½®æ–‡ä»¶ä¸­æ·»åŠ è¯¦ç»†çš„å‚æ•°è¯´æ˜

### 3. é”™è¯¯å¤„ç†
- **è¾“å…¥éªŒè¯**ï¼šæ£€æŸ¥è¾“å…¥æ•°æ®çš„æ ¼å¼å’ŒèŒƒå›´
- **å¼‚å¸¸å¤„ç†**ï¼šæä¾›æœ‰æ„ä¹‰çš„é”™è¯¯ä¿¡æ¯å’Œå»ºè®®
- **é™çº§æœºåˆ¶**ï¼šåœ¨ç»„ä»¶ä¸å¯ç”¨æ—¶æä¾›å¤‡é€‰æ–¹æ¡ˆ
- **æ—¥å¿—è®°å½•**ï¼šè®°å½•å…³é”®æ“ä½œå’Œé”™è¯¯ä¿¡æ¯

### 4. æ€§èƒ½ä¼˜åŒ–
- **å†…å­˜æ•ˆç‡**ï¼šé¿å…ä¸å¿…è¦çš„å†…å­˜åˆ†é…å’Œå¤åˆ¶
- **è®¡ç®—ä¼˜åŒ–**ï¼šä½¿ç”¨å‘é‡åŒ–æ“ä½œå’ŒGPUåŠ é€Ÿ
- **ç¼“å­˜æœºåˆ¶**ï¼šç¼“å­˜é‡å¤è®¡ç®—çš„ç»“æœ
- **æ‰¹å¤„ç†**ï¼šæ”¯æŒæ‰¹é‡å¤„ç†ä»¥æé«˜æ•ˆç‡

### 5. æµ‹è¯•ç­–ç•¥
- **å•å…ƒæµ‹è¯•**ï¼šä¸ºæ¯ä¸ªç»„ä»¶ç¼–å†™ç‹¬ç«‹çš„æµ‹è¯•
- **é›†æˆæµ‹è¯•**ï¼šæµ‹è¯•ç»„ä»¶ä¹‹é—´çš„åä½œ
- **æ€§èƒ½æµ‹è¯•**ï¼šéªŒè¯ç»„ä»¶çš„æ€§èƒ½è¡¨ç°
- **å›å½’æµ‹è¯•**ï¼šç¡®ä¿ä¿®æ”¹ä¸å½±å“ç°æœ‰åŠŸèƒ½

## ğŸš€ æ‰©å±•ç¤ºä¾‹

### æ·»åŠ æ–°çš„æ•°æ®å¢å¼ºç­–ç•¥

```python
# src/data_preprocessing/augmentations.py
import torch
import torchvision.transforms as transforms
from PIL import Image, ImageFilter
import random

class MixUp:
    """MixUpæ•°æ®å¢å¼º"""
    
    def __init__(self, alpha=1.0):
        self.alpha = alpha
        
    def __call__(self, batch):
        images, labels = batch
        batch_size = images.size(0)
        
        # ç”Ÿæˆæ··åˆæƒé‡
        lam = torch.distributions.Beta(self.alpha, self.alpha).sample()
        
        # éšæœºæ’åˆ—
        indices = torch.randperm(batch_size)
        
        # æ··åˆå›¾åƒ
        mixed_images = lam * images + (1 - lam) * images[indices]
        
        # æ··åˆæ ‡ç­¾
        mixed_labels = (labels, labels[indices], lam)
        
        return mixed_images, mixed_labels

class CutMix:
    """CutMixæ•°æ®å¢å¼º"""
    
    def __init__(self, alpha=1.0):
        self.alpha = alpha
        
    def __call__(self, batch):
        images, labels = batch
        batch_size = images.size(0)
        
        # ç”Ÿæˆæ··åˆæ¯”ä¾‹
        lam = torch.distributions.Beta(self.alpha, self.alpha).sample()
        
        # è®¡ç®—è£å‰ªåŒºåŸŸ
        _, _, H, W = images.shape
        cut_rat = torch.sqrt(1. - lam)
        cut_w = int(W * cut_rat)
        cut_h = int(H * cut_rat)
        
        cx = torch.randint(0, W, (1,))
        cy = torch.randint(0, H, (1,))
        
        bbx1 = torch.clamp(cx - cut_w // 2, 0, W)
        bby1 = torch.clamp(cy - cut_h // 2, 0, H)
        bbx2 = torch.clamp(cx + cut_w // 2, 0, W)
        bby2 = torch.clamp(cy + cut_h // 2, 0, H)
        
        # éšæœºæ’åˆ—
        indices = torch.randperm(batch_size)
        
        # åº”ç”¨CutMix
        mixed_images = images.clone()
        mixed_images[:, :, bby1:bby2, bbx1:bbx2] = images[indices, :, bby1:bby2, bbx1:bbx2]
        
        # è°ƒæ•´lambda
        lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (W * H))
        
        mixed_labels = (labels, labels[indices], lam)
        
        return mixed_images, mixed_labels
```

é€šè¿‡éµå¾ªè¿™ä¸ªå¼€å‘æŒ‡å—ï¼Œæ‚¨å¯ä»¥è½»æ¾åœ°æ‰©å±•è®­ç»ƒæ¡†æ¶ä»¥æ»¡è¶³ç‰¹å®šéœ€æ±‚ï¼ŒåŒæ—¶ä¿æŒä»£ç çš„è´¨é‡å’Œå¯ç»´æŠ¤æ€§ï¼
