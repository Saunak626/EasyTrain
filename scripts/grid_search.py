"""ç½‘æ ¼æœç´¢å¯åŠ¨è„šæœ¬

å®ç°è¶…å‚æ•°ç½‘æ ¼æœç´¢ï¼Œæ”¯æŒè¿›ç¨‹å†…è°ƒç”¨å’Œå¤šç§å‚æ•°ç»„åˆç­–ç•¥ã€‚
ä¸»è¦åŠŸèƒ½ï¼šå‚æ•°ç»„åˆç”Ÿæˆã€å®éªŒæ‰§è¡Œã€ç»“æœæ”¶é›†å’ŒCSVæŠ¥å‘Šç”Ÿæˆã€‚
"""
import itertools
import subprocess
import yaml
import os
import sys
import csv
import json
import random
import fcntl
import pandas as pd
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.utils.config_parser import parse_arguments

def load_grid_config(path="config/grid.yaml"):
    """åŠ è½½ç½‘æ ¼æœç´¢é…ç½®"""
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def _as_list(v):
    """å°†è¾“å…¥è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼

    Args:
        v: ä»»æ„ç±»å‹çš„å‚æ•°å€¼

    Returns:
        list: ç»Ÿä¸€æ ¼å¼åŒ–åçš„åˆ—è¡¨
    """
    if v is None:
        return []
    return v if isinstance(v, (list, tuple)) else [v]

def generate_combinations(config):
    """
    åˆ†ç»„å¼å‚æ•°ç»„åˆç”Ÿæˆå™¨

    è®¾è®¡é€»è¾‘ï¼š
    1. ä»YAMLä¸­è·å–groupsé…ç½®ï¼Œæ¯ç»„æœ‰è‡ªå·±çš„æ¨¡å‹å’Œè¶…å‚æ•°èŒƒå›´
    2. ä¸ºæ¯ç»„å†…çš„å‚æ•°è¿›è¡Œç¬›å¡å°”ç§¯ç»„åˆ
    3. æ ¹æ®models_to_trainè¿‡æ»¤å¯ç”¨çš„æ¨¡å‹
    4. é¿å…æ— æ„ä¹‰çš„æ¨¡å‹-å‚æ•°ç»„åˆï¼ŒèŠ‚çœç®—åŠ›

    Args:
        config (dict): ç½‘æ ¼æœç´¢é…ç½®

    Returns:
        list[dict]: å‚æ•°ç»„åˆåˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸ä»£è¡¨ä¸€ç»„å®éªŒå‚æ•°
    """
    gs = (config or {}).get("grid_search", {}) or {}
    fixed = gs.get("fixed", {}) or {}
    models_to_train = config.get("models_to_train", [])

    # === åˆ†ç»„å¼é…ç½®å¤„ç† ===
    if "groups" in gs and gs["groups"]:
        print(f"ğŸ“‹ ä½¿ç”¨åˆ†ç»„å¼ç½‘æ ¼æœç´¢é…ç½®")
        return _generate_combinations_by_groups(gs["groups"], fixed, models_to_train)

    # === è¾¹ç•Œæƒ…å†µï¼šæ— æœç´¢å‚æ•° ===
    else:
        print(f"âš ï¸  æœªæ‰¾åˆ°groupsé…ç½®ï¼Œè¿”å›å›ºå®šå‚æ•°")
        return [fixed] if fixed else [{}]


def _generate_combinations_by_groups(groups_config, fixed, models_to_train):
    """åˆ†ç»„å¼å‚æ•°ç»„åˆç”Ÿæˆå™¨ - æ”¯æŒç»„å†…æ¨¡å‹-batch_sizeæ™ºèƒ½é…å¯¹"""
    all_combinations = []
    total_groups = len(groups_config)
    
    print(f"ğŸ¯ å‘ç° {total_groups} ä¸ªæ¨¡å‹ç»„:")
    for group_name in groups_config.keys():
        group_models = _as_list(groups_config[group_name].get("model.type", []))
        print(f"   - {group_name}: {group_models}")
    
    for group_name, group_params in groups_config.items():
        print(f"\nğŸ”§ å¤„ç†æ¨¡å‹ç»„: {group_name}")
        
        # === ç¬¬1æ­¥ï¼šè·å–ç»„å†…çš„æ¨¡å‹å’Œbatch_size ===
        group_models = _as_list(group_params.get("model.type", []))
        group_batch_sizes = _as_list(group_params.get("hp.batch_size", []))
        
        print(f"   ğŸ“‹ ç»„å†…é…ç½®:")
        print(f"      model.type: {group_models} (é•¿åº¦: {len(group_models)})")
        print(f"      hp.batch_size: {group_batch_sizes} (é•¿åº¦: {len(group_batch_sizes)})")
        
        # === ç¬¬2æ­¥ï¼šå¤„ç†æ¨¡å‹-batch_sizeé…å¯¹é€»è¾‘ ===
        if group_batch_sizes:
            if len(group_batch_sizes) == 1:
                # æƒ…å†µ1ï¼šbatch_sizeé•¿åº¦=1ï¼Œæ‰©å……åˆ°ä¸model.typeä¸€è‡´
                group_batch_sizes = group_batch_sizes * len(group_models)
                print(f"   ğŸ”„ æ‰©å……batch_size: {group_batch_sizes} (æ‰©å……åˆ°ä¸model.typeé•¿åº¦ä¸€è‡´)")
                # åˆ›å»ºä¸€å¯¹ä¸€é…å¯¹å­—å…¸
                model_batch_dict = dict(zip(group_models, group_batch_sizes))
                print(f"   ğŸ“Š æ¨¡å‹-batch_sizeé…å¯¹å­—å…¸: {model_batch_dict}")
            elif len(group_batch_sizes) == len(group_models):
                # æƒ…å†µ2ï¼šbatch_sizeé•¿åº¦=model.typeé•¿åº¦ï¼ŒæŒ‰é¡ºåºé…å¯¹
                print(f"   âœ… é•¿åº¦åŒ¹é…ï¼Œå°†æŒ‰é¡ºåºé…å¯¹")
                model_batch_dict = dict(zip(group_models, group_batch_sizes))
                print(f"   ğŸ“Š æ¨¡å‹-batch_sizeé…å¯¹å­—å…¸: {model_batch_dict}")
            else:
                # æƒ…å†µ3ï¼šbatch_sizeé•¿åº¦â‰ 1ä¸”â‰ model.typeé•¿åº¦ï¼Œä½œä¸ºç‹¬ç«‹å‚æ•°å¤„ç†
                print(f"   ğŸ”„ batch_sizeä½œä¸ºç‹¬ç«‹å‚æ•°å¤„ç†ï¼Œå°†ä¸æ¨¡å‹è¿›è¡Œç¬›å¡å°”ç§¯ç»„åˆ")
                model_batch_dict = None  # æ ‡è®°ä¸ºç‹¬ç«‹å‚æ•°å¤„ç†
        else:
            # æ²¡æœ‰batch_sizeé…ç½®ï¼Œæ‰€æœ‰æ¨¡å‹ä½¿ç”¨é»˜è®¤å€¼
            model_batch_dict = {model: None for model in group_models}
            print(f"   ğŸ“Š æ— batch_sizeé…ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼")
        
        # === ç¬¬4æ­¥ï¼šæ ¹æ®models_to_trainè¿‡æ»¤æ¨¡å‹ ===
        if models_to_train:
            enabled_models = [model for model in group_models if model in models_to_train]
            if not enabled_models:
                print(f"   â­ï¸  è·³è¿‡ç»„ {group_name}ï¼šæ— å¯ç”¨çš„æ¨¡å‹")
                continue
            print(f"   ğŸ¯ å¯ç”¨çš„æ¨¡å‹: {enabled_models}")
        else:
            enabled_models = group_models
            print(f"   ğŸ¯ ä½¿ç”¨æ‰€æœ‰æ¨¡å‹: {enabled_models}")
        
        # === ç¬¬5æ­¥ï¼šå¤„ç†å‚æ•°ç»„åˆ ===
        if model_batch_dict is not None:
            # æœ‰æ¨¡å‹-batch_sizeé…å¯¹çš„æƒ…å†µ
            enabled_pairs = {model: batch_size for model, batch_size in model_batch_dict.items() 
                            if model in enabled_models}
            print(f"   ğŸ¯ å¯ç”¨çš„æ¨¡å‹é…å¯¹: {enabled_pairs}")
            
            # è·å–å…¶ä»–å‚æ•°ï¼ˆæ’é™¤model.typeå’Œhp.batch_sizeï¼‰
            other_params = {k: v for k, v in group_params.items() 
                           if k not in ["model.type", "hp.batch_size"]}
            
            # ç”Ÿæˆç»„åˆ
            if not other_params:
                # åªæœ‰æ¨¡å‹-batch_sizeé…å¯¹ï¼Œæ— å…¶ä»–å‚æ•°
                for model, batch_size in enabled_pairs.items():
                    combo = {**fixed, "model.type": model, "group": group_name}
                    if batch_size is not None:
                        combo["hp.batch_size"] = batch_size
                    all_combinations.append(combo)
            else:
                # æœ‰å…¶ä»–å‚æ•°ï¼Œè¿›è¡Œç¬›å¡å°”ç§¯ç»„åˆ
                param_items = [(k, _as_list(v)) for k, v in other_params.items() if _as_list(v)]
                if param_items:
                    param_keys, param_values_lists = zip(*param_items)
                    for model, batch_size in enabled_pairs.items():
                        for param_combo in itertools.product(*param_values_lists):
                            combo = {
                                **fixed,
                                "model.type": model,
                                "group": group_name
                            }
                            if batch_size is not None:
                                combo["hp.batch_size"] = batch_size
                            combo.update(dict(zip(param_keys, param_combo)))
                            all_combinations.append(combo)
                else:
                    # å…¶ä»–å‚æ•°éƒ½ä¸ºç©º
                    for model, batch_size in enabled_pairs.items():
                        combo = {**fixed, "model.type": model, "group": group_name}
                        if batch_size is not None:
                            combo["hp.batch_size"] = batch_size
                        all_combinations.append(combo)
        else:
            # batch_sizeä½œä¸ºç‹¬ç«‹å‚æ•°ï¼Œä¸æ¨¡å‹è¿›è¡Œç¬›å¡å°”ç§¯ç»„åˆ
            all_params = {k: v for k, v in group_params.items() if k != "model.type"}
            param_items = [(k, _as_list(v)) for k, v in all_params.items() if _as_list(v)]
            
            if param_items:
                param_keys, param_values_lists = zip(*param_items)
                for model in enabled_models:
                    for param_combo in itertools.product(*param_values_lists):
                        combo = {
                            **fixed,
                            "model.type": model,
                            "group": group_name
                        }
                        combo.update(dict(zip(param_keys, param_combo)))
                        all_combinations.append(combo)
            else:
                # æ— å…¶ä»–å‚æ•°
                for model in enabled_models:
                    combo = {**fixed, "model.type": model, "group": group_name}
                    all_combinations.append(combo)
        
        # è®¡ç®—å½“å‰ç»„çš„ç»„åˆæ•°é‡å’Œè®¡ç®—è¿‡ç¨‹
        group_combinations = len([c for c in all_combinations if c.get("group") == group_name])

        # è®¡ç®—ç»„åˆæ•°é‡çš„åˆ†è§£
        if model_batch_dict is not None:
            # æœ‰æ¨¡å‹-batch_sizeé…å¯¹çš„æƒ…å†µ
            model_count = len(enabled_pairs)
            other_params = {k: v for k, v in group_params.items()
                           if k not in ["model.type", "hp.batch_size"]}
            param_items = [(k, _as_list(v)) for k, v in other_params.items() if _as_list(v)]
            if param_items:
                param_counts = [len(_as_list(v)) for _, v in other_params.items() if _as_list(v)]
                other_count = 1
                for count in param_counts:
                    other_count *= count
                print(f"   âœ… ç»„ {group_name} ç”Ÿæˆ {group_combinations} ä¸ªç»„åˆ ({model_count}æ¨¡å‹ Ã— {other_count}å‚æ•°ç»„åˆ)")
            else:
                print(f"   âœ… ç»„ {group_name} ç”Ÿæˆ {group_combinations} ä¸ªç»„åˆ ({model_count}æ¨¡å‹)")
        else:
            # batch_sizeä½œä¸ºç‹¬ç«‹å‚æ•°çš„æƒ…å†µ
            model_count = len(enabled_models)
            all_params = {k: v for k, v in group_params.items() if k != "model.type"}
            param_items = [(k, _as_list(v)) for k, v in all_params.items() if _as_list(v)]
            if param_items:
                param_counts = [len(_as_list(v)) for _, v in all_params.items() if _as_list(v)]
                other_count = 1
                for count in param_counts:
                    other_count *= count
                print(f"   âœ… ç»„ {group_name} ç”Ÿæˆ {group_combinations} ä¸ªç»„åˆ ({model_count}æ¨¡å‹ Ã— {other_count}å‚æ•°ç»„åˆ)")
            else:
                print(f"   âœ… ç»„ {group_name} ç”Ÿæˆ {group_combinations} ä¸ªç»„åˆ ({model_count}æ¨¡å‹)")
    
    print(f"\nğŸ‰ åˆ†ç»„å¼æœç´¢æ€»è®¡ç”Ÿæˆ {len(all_combinations)} ä¸ªç»„åˆ")
    return all_combinations



def get_csv_fieldnames(all_params):
    """è·å–CSVæ–‡ä»¶çš„å­—æ®µååˆ—è¡¨"""
    param_keys = sorted({k for params in all_params for k in params.keys()})
    
    # å°†model.typeç§»åˆ°ç¬¬3åˆ—ï¼Œgroupç§»åˆ°ç¬¬4åˆ—ï¼Œå…¶ä»–å‚æ•°æŒ‰åŸé¡ºåºæ’åˆ—
    other_param_keys = [k for k in param_keys if k not in ["model.type", "group"]]
    
    fieldnames = [
        "experiment_id", "exp_name", "model.type", "group", "success",
        "best_accuracy", "final_accuracy"
    ] + other_param_keys
    
    return fieldnames


def initialize_csv_file(filepath, fieldnames):
    """åˆå§‹åŒ–CSVæ–‡ä»¶ï¼Œå†™å…¥è¡¨å¤´"""
    results_dir = os.path.dirname(filepath)
    os.makedirs(results_dir, exist_ok=True)
    
    with open(filepath, "w", newline="", encoding="utf-8") as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()


def append_result_to_csv(result, filepath, fieldnames, experiment_id):
    """å®æ—¶è¿½åŠ å•ä¸ªç»“æœåˆ°CSVæ–‡ä»¶ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
    
    Args:
        result (dict): å®éªŒç»“æœ
        filepath (str): CSVæ–‡ä»¶è·¯å¾„
        fieldnames (list): CSVå­—æ®µååˆ—è¡¨
        experiment_id (int): å®éªŒID
    """
    try:
        # ä½¿ç”¨æ–‡ä»¶é”ç¡®ä¿çº¿ç¨‹å®‰å…¨
        with open(filepath, "a", newline="", encoding="utf-8") as csvfile:
            # è·å–æ–‡ä»¶é”
            fcntl.flock(csvfile.fileno(), fcntl.LOCK_EX)
            
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            
            row = {
                "experiment_id": f"{experiment_id:03d}",
                "exp_name": result.get("exp_name"),
                "success": result.get("success"),
                "best_accuracy": result.get("best_accuracy"),
                "final_accuracy": result.get("final_accuracy"),
            }
            row.update(result.get("params", {}))
            
            writer.writerow(row)
            
            # é‡Šæ”¾æ–‡ä»¶é”
            fcntl.flock(csvfile.fileno(), fcntl.LOCK_UN)
            
    except Exception as e:
        print(f"âš ï¸  å†™å…¥CSVå¤±è´¥: {e}")


def load_completed_experiments(filepath):
    """åŠ è½½å·²å®Œæˆçš„å®éªŒï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
    
    Args:
        filepath (str): CSVæ–‡ä»¶è·¯å¾„
        
    Returns:
        set: å·²å®Œæˆçš„å®éªŒåç§°é›†åˆ
    """
    if not os.path.exists(filepath):
        return set()
    
    try:
        df = pd.read_csv(filepath)
        completed_experiments = set(df['exp_name'].tolist())
        print(f"ğŸ”„ å‘ç°å·²å®Œæˆçš„å®éªŒ: {len(completed_experiments)} ä¸ª")
        return completed_experiments
    except Exception as e:
        print(f"âš ï¸  è¯»å–å·²å®Œæˆå®éªŒå¤±è´¥: {e}")
        return set()


def save_results_to_csv(results, filename):
    """ä¿å­˜å®éªŒç»“æœåˆ°CSVæ–‡ä»¶ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰

    Args:
        results (list[dict]): å®éªŒç»“æœåˆ—è¡¨
        filename (str): CSVæ–‡ä»¶å

    Returns:
        str|None: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """
    if not results:
        return None

    results_dir = "runs"
    os.makedirs(results_dir, exist_ok=True)
    filepath = os.path.join(results_dir, filename)

    # è·å–æ‰€æœ‰å‚æ•°çš„å­—æ®µå
    all_params = [r.get("params", {}) for r in results]
    fieldnames = get_csv_fieldnames(all_params)

    with open(filepath, "w", newline="", encoding="utf-8") as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        for i, r in enumerate(results, 1):
            row = {
                "experiment_id": f"{i:03d}",
                "exp_name": r.get("exp_name"),
                "success": r.get("success"),
                "best_accuracy": r.get("best_accuracy"),
                "final_accuracy": r.get("final_accuracy"),
            }
            row.update(r.get("params", {}))
            writer.writerow(row)

    return filepath


def apply_param_overrides(config, params):
    """åº”ç”¨å‚æ•°è¦†ç›–åˆ°é…ç½®å­—å…¸

    Args:
        config (dict): åŸºç¡€é…ç½®å­—å…¸
        params (dict): å‚æ•°è¦†ç›–å­—å…¸ï¼Œæ”¯æŒåµŒå¥—è·¯å¾„

    Returns:
        dict: åº”ç”¨è¦†ç›–åçš„é…ç½®å­—å…¸
    """
    import copy
    config = copy.deepcopy(config)
    
    for k, v in (params or {}).items():
        keys = k.split('.')
        target = config
        for key in keys[:-1]:
            target = target.setdefault(key, {})
        target[keys[-1]] = v
    
    return config


def run_single_experiment_in_process(params, exp_id, config_path):
    """è¿›ç¨‹å†…è°ƒç”¨æ–¹å¼è¿è¡Œå•ä¸ªå®éªŒï¼ˆå•å¡è®­ç»ƒï¼‰"""
    exp_name = f"grid_{exp_id}"
    
    try:
        # å¯¼å…¥è®­ç»ƒå‡½æ•°
        from src.trainers.base_trainer import run_training
        
        # åŠ è½½åŸºç¡€é…ç½®
        config = load_grid_config(config_path)
        
        # åº”ç”¨å‚æ•°è¦†ç›–
        config = apply_param_overrides(config, params)
        
        # ç›´æ¥è°ƒç”¨è®­ç»ƒå‡½æ•°
        result = run_training(config, exp_name)
        
        # æ·»åŠ å‚æ•°ä¿¡æ¯åˆ°ç»“æœä¸­
        result["params"] = params
        
        return result
        
    except Exception as e:
        return {
            "success": False,
            "exp_name": exp_name,
            "params": params,
            "best_accuracy": 0.0,
            "final_accuracy": 0.0,
            "error": str(e)
        }


def run_single_experiment_subprocess(params, exp_id, use_multi_gpu, config_path):
    """å­è¿›ç¨‹æ–¹å¼è¿è¡Œå•ä¸ªå®éªŒï¼ˆå¤šå¡è®­ç»ƒï¼‰"""
    exp_name = f"grid_{exp_id}"
    
    # åˆ›å»ºä¸´æ—¶ç»“æœæ–‡ä»¶ç”¨äºè¿›ç¨‹é—´é€šä¿¡
    temp_result_file = f"/tmp/grid_result_{exp_id}_{random.randint(1000,9999)}.json"
    
    # ç»„è£…å‘½ä»¤
    if use_multi_gpu:
        import torch  # å±€éƒ¨å¯¼å…¥ï¼Œä»…åœ¨éœ€è¦æ—¶ä½¿ç”¨
        cmd = ["accelerate", "launch", "--multi_gpu", "--num_processes", str(torch.cuda.device_count())]
    else:
        cmd = [sys.executable, "-u"]
    
    # æ·»åŠ è®­ç»ƒè„šæœ¬å’ŒåŸºç¡€å‚æ•°
    cmd.extend(["scripts/train.py", "--config", config_path, "--exp_name", exp_name])
    cmd.extend(["--result_file", temp_result_file])  # æ–°å¢ï¼šæŒ‡å®šç»“æœæ–‡ä»¶
    
    # æ·»åŠ å‚æ•°è¦†ç›–ï¼ˆæ’é™¤groupå‚æ•°ï¼Œå®ƒåªç”¨äºè®°å½•ï¼‰
    for k, v in (params or {}).items():
        if k != "group":  # groupå‚æ•°ä¸ä¼ é€’ç»™è®­ç»ƒè„šæœ¬
            cmd.extend([f"--{k}", str(v)])

    # æ¸…ç†ç¯å¢ƒå˜é‡å¹¶è®¾ç½®å”¯ä¸€ç«¯å£
    env = os.environ.copy()
    for k in ["LOCAL_RANK", "RANK", "WORLD_SIZE", "MASTER_ADDR", "MASTER_PORT"]:
        env.pop(k, None)
    env["MASTER_ADDR"] = env.get("MASTER_ADDR", "127.0.0.1")
    env["MASTER_PORT"] = str(20000 + random.randint(0, 10000))

    # å¯åŠ¨å­è¿›ç¨‹
    process = subprocess.Popen(cmd, env=env)
    try:
        rc = process.wait()
    except KeyboardInterrupt:
        print(f"æ•è·åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ç»ˆæ­¢å­è¿›ç¨‹ {process.pid}...")
        process.terminate()
        process.wait()
        raise

    success = (rc == 0)
    
    # è¯»å–ç»“æœæ–‡ä»¶
    try:
        if os.path.exists(temp_result_file):
            with open(temp_result_file, 'r', encoding='utf-8') as f:
                result = json.load(f)
            os.remove(temp_result_file)  # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            
            # ç¡®ä¿ç»“æœåŒ…å«å¿…è¦çš„å­—æ®µ
            result["params"] = params
            result["success"] = result.get("success", success)
            result["exp_name"] = result.get("exp_name", exp_name)
            
            # ç¡®ä¿accuracyå­—æ®µä¸ä¸ºNone
            if result.get("best_accuracy") is None:
                result["best_accuracy"] = 0.0
            if result.get("final_accuracy") is None:
                result["final_accuracy"] = 0.0
                
            return result
        else:
            print(f"ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {temp_result_file}")
    except Exception as e:
        print(f"è¯»å–ç»“æœæ–‡ä»¶å¤±è´¥: {e}")
        if os.path.exists(temp_result_file):
            try:
                with open(temp_result_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                print(f"æ–‡ä»¶å†…å®¹: {content[:200]}...")  # æ˜¾ç¤ºå‰200ä¸ªå­—ç¬¦
            except:
                pass
    
    # å›é€€ï¼šè¿”å›é»˜è®¤ç»“æœ
    return {
        "success": success,
        "exp_name": exp_name,
        "params": params,
        "best_accuracy": 0.0,
        "final_accuracy": 0.0,
        "error": "Failed to read result file" if success else "Training process failed"
    }


def run_single_experiment(params, exp_id, use_multi_gpu=False, config_path="config/grid.yaml"):
    """è¿è¡Œå•ä¸ªå®éªŒ

    Args:
        params (dict): å®éªŒå‚æ•°è¦†ç›–
        exp_id (str): å®éªŒID
        use_multi_gpu (bool): æ˜¯å¦ä½¿ç”¨å¤šGPU
        config_path (str): é…ç½®æ–‡ä»¶è·¯å¾„

    Returns:
        dict: å®éªŒç»“æœå­—å…¸
    """
    exp_name = f"grid_{exp_id}"

    # å®éªŒä¿¡æ¯å°†åœ¨è®­ç»ƒå™¨ä¸­çš„SwanLabå¯åŠ¨åæ˜¾ç¤º

    if use_multi_gpu:
        # å¤šå¡è®­ç»ƒï¼šä½¿ç”¨å­è¿›ç¨‹æ–¹å¼
        result = run_single_experiment_subprocess(params, exp_id, use_multi_gpu, config_path)
    else:
        # å•å¡è®­ç»ƒï¼šä½¿ç”¨è¿›ç¨‹å†…è°ƒç”¨æ–¹å¼
        result = run_single_experiment_in_process(params, exp_id, config_path)
    
    print(f"âœ… å®éªŒ {exp_name} å®Œæˆï¼Œæœ€ä½³: {result['best_accuracy']:.2f}% | æœ€ç»ˆ: {result['final_accuracy']:.2f}%")
    
    return result


def run_grid_search(args):
    """è¿è¡Œç½‘æ ¼æœç´¢"""
    config = load_grid_config(args.config)
    combinations = generate_combinations(config)

    if len(combinations) > args.max_experiments:
        combinations = combinations[:args.max_experiments]

    # å‡†å¤‡CSVæ–‡ä»¶
    results_dir = "runs"
    if args.results_file:
        # ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„æ–‡ä»¶å
        results_filename = args.results_file
    else:
        # ä½¿ç”¨é»˜è®¤çš„æ—¶é—´æˆ³æ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_filename = f"grid_search_results_{timestamp}.csv"
    csv_filepath = os.path.join(results_dir, results_filename)
    
    # è·å–CSVå­—æ®µå
    all_params = [params for params in combinations]
    fieldnames = get_csv_fieldnames(all_params)
    
    # æ–­ç‚¹ç»­ä¼ ï¼šæ£€æŸ¥å·²å®Œæˆçš„å®éªŒ
    completed_experiments = set()
    if args.save_results:
        os.makedirs(results_dir, exist_ok=True)
        
        # å¦‚æœç”¨æˆ·æŒ‡å®šäº†ç»“æœæ–‡ä»¶ï¼Œä¼˜å…ˆæ£€æŸ¥è¯¥æ–‡ä»¶
        if args.results_file and os.path.exists(csv_filepath):
            completed_experiments = load_completed_experiments(csv_filepath)
            if completed_experiments:
                print(f"ğŸ”„ æ–­ç‚¹ç»­ä¼ : ä½¿ç”¨æŒ‡å®šçš„ç»“æœæ–‡ä»¶ {results_filename}")
        else:
            # å¦åˆ™æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–ç»“æœæ–‡ä»¶å­˜åœ¨ï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰
            existing_files = [f for f in os.listdir(results_dir) if f.startswith("grid_search_results_") and f.endswith(".csv")]
            if existing_files and not args.results_file:
                latest_file = max(existing_files, key=lambda x: os.path.getctime(os.path.join(results_dir, x)))
                latest_filepath = os.path.join(results_dir, latest_file)
                completed_experiments = load_completed_experiments(latest_filepath)
                
                if completed_experiments:
                    # ä½¿ç”¨å·²å­˜åœ¨çš„æ–‡ä»¶ç»§ç»­å†™å…¥
                    csv_filepath = latest_filepath
                    results_filename = latest_file
                    print(f"ğŸ”„ æ–­ç‚¹ç»­ä¼ : ä½¿ç”¨å·²å­˜åœ¨çš„ç»“æœæ–‡ä»¶ {latest_file}")
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å·²å®Œæˆçš„å®éªŒï¼Œåˆå§‹åŒ–æ–°çš„CSVæ–‡ä»¶
        if not completed_experiments:
            initialize_csv_file(csv_filepath, fieldnames)
    else:
        # ä¸ä¿å­˜ç»“æœæ—¶ä¹Ÿéœ€è¦åˆå§‹åŒ–
        initialize_csv_file(csv_filepath, fieldnames)

    print(f"ğŸš€ å¼€å§‹ç½‘æ ¼æœç´¢ï¼Œå…± {len(combinations)} ä¸ªå®éªŒ")
    print(f"ğŸ“Š ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
    print(f"ğŸ’¾ ç»“æœæ–‡ä»¶: {csv_filepath}")
    
    # æ˜¾ç¤ºå…¨å±€å‚æ•°è¦†ç›–
    if args.data_percentage is not None:
        print(f"ğŸ¯ å…¨å±€å‚æ•°è¦†ç›–: data_percentage={args.data_percentage}")
    
    print("=" * 60)

    results = []
    successful = 0
    skipped = 0

    for i, params in enumerate(combinations, 1):
        exp_name = f"grid_{i:03d}"
        
        # æ–­ç‚¹ç»­ä¼ ï¼šè·³è¿‡å·²å®Œæˆçš„å®éªŒ
        if exp_name in completed_experiments:
            print(f"â­ï¸  è·³è¿‡å·²å®Œæˆçš„å®éªŒ {i}/{len(combinations)}: {exp_name}")
            skipped += 1
            continue
            
        print(f"ğŸ“Š å‡†å¤‡å®éªŒ {i}/{len(combinations)}")
        
        # å°†å‘½ä»¤è¡Œå‚æ•°æ·»åŠ åˆ°å®éªŒå‚æ•°ä¸­
        experiment_params = params.copy()
        if args.data_percentage is not None:
            experiment_params['data_percentage'] = args.data_percentage

        result = run_single_experiment(
            experiment_params, f"{i:03d}",
            use_multi_gpu=args.multi_gpu,
            config_path=args.config,
        )

        results.append(result)
        if result["success"]:
            successful += 1
            
        # å®æ—¶å†™å…¥CSV
        if args.save_results:
            append_result_to_csv(result, csv_filepath, fieldnames, i)
            
        # å®æ—¶æ˜¾ç¤ºæœ€ä½³ç»“æœ
        if successful > 0:
            current_best = max([r for r in results if r["success"]], key=lambda x: x["best_accuracy"])
            print(f"ğŸ† å½“å‰æœ€ä½³: {current_best['exp_name']} - {current_best['best_accuracy']:.2f}%")

    # æ€»ç»“
    print("=" * 60)
    print(f"ğŸ“ˆ ç½‘æ ¼æœç´¢å®Œæˆï¼")
    print(f"âœ… æˆåŠŸå®éªŒæ•°é‡: {successful}/{len(combinations)}")
    if skipped > 0:
        print(f"â­ï¸  è·³è¿‡å·²å®Œæˆå®éªŒ: {skipped} ä¸ª")

    if successful > 0:
        successful_results = [r for r in results if r["success"]]
        # æ‰¾åˆ°â€œæœ€ä½³å‡†ç¡®ç‡â€æœ€é«˜çš„å®éªŒç»“æœ
        best_result = max(successful_results, key=lambda x: x["best_accuracy"])

        print(f"ğŸ† æœ€ä½³å®éªŒç»“æœ:")
        print(f"å®éªŒåç§°: {best_result['exp_name']}, æœ€ä½³å‡†ç¡®ç‡: {best_result['best_accuracy']:.2f}%, æœ€ç»ˆå‡†ç¡®ç‡: {best_result['final_accuracy']:.2f}%")
        
        # æŒ‰æœ€ä½³ç²¾åº¦æ’åºå‰nç»„ç»“æœ
        top_results = sorted(successful_results, key=lambda x: x["best_accuracy"], reverse=True)[:args.top_n]
        
        print(f"ğŸ“Š å‰{args.top_n}åå®éªŒç»“æœ:")
        for i, r in enumerate(top_results, 1):
            print(f"{i}. {r['exp_name']} - {r['best_accuracy']:.2f}% - {r['params']}")

    if args.save_results:
        print(f"ğŸ’¾ ç»“æœå·²å®æ—¶ä¿å­˜åˆ°: {csv_filepath}")

    return 0 if successful > 0 else 1


def main():
    """ä¸»å‡½æ•°ï¼šè°ƒåº¦å™¨å§‹ç»ˆå•è¿›ç¨‹ï¼Œä¸è¿›å…¥ Accelerate ç¯å¢ƒ"""
    args, _ = parse_arguments(mode="grid_search")
    return run_grid_search(args)


if __name__ == "__main__":
    sys.exit(main())