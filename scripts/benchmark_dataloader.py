#!/usr/bin/env python3
"""
æ•°æ®åŠ è½½å™¨æ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬

ç”¨äºæµ‹è¯•ä¸åŒé…ç½®ä¸‹çš„æ•°æ®åŠ è½½é€Ÿåº¦ï¼Œå¸®åŠ©è¯Šæ–­I/Oç“¶é¢ˆã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/benchmark_dataloader.py --num_workers 4 --batch_size 8
"""

import os
import sys
import time
import argparse
import torch
from torch.utils.data import DataLoader

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.datasets.neonatal_multilabel_simple import NeonatalMultilabelSimple


def benchmark_dataloader(num_workers, batch_size, num_batches=50, persistent_workers=True, prefetch_factor=2):
    """åŸºå‡†æµ‹è¯•æ•°æ®åŠ è½½å™¨æ€§èƒ½
    
    Args:
        num_workers: workerè¿›ç¨‹æ•°
        batch_size: æ‰¹å¤§å°
        num_batches: æµ‹è¯•çš„batchæ•°é‡
        persistent_workers: æ˜¯å¦å¯ç”¨persistent_workers
        prefetch_factor: é¢„åŠ è½½å› å­
    """
    print("=" * 80)
    print(f"æ•°æ®åŠ è½½å™¨æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 80)
    print(f"é…ç½®:")
    print(f"  - num_workers: {num_workers}")
    print(f"  - batch_size: {batch_size}")
    print(f"  - persistent_workers: {persistent_workers}")
    print(f"  - prefetch_factor: {prefetch_factor}")
    print(f"  - æµ‹è¯•batchæ•°: {num_batches}")
    print("-" * 80)
    
    # åˆ›å»ºæ•°æ®é›†
    print("æ­£åœ¨åˆ›å»ºæ•°æ®é›†...")
    dataset = NeonatalMultilabelSimple(
        frames_dir='../Neonate-Feeding-Assessment/data/cpu_processed_627/frames_segments',
        labels_file='../Neonate-Feeding-Assessment/result_xlsx/shanghai/multi_hot_labels.xlsx',
        split='test',  # ä½¿ç”¨æµ‹è¯•é›†ï¼ˆè¾ƒå°ï¼‰
        clip_len=16
    )
    print(f"æ•°æ®é›†å¤§å°: {len(dataset)} æ ·æœ¬")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("æ­£åœ¨åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    use_persistent = persistent_workers and num_workers > 0
    use_prefetch = prefetch_factor if num_workers > 0 else None
    
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        persistent_workers=use_persistent,
        prefetch_factor=use_prefetch
    )
    
    # é¢„çƒ­ï¼ˆç¬¬ä¸€æ¬¡è¿­ä»£é€šå¸¸è¾ƒæ…¢ï¼‰
    print("é¢„çƒ­ä¸­...")
    for i, (frames, labels) in enumerate(dataloader):
        if i >= 2:
            break
    
    # åŸºå‡†æµ‹è¯•
    print(f"\nå¼€å§‹åŸºå‡†æµ‹è¯•ï¼ˆ{num_batches} batchesï¼‰...")
    batch_times = []
    total_start = time.time()
    
    for i, (frames, labels) in enumerate(dataloader):
        if i >= num_batches:
            break
        
        batch_start = time.time()
        # æ¨¡æ‹ŸGPUè®¡ç®—ï¼ˆå°†æ•°æ®ç§»åˆ°GPUï¼‰
        if torch.cuda.is_available():
            frames = frames.cuda()
            labels = labels.cuda()
        batch_time = time.time() - batch_start
        batch_times.append(batch_time)
        
        if (i + 1) % 10 == 0:
            avg_time = sum(batch_times[-10:]) / 10
            print(f"  Batch {i+1}/{num_batches} | å¹³å‡æ—¶é—´: {avg_time:.3f}s/batch")
    
    total_time = time.time() - total_start
    
    # ç»Ÿè®¡ç»“æœ
    print("\n" + "=" * 80)
    print("åŸºå‡†æµ‹è¯•ç»“æœ:")
    print("=" * 80)
    print(f"æ€»æ—¶é—´: {total_time:.2f}s")
    print(f"å¹³å‡æ—¶é—´: {sum(batch_times) / len(batch_times):.3f}s/batch")
    print(f"æœ€å¿«: {min(batch_times):.3f}s/batch")
    print(f"æœ€æ…¢: {max(batch_times):.3f}s/batch")
    print(f"ååé‡: {batch_size * len(batch_times) / total_time:.1f} æ ·æœ¬/ç§’")
    
    # é¢„ä¼°å®Œæ•´epochæ—¶é—´
    total_batches = len(dataloader)
    estimated_epoch_time = (sum(batch_times) / len(batch_times)) * total_batches
    print(f"\né¢„ä¼°å®Œæ•´epochæ—¶é—´: {estimated_epoch_time / 60:.1f} åˆ†é’Ÿ ({total_batches} batches)")
    
    return {
        'avg_time': sum(batch_times) / len(batch_times),
        'total_time': total_time,
        'throughput': batch_size * len(batch_times) / total_time
    }


def compare_configurations():
    """æ¯”è¾ƒä¸åŒé…ç½®çš„æ€§èƒ½"""
    print("\n" + "ğŸ”¬ " * 20)
    print("å¤šé…ç½®å¯¹æ¯”æµ‹è¯•")
    print("ğŸ”¬ " * 20 + "\n")
    
    configs = [
        {'num_workers': 0, 'batch_size': 8, 'persistent_workers': False, 'prefetch_factor': None},
        {'num_workers': 2, 'batch_size': 8, 'persistent_workers': False, 'prefetch_factor': 2},
        {'num_workers': 4, 'batch_size': 8, 'persistent_workers': True, 'prefetch_factor': 2},
        {'num_workers': 8, 'batch_size': 8, 'persistent_workers': True, 'prefetch_factor': 2},
        {'num_workers': 12, 'batch_size': 8, 'persistent_workers': True, 'prefetch_factor': 2},
    ]
    
    results = []
    for config in configs:
        result = benchmark_dataloader(num_batches=30, **config)
        results.append({**config, **result})
        time.sleep(2)  # çŸ­æš‚ä¼‘æ¯
    
    # æ‰“å°å¯¹æ¯”è¡¨
    print("\n" + "=" * 100)
    print("é…ç½®å¯¹æ¯”æ€»ç»“")
    print("=" * 100)
    print(f"{'Workers':<10} {'Batch':<8} {'Persistent':<12} {'Prefetch':<10} {'å¹³å‡æ—¶é—´':<12} {'ååé‡':<15}")
    print("-" * 100)
    for r in results:
        print(f"{r['num_workers']:<10} {r['batch_size']:<8} {str(r['persistent_workers']):<12} "
              f"{str(r['prefetch_factor']):<10} {r['avg_time']:.3f}s/batch  {r['throughput']:.1f} æ ·æœ¬/ç§’")
    
    # æ‰¾å‡ºæœ€ä½³é…ç½®
    best = min(results, key=lambda x: x['avg_time'])
    print("\nğŸ† æœ€ä½³é…ç½®:")
    print(f"  num_workers={best['num_workers']}, batch_size={best['batch_size']}, "
          f"persistent_workers={best['persistent_workers']}, prefetch_factor={best['prefetch_factor']}")
    print(f"  å¹³å‡æ—¶é—´: {best['avg_time']:.3f}s/batch")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='æ•°æ®åŠ è½½å™¨æ€§èƒ½åŸºå‡†æµ‹è¯•')
    parser.add_argument('--num_workers', type=int, default=4, help='workerè¿›ç¨‹æ•°')
    parser.add_argument('--batch_size', type=int, default=8, help='æ‰¹å¤§å°')
    parser.add_argument('--num_batches', type=int, default=50, help='æµ‹è¯•çš„batchæ•°é‡')
    parser.add_argument('--persistent_workers', action='store_true', help='å¯ç”¨persistent_workers')
    parser.add_argument('--prefetch_factor', type=int, default=2, help='é¢„åŠ è½½å› å­')
    parser.add_argument('--compare', action='store_true', help='å¯¹æ¯”å¤šç§é…ç½®')
    
    args = parser.parse_args()
    
    if args.compare:
        compare_configurations()
    else:
        benchmark_dataloader(
            num_workers=args.num_workers,
            batch_size=args.batch_size,
            num_batches=args.num_batches,
            persistent_workers=args.persistent_workers,
            prefetch_factor=args.prefetch_factor
        )

