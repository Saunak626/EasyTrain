import torch
import torch.nn as nn
from torchvision import models
from .model_registry import validate_model_for_task


class MLPClassifier(nn.Module):
    """è‡ªå®šä¹‰MLPåˆ†ç±»å™¨ï¼Œå‚è€ƒtmp/model.pyçš„å®ç°"""
    def __init__(self, input_dim=512, hidden_dim=256, output_dim=101, dropout=0.5):
        super(MLPClassifier, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.bn1 = nn.BatchNorm1d(hidden_dim)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.bn2 = nn.BatchNorm1d(hidden_dim)
        self.relu2 = nn.ReLU()
        self.dropout2 = nn.Dropout(dropout)
        self.fc3 = nn.Linear(hidden_dim, output_dim)
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()

    def _init_weights(self):
        """åˆå§‹åŒ–åˆ†ç±»å™¨æƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        # åªåœ¨è®­ç»ƒæ¨¡å¼ä¸”batch_size > 1æ—¶ä½¿ç”¨BatchNorm
        if self.training and x.size(0) > 1:
            x = self.bn1(x)
        x = self.dropout(x)
        x = self.fc2(x)
        x = self.relu2(x)
        # åªåœ¨è®­ç»ƒæ¨¡å¼ä¸”batch_size > 1æ—¶ä½¿ç”¨BatchNorm
        if self.training and x.size(0) > 1:
            x = self.bn2(x)
        x = self.dropout2(x)
        x = self.fc3(x)
        return x  # ä¸ä½¿ç”¨softmaxï¼Œè®©CrossEntropyLosså¤„ç†


class VideoNetModel(nn.Module):
    """
    è§†é¢‘åˆ†ç±»æ¨¡å‹åŒ…è£…å™¨ - å‚è€ƒtmp/model.pyçš„æ¶æ„è®¾è®¡
    ä½¿ç”¨ç‰¹å¾æå–å™¨ + è‡ªå®šä¹‰åˆ†ç±»å™¨çš„æ–¹å¼ï¼Œä¿æŒé¢„è®­ç»ƒç‰¹å¾çš„å®Œæ•´æ€§
    """
    
    def __init__(self, model_type='r3d_18', num_classes=101, pretrained=True, feature_dim=512, freeze_backbone=False):
        """
        åˆå§‹åŒ–è§†é¢‘åˆ†ç±»æ¨¡å‹

        Args:
            model_type: æ¨¡å‹ç±»å‹
            num_classes: åˆ†ç±»ç±»åˆ«æ•°
            pretrained: æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
            feature_dim: ç‰¹å¾ç»´åº¦
            freeze_backbone: æ˜¯å¦å†»ç»“éª¨å¹²ç½‘ç»œ
        """
        super(VideoNetModel, self).__init__()
        self.model_type = model_type
        self.num_classes = num_classes
        self.pretrained = pretrained
        self.feature_dim = feature_dim
        self.freeze_backbone = freeze_backbone
        
        # åˆ›å»ºç‰¹å¾æå–å™¨ï¼ˆä¿æŒé¢„è®­ç»ƒåˆ†ç±»å¤´ï¼‰
        self.feature_extractor = self._create_feature_extractor()
        
        # è·å–ç‰¹å¾ç»´åº¦
        original_feature_dim = self._get_feature_dim()
        
        # æ·»åŠ ç‰¹å¾é™ç»´å±‚
        self.pool = nn.AdaptiveAvgPool1d(self.feature_dim)
        
        # è‡ªå®šä¹‰åˆ†ç±»å™¨
        self.classifier = MLPClassifier(
            input_dim=self.feature_dim,
            hidden_dim=self.feature_dim // 2,
            output_dim=self.num_classes
        )
        
        # å¯é€‰ï¼šå†»ç»“éª¨å¹²ç½‘ç»œ
        if self.freeze_backbone:
            self._freeze_backbone()
    
    def _create_feature_extractor(self):
        """åˆ›å»ºç‰¹å¾æå–å™¨ï¼Œä¿æŒé¢„è®­ç»ƒæ¨¡å‹å®Œæ•´æ€§"""
        # å¯¼å…¥å…·ä½“çš„æƒé‡ç±»ï¼Œç¡®ä¿æ­£ç¡®åŠ è½½é¢„è®­ç»ƒæƒé‡
        from torchvision.models.video import (
            R3D_18_Weights, MC3_18_Weights, R2Plus1D_18_Weights, S3D_Weights,
            MViT_V1_B_Weights, MViT_V2_S_Weights,
            Swin3D_B_Weights, Swin3D_S_Weights, Swin3D_T_Weights
        )
        
        if self.model_type == 'r3d_18':
            weights = R3D_18_Weights.DEFAULT if self.pretrained else None
            model = models.video.r3d_18(weights=weights)
        elif self.model_type == 'mc3_18':
            weights = MC3_18_Weights.DEFAULT if self.pretrained else None
            model = models.video.mc3_18(weights=weights)
        elif self.model_type == 'r2plus1d_18':
            weights = R2Plus1D_18_Weights.DEFAULT if self.pretrained else None
            model = models.video.r2plus1d_18(weights=weights)
        elif self.model_type == 's3d':
            weights = S3D_Weights.DEFAULT if self.pretrained else None
            model = models.video.s3d(weights=weights)
        elif self.model_type == 'mvit_v1_b':
            weights = MViT_V1_B_Weights.DEFAULT if self.pretrained else None
            model = models.video.mvit_v1_b(weights=weights)
        elif self.model_type == 'mvit_v2_s':
            weights = MViT_V2_S_Weights.DEFAULT if self.pretrained else None
            model = models.video.mvit_v2_s(weights=weights)
        elif self.model_type == 'swin3d_b':
            weights = Swin3D_B_Weights.DEFAULT if self.pretrained else None
            model = models.video.swin3d_b(weights=weights)
        elif self.model_type == 'swin3d_s':
            weights = Swin3D_S_Weights.DEFAULT if self.pretrained else None
            model = models.video.swin3d_s(weights=weights)
        elif self.model_type == 'swin3d_t':
            weights = Swin3D_T_Weights.DEFAULT if self.pretrained else None
            model = models.video.swin3d_t(weights=weights)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è§†é¢‘æ¨¡å‹: {self.model_type}")
        
        # æ‰“å°æƒé‡åŠ è½½ä¿¡æ¯å’Œé¢„è®­ç»ƒæ•°æ®é›†ä¿¡æ¯
        if self.pretrained:
            pretrain_info = self._get_pretrain_info()
            print(
                f"âœ… å·²åŠ è½½ {self.model_type} é¢„è®­ç»ƒæƒé‡ | "
                f"ğŸ“Š æ•°æ®é›†: {pretrain_info['dataset']} | "
                f"ğŸ¯ ç±»åˆ«: {pretrain_info['classes']} | "
                f"ğŸ”§ ç­–ç•¥: ä¿æŒé¢„è®­ç»ƒåˆ†ç±»å¤´ + è‡ªå®šä¹‰åˆ†ç±»å™¨"
            )
            if pretrain_info['note']:
                print(f"   ğŸ’¡ æ³¨æ„: {pretrain_info['note']}")
        else:
            print(f"âš ï¸  {self.model_type} ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡")
        
        return model
    
    def _get_feature_dim(self):
        """è·å–æ¨¡å‹çš„åŸå§‹ç‰¹å¾ç»´åº¦"""
        feature_dims = {
            'r3d_18': 512,
            'mc3_18': 512, 
            'r2plus1d_18': 512,
            's3d': 1024,
            'mvit_v1_b': 768,
            'mvit_v2_s': 768,
            'swin3d_b': 1024,
            'swin3d_s': 768,
            'swin3d_t': 768
        }
        return feature_dims.get(self.model_type, 512)
    
    def _freeze_backbone(self):
        """å†»ç»“éª¨å¹²ç½‘ç»œï¼Œåªè®­ç»ƒåˆ†ç±»å™¨"""
        print(f"ğŸ§Š å†»ç»“ {self.model_type} éª¨å¹²ç½‘ç»œï¼Œåªè®­ç»ƒè‡ªå®šä¹‰åˆ†ç±»å™¨")
        for param in self.feature_extractor.parameters():
            param.requires_grad = False
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­ - ä½¿ç”¨ç‰¹å¾æå–å™¨ + è‡ªå®šä¹‰åˆ†ç±»å™¨çš„æ–¹å¼"""
        # é€šè¿‡é¢„è®­ç»ƒæ¨¡å‹æå–ç‰¹å¾
        features = self.feature_extractor(x)
        
        # ç‰¹å¾é™ç»´ (ä»åŸå§‹ç»´åº¦é™åˆ°æŒ‡å®šç»´åº¦)
        features = self.pool(features)
        
        # é€šè¿‡è‡ªå®šä¹‰åˆ†ç±»å™¨
        output = self.classifier(features)
        
        return output
    
    def get_transforms(self):
        """è·å–æ¨¡å‹å¯¹åº”çš„é¢„å¤„ç†transforms"""
        if hasattr(self.feature_extractor, 'transforms'):
            return self.feature_extractor.transforms()
        return None
    
    def _get_pretrain_info(self):
        """è·å–é¢„è®­ç»ƒæ¨¡å‹çš„æ•°æ®é›†ä¿¡æ¯"""
        pretrain_datasets = {
            'r3d_18': {
                'dataset': 'Kinetics-400',
                'classes': 400,
                'note': 'Kinetics-400åŒ…å«UCF-101çš„éƒ¨åˆ†åŠ¨ä½œç±»åˆ«ï¼Œè¿ç§»æ•ˆæœè¾ƒå¥½'
            },
            'mc3_18': {
                'dataset': 'Kinetics-400', 
                'classes': 400,
                'note': 'Kinetics-400åŒ…å«UCF-101çš„éƒ¨åˆ†åŠ¨ä½œç±»åˆ«ï¼Œè¿ç§»æ•ˆæœè¾ƒå¥½'
            },
            'r2plus1d_18': {
                'dataset': 'Kinetics-400',
                'classes': 400, 
                'note': 'Kinetics-400åŒ…å«UCF-101çš„éƒ¨åˆ†åŠ¨ä½œç±»åˆ«ï¼Œè¿ç§»æ•ˆæœè¾ƒå¥½'
            },
            's3d': {
                'dataset': 'Kinetics-400',
                'classes': 400,
                'note': 'S3Dæ¶æ„è¾ƒå¤æ‚ï¼Œå¯èƒ½éœ€è¦æ›´ä»”ç»†çš„åˆ†ç±»å¤´å¤„ç†'
            },
            'mvit_v1_b': {
                'dataset': 'Kinetics-400',
                'classes': 400,
                'note': 'MViTæ˜¯Transformeræ¶æ„ï¼Œé€šå¸¸è¿ç§»æ•ˆæœå¾ˆå¥½'
            },
            'mvit_v2_s': {
                'dataset': 'Kinetics-400',
                'classes': 400,
                'note': 'MViT v2æ”¹è¿›ç‰ˆæœ¬ï¼Œé€šå¸¸è¿ç§»æ•ˆæœå¾ˆå¥½'
            },
            'swin3d_b': {
                'dataset': 'Kinetics-400',
                'classes': 400,
                'note': 'Swin3Dæ˜¯è¾ƒæ–°çš„æ¶æ„ï¼Œå¯èƒ½éœ€è¦ç‰¹æ®Šçš„å¾®è°ƒç­–ç•¥'
            },
            'swin3d_s': {
                'dataset': 'Kinetics-400', 
                'classes': 400,
                'note': 'Swin3Dæ˜¯è¾ƒæ–°çš„æ¶æ„ï¼Œå¯èƒ½éœ€è¦ç‰¹æ®Šçš„å¾®è°ƒç­–ç•¥'
            },
            'swin3d_t': {
                'dataset': 'Kinetics-400',
                'classes': 400,
                'note': 'Swin3Dæ˜¯è¾ƒæ–°çš„æ¶æ„ï¼Œå¯èƒ½éœ€è¦ç‰¹æ®Šçš„å¾®è°ƒç­–ç•¥'
            }
        }
        
        return pretrain_datasets.get(self.model_type, {
            'dataset': 'Unknown',
            'classes': 'Unknown', 
            'note': ''
        })
    
    # æ—§çš„forwardæ–¹æ³•å·²åˆ é™¤ï¼Œä½¿ç”¨æ–°çš„å®ç°


def debug_model_structure(model, model_type):
    """è°ƒè¯•æ¨¡å‹ç»“æ„ï¼Œå¸®åŠ©ç†è§£åˆ†ç±»å¤´"""
    print(f"ğŸ” {model_type} æ¨¡å‹ç»“æ„åˆ†æ:")
    
    # æ‰“å°ä¸»è¦æ¨¡å—
    for name, module in model.named_children():
        print(f"  - {name}: {type(module).__name__}")
        
        # ç‰¹åˆ«å…³æ³¨åˆ†ç±»ç›¸å…³çš„å±‚
        if name in ['fc', 'classifier', 'head']:
            if isinstance(module, nn.Sequential):
                print(f"    SequentialåŒ…å«:")
                for i, sub_module in enumerate(module):
                    print(f"      [{i}] {type(sub_module).__name__}: {sub_module}")
            else:
                print(f"    {type(module).__name__}: {module}")


def get_model_specific_config(model_type):
    """è·å–æ¨¡å‹ç‰¹å®šçš„è®­ç»ƒé…ç½®"""
    configs = {
        # ResNet3Dç³»åˆ— - ç›¸å¯¹ç¨³å®šï¼Œä½¿ç”¨æ ‡å‡†é…ç½®
        'r3d_18': {
            'feature_dim': 512,
            'freeze_backbone': False,
            'suggested_lr': 0.001,
            'suggested_batch_size': 32
        },
        'mc3_18': {
            'feature_dim': 512,
            'freeze_backbone': False,
            'suggested_lr': 0.001,
            'suggested_batch_size': 32
        },
        'r2plus1d_18': {
            'feature_dim': 512,
            'freeze_backbone': False,
            'suggested_lr': 0.001,
            'suggested_batch_size': 32
        },
        # S3D - å¤æ‚æ¶æ„ï¼Œéœ€è¦æ›´å°å­¦ä¹ ç‡
        's3d': {
            'feature_dim': 512,  # é™ç»´åˆ°512
            'freeze_backbone': True,  # å…ˆå†»ç»“éª¨å¹²ç½‘ç»œ
            'suggested_lr': 0.0001,
            'suggested_batch_size': 16
        },
        # MViTç³»åˆ— - Transformeræ¶æ„ï¼Œé€šå¸¸æ•ˆæœå¥½
        'mvit_v1_b': {
            'feature_dim': 512,
            'freeze_backbone': False,
            'suggested_lr': 0.0005,
            'suggested_batch_size': 16
        },
        'mvit_v2_s': {
            'feature_dim': 512,
            'freeze_backbone': False,
            'suggested_lr': 0.0005,
            'suggested_batch_size': 16
        },
        # Swin3Dç³»åˆ— - æ–°æ¶æ„ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
        'swin3d_b': {
            'feature_dim': 256,  # æ›´å°çš„ç‰¹å¾ç»´åº¦
            'freeze_backbone': True,  # å†»ç»“éª¨å¹²ç½‘ç»œ
            'suggested_lr': 0.00001,
            'suggested_batch_size': 8
        },
        'swin3d_s': {
            'feature_dim': 256,
            'freeze_backbone': True,
            'suggested_lr': 0.00001,
            'suggested_batch_size': 8
        },
        'swin3d_t': {
            'feature_dim': 256,
            'freeze_backbone': True,
            'suggested_lr': 0.00001,
            'suggested_batch_size': 8
        }
    }
    
    return configs.get(model_type, {
        'feature_dim': 512,
        'freeze_backbone': False,
        'suggested_lr': 0.001,
        'suggested_batch_size': 32
    })


def get_video_model(model_type, num_classes=101, **kwargs):
    """
    è§†é¢‘æ¨¡å‹å·¥å‚å‡½æ•° - ä½¿ç”¨æ¨¡å‹ç‰¹å®šé…ç½®

    Args:
        model_type: æ¨¡å‹ç±»å‹
        num_classes: åˆ†ç±»ç±»åˆ«æ•°
        **kwargs: å…¶ä»–æ¨¡å‹å‚æ•°

    Returns:
        torch.nn.Module: é…ç½®å¥½çš„è§†é¢‘æ¨¡å‹å®ä¾‹
    """
    # è·å–æ¨¡å‹ç‰¹å®šé…ç½®
    model_config = get_model_specific_config(model_type)
    
    # åˆå¹¶ç”¨æˆ·å‚æ•°å’Œé»˜è®¤é…ç½®
    pretrained = kwargs.get('pretrained', True)
    feature_dim = kwargs.get('feature_dim', model_config['feature_dim'])
    freeze_backbone = kwargs.get('freeze_backbone', model_config['freeze_backbone'])
    debug = kwargs.get('debug', False)
    
    print(
        f"ğŸ—ï¸ åˆ›å»º {model_type} æ¨¡å‹ | "
        f"ğŸ¯ ç‰¹å¾ç»´åº¦: {feature_dim} | "
        f"ğŸ§Š å†»ç»“éª¨å¹²: {'æ˜¯' if freeze_backbone else 'å¦'} | "
        f"ğŸ“š å»ºè®®å­¦ä¹ ç‡: {model_config['suggested_lr']} | "
        f"ğŸ“¦ å»ºè®®æ‰¹å¤§å°: {model_config['suggested_batch_size']}"
    )
    
    model = VideoNetModel(
        model_type=model_type,
        num_classes=num_classes,
        pretrained=pretrained,
        feature_dim=feature_dim,
        freeze_backbone=freeze_backbone
    )
    
    # å¦‚æœå¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œæ‰“å°æ¨¡å‹ç»“æ„
    if debug:
        debug_model_structure(model.feature_extractor, model_type)
    
    return model