"""
å­¦ä¹ ç‡è°ƒåº¦å™¨å·¥å‚æ¨¡å—
åŒ…å«å¸¸ç”¨å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥çš„å®šä¹‰å’Œå·¥å‚å‡½æ•°
"""

import torch
import torch.optim.lr_scheduler as lr_scheduler


def get_scheduler(optimizer, scheduler_config=None, hyperparams=None, scheduler_name=None, **kwargs):
    """
    å­¦ä¹ ç‡è°ƒåº¦å™¨å·¥å‚å‡½æ•°ï¼Œåˆ›å»ºå¹¶é…ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨å®ä¾‹

    Args:
        optimizer (torch.optim.Optimizer): ä¼˜åŒ–å™¨å®ä¾‹
        scheduler_config (dict, optional): è°ƒåº¦å™¨é…ç½®å­—å…¸
        hyperparams (dict, optional): è¶…å‚æ•°å­—å…¸ï¼ŒåŒ…å«epochs, learning_rateç­‰ä¿¡æ¯
        scheduler_name (str, optional): è°ƒåº¦å™¨åç§°ï¼Œç”¨äºå‘åå…¼å®¹
        **kwargs: è°ƒåº¦å™¨å‚æ•°ï¼Œç”¨äºå‘åå…¼å®¹

    Returns:
        torch.optim.lr_scheduler._LRScheduler: é…ç½®å¥½çš„å­¦ä¹ ç‡è°ƒåº¦å™¨å®ä¾‹

    ç¤ºä¾‹ï¼š
        >>> scheduler = get_scheduler(optimizer, {'type': 'onecycle', 'max_lr': 0.1}, hyperparams)
        >>> scheduler = get_scheduler(optimizer, scheduler_name='cosine', T_max=100)
    """
    # ç®€åŒ–çš„é…ç½®è§£æ
    if scheduler_config:
        scheduler_name = scheduler_config.get('type') or scheduler_config.get('name', 'onecycle')
        params = scheduler_config.get('params', {}) if 'params' in scheduler_config else {k: v for k, v in scheduler_config.items() if k not in ['type', 'name']}
    else:
        scheduler_name = scheduler_name or 'onecycle'
        params = kwargs

    scheduler_name = scheduler_name.lower()

    # è·å–è¶…å‚æ•°
    epochs = hyperparams.get('epochs', 100) if hyperparams else 100
    learning_rate = hyperparams.get('learning_rate', 0.001) if hyperparams else 0.001

    if scheduler_name == "onecycle":
        return lr_scheduler.OneCycleLR(
            optimizer=optimizer,
            max_lr=params.get('max_lr', 5 * learning_rate),
            epochs=params.get('epochs', epochs),
            steps_per_epoch=params.get('steps_per_epoch', 100),
            pct_start=params.get('pct_start', 0.3),
            anneal_strategy=params.get('anneal_strategy', 'cos'),
            div_factor=params.get('div_factor', 25.0),
            final_div_factor=params.get('final_div_factor', 1e4),
            cycle_momentum=params.get('cycle_momentum', True),
            base_momentum=params.get('base_momentum', 0.85),
            max_momentum=params.get('max_momentum', 0.95)
        )
    elif scheduler_name == "step":
        return lr_scheduler.StepLR(
            optimizer=optimizer,
            step_size=params.get('step_size', 30),
            gamma=params.get('gamma', 0.1),
            last_epoch=params.get('last_epoch', -1)
        )
    elif scheduler_name == "cosine":
        # æ”¯æŒeta_min_factorå‚æ•°ï¼Œè®¡ç®—æœ€å°å­¦ä¹ ç‡
        eta_min_factor = params.get('eta_min_factor', 0.0)
        eta_min = params.get('eta_min', learning_rate * eta_min_factor)

        # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿T_maxä¸ä¸º0
        T_max = params.get('T_max', epochs)
        if T_max <= 0:
            print(f"âš ï¸ è­¦å‘Šï¼šT_max={T_max} æ— æ•ˆï¼Œcosineè°ƒåº¦å™¨é€€åŒ–ä¸ºå¸¸æ•°å­¦ä¹ ç‡")
            return lr_scheduler.ConstantLR(
                optimizer=optimizer,
                factor=1.0,  # ä¿æŒåŸå§‹å­¦ä¹ ç‡
                total_iters=max(1, epochs),
                last_epoch=params.get('last_epoch', -1)
            )

        return lr_scheduler.CosineAnnealingLR(
            optimizer=optimizer,
            T_max=T_max,
            eta_min=eta_min,
            last_epoch=params.get('last_epoch', -1)
        )
    elif scheduler_name == "exponential":
        return lr_scheduler.ExponentialLR(
            optimizer=optimizer,
            gamma=params.get('gamma', 0.95),
            last_epoch=params.get('last_epoch', -1)
        )
    elif scheduler_name == "plateau":
        return lr_scheduler.ReduceLROnPlateau(
            optimizer=optimizer,
            mode=params.get('mode', 'min'),
            factor=params.get('factor', 0.1),
            patience=params.get('patience', 10),
            threshold=params.get('threshold', 1e-4),
            min_lr=params.get('min_lr', 0),
            cooldown=params.get('cooldown', 0),
            eps=params.get('eps', 1e-8)
        )
    elif scheduler_name == "linear":
        # Linear decay scheduler (LinearLR)
        return lr_scheduler.LinearLR(
            optimizer=optimizer,
            start_factor=params.get('start_factor', 1.0),
            end_factor=params.get('end_factor', 0.1),
            total_iters=params.get('total_iters', epochs),
            last_epoch=params.get('last_epoch', -1)
        )
    elif scheduler_name == "multistep":
        return lr_scheduler.MultiStepLR(
            optimizer=optimizer,
            milestones=params.get('milestones', [30, 60, 90]),
            gamma=params.get('gamma', 0.1),
            last_epoch=params.get('last_epoch', -1)
        )
    elif scheduler_name == "warmup_cosine":
        # Warmup + Cosine Annealing scheduler
        warmup_epochs = params.get('warmup_epochs', max(1, epochs // 10))  # é»˜è®¤10%çš„epochç”¨äºwarmup

        # ğŸ”§ ä¿®å¤ï¼šå½“æ€»epochæ•°è¿‡å°‘æ—¶çš„å¤„ç†é€»è¾‘
        if epochs <= 1:
            # å½“åªæœ‰1ä¸ªepochæ—¶ï¼Œç›´æ¥ä½¿ç”¨å¸¸æ•°å­¦ä¹ ç‡è°ƒåº¦å™¨
            print(f"âš ï¸ è­¦å‘Šï¼šepochs={epochs} è¿‡å°‘ï¼Œwarmup_cosineè°ƒåº¦å™¨é€€åŒ–ä¸ºå¸¸æ•°å­¦ä¹ ç‡")
            return lr_scheduler.ConstantLR(
                optimizer=optimizer,
                factor=1.0,  # ä¿æŒåŸå§‹å­¦ä¹ ç‡
                total_iters=epochs,
                last_epoch=-1
            )

        # ç¡®ä¿warmup_epochsä¸ä¼šè¶…è¿‡æ€»epochs
        warmup_epochs = min(warmup_epochs, epochs - 1)
        cosine_epochs = epochs - warmup_epochs

        # å¦‚æœcosineé˜¶æ®µçš„epochæ•°ä¸º0ï¼Œåªä½¿ç”¨warmup
        if cosine_epochs <= 0:
            print(f"âš ï¸ è­¦å‘Šï¼šcosineé˜¶æ®µepochæ•°ä¸º{cosine_epochs}ï¼Œåªä½¿ç”¨warmupè°ƒåº¦å™¨")
            return lr_scheduler.LinearLR(
                optimizer=optimizer,
                start_factor=params.get('warmup_start_factor', 0.1),
                end_factor=1.0,
                total_iters=epochs,
                last_epoch=-1
            )

        # åˆ›å»ºç»„åˆè°ƒåº¦å™¨ï¼šå…ˆwarmupï¼Œå†cosine annealing
        warmup_scheduler = lr_scheduler.LinearLR(
            optimizer=optimizer,
            start_factor=params.get('warmup_start_factor', 0.1),  # ä»10%å­¦ä¹ ç‡å¼€å§‹
            end_factor=1.0,  # åˆ°è¾¾å®Œæ•´å­¦ä¹ ç‡
            total_iters=warmup_epochs,
            last_epoch=-1
        )

        # æ”¯æŒeta_min_factorå‚æ•°ï¼Œè®¡ç®—æœ€å°å­¦ä¹ ç‡
        eta_min_factor = params.get('eta_min_factor', 0.01)
        eta_min = params.get('eta_min', learning_rate * eta_min_factor)

        cosine_scheduler = lr_scheduler.CosineAnnealingLR(
            optimizer=optimizer,
            T_max=cosine_epochs,  # ä½¿ç”¨ä¿®å¤åçš„cosine_epochs
            eta_min=eta_min,  # æœ€å°å­¦ä¹ ç‡
            last_epoch=-1
        )

        # ä½¿ç”¨SequentialLRç»„åˆä¸¤ä¸ªè°ƒåº¦å™¨
        return lr_scheduler.SequentialLR(
            optimizer=optimizer,
            schedulers=[warmup_scheduler, cosine_scheduler],
            milestones=[warmup_epochs],
            last_epoch=-1
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„è°ƒåº¦å™¨: {scheduler_name}ã€‚æ”¯æŒçš„è°ƒåº¦å™¨: onecycle, step, cosine, exponential, plateau, linear, multistep, warmup_cosine")
