"""è®­ç»ƒæ—¥å¿—ç®¡ç†å™¨

ç»Ÿä¸€ç®¡ç†è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ‰€æœ‰ç»ˆç«¯è¾“å‡ºå’Œæ—¥å¿—è®°å½•ã€‚
"""

from typing import Dict, Any, Optional
from accelerate import Accelerator
from tqdm import tqdm


class TrainingLogger:
    """ç»Ÿä¸€çš„è®­ç»ƒæ—¥å¿—ç®¡ç†å™¨

    è´Ÿè´£ç®¡ç†è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ‰€æœ‰ç»ˆç«¯è¾“å‡ºã€‚

    ç‰¹æ€§ï¼š
    - è‡ªåŠ¨å¤„ç† accelerator.is_main_process æ£€æŸ¥
    - ç»Ÿä¸€çš„æ—¥å¿—æ ¼å¼
    - ç®€æ´æ¸…æ™°çš„è¾“å‡º
    """

    def __init__(self, accelerator: Accelerator):
        """åˆå§‹åŒ–æ—¥å¿—ç®¡ç†å™¨

        Args:
            accelerator: Acceleratorå®ä¾‹
        """
        self.accelerator = accelerator
    
    def info(self, message: str, force: bool = False):
        """æ‰“å°æ—¥å¿—æ¶ˆæ¯

        Args:
            message: æ—¥å¿—æ¶ˆæ¯
            force: æ˜¯å¦å¼ºåˆ¶æ‰“å°ï¼ˆå¿½ç•¥ä¸»è¿›ç¨‹æ£€æŸ¥ï¼‰
        """
        if force or self.accelerator.is_main_process:
            tqdm.write(message)
    
    def print_experiment_config(self, config: Dict[str, Any]):
        """æ‰“å°å®éªŒé…ç½®ï¼ˆç®€æ´æ¨¡å¼ï¼‰

        Args:
            config: å®éªŒé…ç½®å­—å…¸
        """
        if not self.accelerator.is_main_process:
            return

        # ç¬¬1è¡Œï¼šå®éªŒåç§°ã€æ¨¡å‹ã€æ•°æ®é›†
        model_params = config.get('model_params_m', 0)
        model_size_mb = config.get('model_size_mb', 0)
        self.info(f"ğŸš€ å®éªŒ: {config['exp_name']} | "
                  f"æ¨¡å‹: {config['model_name']} ({model_params:.1f}M, {model_size_mb:.1f}MB) | "
                  f"æ•°æ®: {config['dataset_type']}")

        # ç¬¬2è¡Œï¼šè®­ç»ƒé…ç½®
        train_size = config.get('train_size', 0)
        test_size = config.get('test_size', 0)
        data_pct = config.get('data_percentage', 1.0)
        self.info(f"ğŸ“Š æ•°æ®: è®­ç»ƒ{train_size:,} | æµ‹è¯•{test_size:,} | ä½¿ç”¨{data_pct:.0%} | "
                  f"é…ç½®: {config['epochs']}epÃ—bs{config['batch_size']}Ã—lr{config['learning_rate']}")

        # ç¬¬3è¡Œï¼šä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        scheduler_info = config.get('scheduler_info', 'default')
        optimizer_name = config.get('optimizer_name', 'adam')
        weight_decay = config.get('weight_decay', 0)
        self.info(f"âš™ï¸  ä¼˜åŒ–: {optimizer_name}(wd={weight_decay}) | è°ƒåº¦: {scheduler_info} | "
                  f"å¤šå¡: {'æ˜¯' if self.accelerator.num_processes > 1 else 'å¦'}")

        self.info("â•" * 80)
    
    def print_pos_weight_summary(self, total_samples: int, num_classes: int):
        """æ‰“å°pos_weightè®¡ç®—æ‘˜è¦

        Args:
            total_samples: æ€»æ ·æœ¬æ•°
            num_classes: ç±»åˆ«æ•°
        """
        if self.accelerator.is_main_process:
            self.info(f"âœ… pos_weightå·²è®¡ç®— (åŸºäº{total_samples:,}ä¸ªæ ·æœ¬ï¼Œ{num_classes}ä¸ªç±»åˆ«)")

    def print_learning_rate_info(self, lr_info: Dict[str, Any], epoch: int,
                                 total_epochs: int, phase: str = "å¼€å§‹"):
        """æ‰“å°å­¦ä¹ ç‡ä¿¡æ¯

        Args:
            lr_info: å­¦ä¹ ç‡ä¿¡æ¯å­—å…¸
            epoch: å½“å‰epoch
            total_epochs: æ€»epochæ•°
            phase: é˜¶æ®µæè¿°ï¼ˆ"å¼€å§‹" æˆ– "ç»“æŸ"ï¼‰
        """
        if self.accelerator.is_main_process:
            self.info(f"ğŸ“Š Epoch {epoch}/{total_epochs} {phase} | "
                     f"è°ƒåº¦ç­–ç•¥: {lr_info['scheduler_name']} | "
                     f"åˆå§‹LR: {lr_info['initial_lr']:.6f} | "
                     f"å½“å‰LR: {lr_info['current_lr']:.6f}")

    def print_experiment_info_full(self, config: Dict[str, Any], exp_name: str,
                                   task_info: Dict[str, Any], dataset_info: Dict[str, Any],
                                   model, train_dataloader, test_dataloader):
        """æ‰“å°å®Œæ•´çš„å®éªŒé…ç½®ä¿¡æ¯

        è´Ÿè´£æ‰“å°å®Œæ•´çš„å®éªŒé…ç½®ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ¨¡å‹ã€æ•°æ®ã€è®­ç»ƒé…ç½®ç­‰ã€‚

        Args:
            config: å®Œæ•´é…ç½®å­—å…¸
            exp_name: å®éªŒåç§°
            task_info: ä»»åŠ¡ä¿¡æ¯å­—å…¸
            dataset_info: æ•°æ®é›†ä¿¡æ¯å­—å…¸
            model: å·²åˆ›å»ºçš„æ¨¡å‹
            train_dataloader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            test_dataloader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
        """
        if not self.accelerator.is_main_process:
            return

        hyperparams = config['hp']
        data_config = config.get('data', {})
        model_config = config.get('model', {})
        dataset_type = data_config.get('type', 'cifar10')
        model_name = model_config.get('type', model_config.get('name', task_info['default_model']))

        # è·å–æ¨¡å‹å‚æ•°ä¿¡æ¯
        total_params = sum(p.numel() for p in model.parameters())
        model_size_bytes_per_param = 4  # float32
        bytes_to_mb = 1024 * 1024
        model_size_mb = total_params * model_size_bytes_per_param / bytes_to_mb

        # è°ƒåº¦å™¨ä¿¡æ¯
        scheduler_config = config.get('scheduler', {})
        scheduler_name = scheduler_config.get('name', 'default')
        scheduler_params = []
        if scheduler_name == 'warmup_cosine':
            warmup_epochs = scheduler_config.get('params', {}).get('warmup_epochs', 1)
            eta_min_factor = scheduler_config.get('params', {}).get('eta_min_factor', 0.01)
            scheduler_params.append(f"warmup_epochs={warmup_epochs}")
            scheduler_params.append(f"eta_min_factor={eta_min_factor}")

        scheduler_info = f"{scheduler_name}"
        if scheduler_params:
            scheduler_info += f" ({', '.join(scheduler_params)})"

        # ä¼˜åŒ–å™¨ä¿¡æ¯
        optimizer_name = config.get('optimizer', {}).get('name', 'adam')
        weight_decay = config.get('optimizer', {}).get('params', {}).get('weight_decay', 0)

        # æ„å»ºé…ç½®å­—å…¸
        config_dict = {
            'exp_name': exp_name,
            'model_name': model_name,
            'dataset_type': dataset_type,
            'task_description': task_info['description'],
            'model_params_m': total_params / 1e6,
            'model_size_mb': model_size_mb,
            'train_size': len(train_dataloader.dataset),
            'test_size': len(test_dataloader.dataset),
            'data_percentage': hyperparams.get('data_percentage', 1.0),
            'epochs': hyperparams['epochs'],
            'batch_size': hyperparams['batch_size'],
            'learning_rate': hyperparams['learning_rate'],
            'scheduler_info': scheduler_info,
            'optimizer_name': optimizer_name,
            'weight_decay': weight_decay
        }

        # ä½¿ç”¨æ—¥å¿—ç®¡ç†å™¨æ‰“å°ï¼ˆæ ¹æ®æ¨¡å¼é€‰æ‹©è¯¦ç»†ç¨‹åº¦ï¼‰
        self.print_experiment_config(config_dict)

