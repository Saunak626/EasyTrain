"""ç½‘æ ¼æœç´¢å¯åŠ¨è„šæœ¬

å®ç°è¶…å‚æ•°ç½‘æ ¼æœç´¢ï¼Œæ”¯æŒè¿›ç¨‹å†…è°ƒç”¨å’Œå¤šç§å‚æ•°ç»„åˆç­–ç•¥ã€‚
ä¸»è¦åŠŸèƒ½ï¼šå‚æ•°ç»„åˆç”Ÿæˆã€å®éªŒæ‰§è¡Œã€ç»“æœæ”¶é›†å’ŒCSVæŠ¥å‘Šç”Ÿæˆã€‚
"""
import itertools
import subprocess
import yaml
import os
import sys
import csv
import json
import random
import fcntl
from typing import Dict, List, Any, Optional, Tuple

from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.utils.config_parser import parse_arguments

# ============================================================================
# æ¨¡å—çº§å¸¸é‡é…ç½®
# ============================================================================

# ç½‘æ ¼æœç´¢ç›¸å…³å¸¸é‡
GRID_SEARCH_CONSTANTS = {
    'model_type_key': 'model.type',
    'batch_size_key': 'hp.batch_size',
    'group_key': 'group',
    'excluded_params': ['model.type', 'hp.batch_size'],
    'csv_base_columns': [
        'exp_name', 'model.type', 'group', 'success',
        'best_accuracy', 'final_accuracy', 'trained_epochs'
    ],
    'common_runtime_params': [
        'data_percentage',
        'optimizer.name', 'scheduler.name', 'loss.name'
    ],
    'excluded_csv_params': ['epochs', 'batch_size', 'learning_rate']
}

# ============================================================================
# å‚æ•°ç»„åˆç”Ÿæˆå™¨ç±»
# ============================================================================

class ParameterCombinationGenerator:
    """å‚æ•°ç»„åˆç”Ÿæˆå™¨

    è´Ÿè´£å¤„ç†ç½‘æ ¼æœç´¢çš„å‚æ•°ç»„åˆç”Ÿæˆé€»è¾‘ï¼Œæ”¯æŒåˆ†ç»„å¼é…ç½®å’Œæ¨¡å‹-batch_sizeæ™ºèƒ½é…å¯¹ã€‚
    """

    def __init__(self, config: Dict[str, Any]):
        """åˆå§‹åŒ–å‚æ•°ç»„åˆç”Ÿæˆå™¨

        Args:
            config: ç½‘æ ¼æœç´¢é…ç½®å­—å…¸
        """
        self.config = config
        self.constants = GRID_SEARCH_CONSTANTS

    def generate_combinations(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆå‚æ•°ç»„åˆçš„ä¸»å…¥å£å‡½æ•°

        Returns:
            å‚æ•°ç»„åˆåˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸ä»£è¡¨ä¸€ç»„å®éªŒå‚æ•°
        """
        gs = (self.config or {}).get("grid_search", {}) or {}
        fixed = gs.get("fixed", {}) or {}
        models_to_train = self.config.get("models_to_train", [])

        # åˆ†ç»„å¼é…ç½®å¤„ç†
        if "groups" in gs and gs["groups"]:
            print(f"ğŸ“‹ ä½¿ç”¨åˆ†ç»„å¼ç½‘æ ¼æœç´¢é…ç½®")
            return self._generate_combinations_by_groups(gs["groups"], fixed, models_to_train)

        # è¾¹ç•Œæƒ…å†µï¼šæ— æœç´¢å‚æ•°ï¼Œä»åŸºç¡€é…ç½®ä¸­æå–ä¿¡æ¯
        else:
            print(f"âš ï¸  æœªæ‰¾åˆ°groupsé…ç½®ï¼Œä»åŸºç¡€é…ç½®ä¸­æå–å‚æ•°")
            base_params = {}

            # ä»åŸºç¡€é…ç½®ä¸­æå–æ¨¡å‹ç±»å‹
            if 'model' in self.config and 'type' in self.config['model']:
                base_params[self.constants['model_type_key']] = self.config['model']['type']

            # ä»åŸºç¡€é…ç½®ä¸­æå–å…¶ä»–å‚æ•°
            if 'optimizer' in self.config and 'name' in self.config['optimizer']:
                base_params['optimizer.name'] = self.config['optimizer']['name']

            if 'scheduler' in self.config and 'name' in self.config['scheduler']:
                base_params['scheduler.name'] = self.config['scheduler']['name']

            if 'loss' in self.config and 'name' in self.config['loss']:
                base_params['loss.name'] = self.config['loss']['name']

            # è®¾ç½®é»˜è®¤ç»„å
            base_params[self.constants['group_key']] = 'default'

            # åˆå¹¶å›ºå®šå‚æ•°
            result_params = {**fixed, **base_params}
            return [result_params] if result_params else [{}]

    def _generate_combinations_by_groups(self, groups_config: Dict[str, Any],
                                       fixed: Dict[str, Any],
                                       models_to_train: List[str]) -> List[Dict[str, Any]]:
        """åˆ†ç»„å¼å‚æ•°ç»„åˆç”Ÿæˆå™¨ - æ”¯æŒç»„å†…æ¨¡å‹-batch_sizeæ™ºèƒ½é…å¯¹

        Args:
            groups_config: åˆ†ç»„é…ç½®å­—å…¸
            fixed: å›ºå®šå‚æ•°å­—å…¸
            models_to_train: è¦è®­ç»ƒçš„æ¨¡å‹åˆ—è¡¨

        Returns:
            æ‰€æœ‰ç»„åˆçš„å‚æ•°åˆ—è¡¨
        """
        all_combinations = []
        total_groups = len(groups_config)

        print(f"ğŸ¯ å‘ç° {total_groups} ä¸ªæ¨¡å‹ç»„:")
        for group_name in groups_config.keys():
            group_models = _as_list(groups_config[group_name].get(self.constants['model_type_key'], []))
            print(f"   - {group_name}: {group_models}")

        for group_name, group_params in groups_config.items():
            print(f"\nğŸ”§ å¤„ç†æ¨¡å‹ç»„: {group_name}")

            # ç¬¬1æ­¥ï¼šè§£æç»„é…ç½®
            group_models, group_batch_sizes = self._parse_group_config(group_params)

            # ç¬¬2æ­¥ï¼šå¤„ç†æ¨¡å‹-batch_sizeé…å¯¹é€»è¾‘
            model_batch_dict = self._handle_model_batch_pairing(group_models, group_batch_sizes, group_name)

            # ç¬¬3æ­¥ï¼šè¿‡æ»¤å¯ç”¨çš„æ¨¡å‹
            enabled_models = self._filter_enabled_models(group_models, models_to_train, group_name)
            if not enabled_models:
                continue

            # ç¬¬4æ­¥ï¼šç”Ÿæˆå‚æ•°ç»„åˆ
            group_combinations = self._generate_parameter_combinations(
                enabled_models, group_params, fixed, group_name, model_batch_dict
            )
            all_combinations.extend(group_combinations)

            # ç¬¬5æ­¥ï¼šæ‰“å°ç»„åˆç»Ÿè®¡ä¿¡æ¯
            self._print_group_statistics(group_name, group_combinations, group_params,
                                       enabled_models, model_batch_dict)

        print(f"\nğŸ‰ åˆ†ç»„å¼æœç´¢æ€»è®¡ç”Ÿæˆ {len(all_combinations)} ä¸ªç»„åˆ")
        return all_combinations

    def _parse_group_config(self, group_params: Dict[str, Any]) -> Tuple[List[str], List[Any]]:
        """è§£æç»„é…ç½®

        ä»ç»„å‚æ•°ä¸­æå–æ¨¡å‹åˆ—è¡¨å’Œbatch_sizeåˆ—è¡¨ã€‚

        Args:
            group_params: ç»„å‚æ•°å­—å…¸

        Returns:
            Tuple[æ¨¡å‹åˆ—è¡¨, batch_sizeåˆ—è¡¨]
        """
        group_models = _as_list(group_params.get(self.constants['model_type_key'], []))
        group_batch_sizes = _as_list(group_params.get(self.constants['batch_size_key'], []))

        print(f"   ğŸ“‹ ç»„å†…é…ç½®:")
        print(f"      {self.constants['model_type_key']}: {group_models} (é•¿åº¦: {len(group_models)})")
        print(f"      {self.constants['batch_size_key']}: {group_batch_sizes} (é•¿åº¦: {len(group_batch_sizes)})")

        return group_models, group_batch_sizes

    def _handle_model_batch_pairing(self, group_models: List[str], group_batch_sizes: List[Any],
                                   group_name: str) -> Optional[Dict[str, Any]]:
        """å¤„ç†æ¨¡å‹-batch_sizeé…å¯¹é€»è¾‘

        æ ¹æ®æ¨¡å‹å’Œbatch_sizeçš„æ•°é‡å…³ç³»ï¼Œå†³å®šé…å¯¹ç­–ç•¥ã€‚

        Args:
            group_models: æ¨¡å‹åˆ—è¡¨
            group_batch_sizes: batch_sizeåˆ—è¡¨
            group_name: ç»„åç§°

        Returns:
            æ¨¡å‹-batch_sizeé…å¯¹å­—å…¸ï¼Œå¦‚æœéœ€è¦ç‹¬ç«‹å¤„ç†åˆ™è¿”å›None
        """
        if group_batch_sizes:
            if len(group_batch_sizes) == 1:
                # æƒ…å†µ1ï¼šbatch_sizeé•¿åº¦=1ï¼Œæ‰©å……åˆ°ä¸model.typeä¸€è‡´
                group_batch_sizes = group_batch_sizes * len(group_models)
                print(f"   ğŸ”„ æ‰©å……batch_size: {group_batch_sizes} (æ‰©å……åˆ°ä¸model.typeé•¿åº¦ä¸€è‡´)")
                # åˆ›å»ºä¸€å¯¹ä¸€é…å¯¹å­—å…¸
                model_batch_dict = dict(zip(group_models, group_batch_sizes))
            elif len(group_batch_sizes) == len(group_models):
                # æƒ…å†µ2ï¼šbatch_sizeé•¿åº¦=model.typeé•¿åº¦ï¼ŒæŒ‰é¡ºåºé…å¯¹
                print(f"   âœ… é•¿åº¦åŒ¹é…ï¼Œå°†æŒ‰é¡ºåºé…å¯¹")
                model_batch_dict = dict(zip(group_models, group_batch_sizes))
            else:
                # æƒ…å†µ3ï¼šbatch_sizeé•¿åº¦â‰ 1ä¸”â‰ model.typeé•¿åº¦ï¼Œä½œä¸ºç‹¬ç«‹å‚æ•°å¤„ç†
                print(f"   ğŸ”„ batch_sizeä½œä¸ºç‹¬ç«‹å‚æ•°å¤„ç†ï¼Œå°†ä¸æ¨¡å‹è¿›è¡Œç¬›å¡å°”ç§¯ç»„åˆ")
                model_batch_dict = None  # æ ‡è®°ä¸ºç‹¬ç«‹å‚æ•°å¤„ç†
        else:
            # æ²¡æœ‰batch_sizeé…ç½®ï¼Œæ‰€æœ‰æ¨¡å‹ä½¿ç”¨é»˜è®¤å€¼
            model_batch_dict = {model: None for model in group_models}
            print(f"   ğŸ“Š æ— batch_sizeé…ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼")

        return model_batch_dict

    def _filter_enabled_models(self, group_models: List[str], models_to_train: List[str],
                              group_name: str) -> List[str]:
        """è¿‡æ»¤å¯ç”¨çš„æ¨¡å‹

        æ ¹æ®models_to_trainé…ç½®è¿‡æ»¤å‡ºéœ€è¦è®­ç»ƒçš„æ¨¡å‹ã€‚

        Args:
            group_models: ç»„å†…æ‰€æœ‰æ¨¡å‹
            models_to_train: è¦è®­ç»ƒçš„æ¨¡å‹åˆ—è¡¨
            group_name: ç»„åç§°

        Returns:
            å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
        """
        if models_to_train:
            enabled_models = [model for model in group_models if model in models_to_train]
            if not enabled_models:
                print(f"   â­ï¸  è·³è¿‡ç»„ {group_name}ï¼šæ— å¯ç”¨çš„æ¨¡å‹")
                return []
        else:
            enabled_models = group_models

        return enabled_models

    def _generate_parameter_combinations(self, enabled_models: List[str], group_params: Dict[str, Any],
                                       fixed: Dict[str, Any], group_name: str,
                                       model_batch_dict: Optional[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆå‚æ•°ç»„åˆ

        æ ¹æ®æ¨¡å‹-batch_sizeé…å¯¹ç­–ç•¥ç”Ÿæˆæ‰€æœ‰å‚æ•°ç»„åˆã€‚

        Args:
            enabled_models: å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
            group_params: ç»„å‚æ•°å­—å…¸
            fixed: å›ºå®šå‚æ•°å­—å…¸
            group_name: ç»„åç§°
            model_batch_dict: æ¨¡å‹-batch_sizeé…å¯¹å­—å…¸ï¼ŒNoneè¡¨ç¤ºç‹¬ç«‹å¤„ç†

        Returns:
            å‚æ•°ç»„åˆåˆ—è¡¨
        """
        combinations = []

        if model_batch_dict is not None:
            # æœ‰æ¨¡å‹-batch_sizeé…å¯¹çš„æƒ…å†µ
            enabled_pairs = {model: batch_size for model, batch_size in model_batch_dict.items()
                            if model in enabled_models}
            print(f"   ğŸ¯ å¯ç”¨çš„æ¨¡å‹é…å¯¹: {enabled_pairs}")

            # è·å–å…¶ä»–å‚æ•°ï¼ˆæ’é™¤model.typeå’Œhp.batch_sizeï¼‰
            other_params = {k: v for k, v in group_params.items()
                           if k not in self.constants['excluded_params']}

            # ç”Ÿæˆç»„åˆ
            if not other_params:
                # åªæœ‰æ¨¡å‹-batch_sizeé…å¯¹ï¼Œæ— å…¶ä»–å‚æ•°
                for model, batch_size in enabled_pairs.items():
                    combo = {**fixed, self.constants['model_type_key']: model, self.constants['group_key']: group_name}
                    if batch_size is not None:
                        combo[self.constants['batch_size_key']] = batch_size
                    combinations.append(combo)
            else:
                # æœ‰å…¶ä»–å‚æ•°ï¼Œè¿›è¡Œç¬›å¡å°”ç§¯ç»„åˆ
                param_items = [(k, _as_list(v)) for k, v in other_params.items() if _as_list(v)]
                if param_items:
                    param_keys, param_values_lists = zip(*param_items)
                    for model, batch_size in enabled_pairs.items():
                        for param_combo in itertools.product(*param_values_lists):
                            combo = {
                                **fixed,
                                self.constants['model_type_key']: model,
                                self.constants['group_key']: group_name
                            }
                            if batch_size is not None:
                                combo[self.constants['batch_size_key']] = batch_size
                            combo.update(dict(zip(param_keys, param_combo)))
                            combinations.append(combo)
                else:
                    # å…¶ä»–å‚æ•°éƒ½ä¸ºç©º
                    for model, batch_size in enabled_pairs.items():
                        combo = {**fixed, self.constants['model_type_key']: model, self.constants['group_key']: group_name}
                        if batch_size is not None:
                            combo[self.constants['batch_size_key']] = batch_size
                        combinations.append(combo)
        else:
            # batch_sizeä½œä¸ºç‹¬ç«‹å‚æ•°ï¼Œä¸æ¨¡å‹è¿›è¡Œç¬›å¡å°”ç§¯ç»„åˆ
            all_params = {k: v for k, v in group_params.items() if k != self.constants['model_type_key']}
            param_items = [(k, _as_list(v)) for k, v in all_params.items() if _as_list(v)]

            if param_items:
                param_keys, param_values_lists = zip(*param_items)
                for model in enabled_models:
                    for param_combo in itertools.product(*param_values_lists):
                        combo = {
                            **fixed,
                            self.constants['model_type_key']: model,
                            self.constants['group_key']: group_name
                        }
                        combo.update(dict(zip(param_keys, param_combo)))
                        combinations.append(combo)
            else:
                # æ— å…¶ä»–å‚æ•°
                for model in enabled_models:
                    combo = {**fixed, self.constants['model_type_key']: model, self.constants['group_key']: group_name}
                    combinations.append(combo)

        return combinations

    def _print_group_statistics(self, group_name: str, group_combinations: List[Dict[str, Any]],
                               group_params: Dict[str, Any], enabled_models: List[str],
                               model_batch_dict: Optional[Dict[str, Any]]) -> None:
        """æ‰“å°ç»„åˆç»Ÿè®¡ä¿¡æ¯

        Args:
            group_name: ç»„åç§°
            group_combinations: ç»„åˆåˆ—è¡¨
            group_params: ç»„å‚æ•°å­—å…¸
            enabled_models: å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
            model_batch_dict: æ¨¡å‹-batch_sizeé…å¯¹å­—å…¸
        """
        group_count = len(group_combinations)

        # è®¡ç®—ç»„åˆæ•°é‡çš„åˆ†è§£
        if model_batch_dict is not None:
            # æœ‰æ¨¡å‹-batch_sizeé…å¯¹çš„æƒ…å†µ
            enabled_pairs = {model: batch_size for model, batch_size in model_batch_dict.items()
                            if model in enabled_models}
            model_count = len(enabled_pairs)
            other_params = {k: v for k, v in group_params.items()
                           if k not in self.constants['excluded_params']}
            param_items = [(k, _as_list(v)) for k, v in other_params.items() if _as_list(v)]
            if param_items:
                param_counts = [len(_as_list(v)) for _, v in other_params.items() if _as_list(v)]
                other_count = 1
                for count in param_counts:
                    other_count *= count
                print(f"   âœ… ç»„ {group_name} ç”Ÿæˆ {group_count} ä¸ªç»„åˆ ({model_count}æ¨¡å‹ Ã— {other_count}å‚æ•°ç»„åˆ)")
            else:
                print(f"   âœ… ç»„ {group_name} ç”Ÿæˆ {group_count} ä¸ªç»„åˆ ({model_count}æ¨¡å‹)")
        else:
            # batch_sizeä½œä¸ºç‹¬ç«‹å‚æ•°çš„æƒ…å†µ
            model_count = len(enabled_models)
            all_params = {k: v for k, v in group_params.items() if k != self.constants['model_type_key']}
            param_items = [(k, _as_list(v)) for k, v in all_params.items() if _as_list(v)]
            if param_items:
                param_counts = [len(_as_list(v)) for _, v in all_params.items() if _as_list(v)]
                other_count = 1
                for count in param_counts:
                    other_count *= count
                print(f"   âœ… ç»„ {group_name} ç”Ÿæˆ {group_count} ä¸ªç»„åˆ ({model_count}æ¨¡å‹ Ã— {other_count}å‚æ•°ç»„åˆ)")
            else:
                print(f"   âœ… ç»„ {group_name} ç”Ÿæˆ {group_count} ä¸ªç»„åˆ ({model_count}æ¨¡å‹)")


# ============================================================================
# å®éªŒç»“æœç®¡ç†å™¨ç±»
# ============================================================================

class ExperimentResultsManager:
    """å®éªŒç»“æœç®¡ç†å™¨

    è´Ÿè´£ç®¡ç†CSVæ–‡ä»¶çš„åˆ›å»ºã€å†™å…¥å’Œå­—æ®µåç”Ÿæˆç­‰æ“ä½œã€‚
    """

    def __init__(self, csv_filepath: str):
        """åˆå§‹åŒ–å®éªŒç»“æœç®¡ç†å™¨

        Args:
            csv_filepath: CSVæ–‡ä»¶è·¯å¾„
        """
        self.csv_filepath = csv_filepath
        self.fieldnames = None
        self.constants = GRID_SEARCH_CONSTANTS

    def get_csv_fieldnames(self, all_params: List[Dict[str, Any]]) -> List[str]:
        """è·å–CSVæ–‡ä»¶çš„å­—æ®µååˆ—è¡¨

        Args:
            all_params: æ‰€æœ‰å‚æ•°ç»„åˆåˆ—è¡¨

        Returns:
            CSVå­—æ®µååˆ—è¡¨
        """
        param_keys = sorted({k for params in all_params for k in params.keys()})

        # åˆå¹¶æ‰€æœ‰å‚æ•°é”®ï¼Œå»é‡å¹¶æ’åºï¼Œæ’é™¤å†—ä½™å‚æ•°
        all_param_keys = sorted(set(param_keys + self.constants['common_runtime_params']) - set(self.constants['excluded_csv_params']))

        # å°†model.typeç§»åˆ°ç¬¬3åˆ—ï¼Œgroupç§»åˆ°ç¬¬4åˆ—ï¼Œå…¶ä»–å‚æ•°æŒ‰åŸé¡ºåºæ’åˆ—
        other_param_keys = [k for k in all_param_keys if k not in [self.constants['model_type_key'], self.constants['group_key']]]

        fieldnames = self.constants['csv_base_columns'] + other_param_keys
        self.fieldnames = fieldnames
        return fieldnames

    def initialize_csv_file(self, fieldnames: List[str]) -> None:
        """åˆå§‹åŒ–CSVæ–‡ä»¶ï¼Œå†™å…¥è¡¨å¤´

        Args:
            fieldnames: CSVå­—æ®µååˆ—è¡¨
        """
        results_dir = os.path.dirname(self.csv_filepath)
        os.makedirs(results_dir, exist_ok=True)

        with open(self.csv_filepath, "w", newline="", encoding="utf-8") as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()

        self.fieldnames = fieldnames

    def append_result_to_csv(self, result: Dict[str, Any]) -> None:
        """å®æ—¶è¿½åŠ å•ä¸ªç»“æœåˆ°CSVæ–‡ä»¶ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰

        Args:
            result: å®éªŒç»“æœ
        """
        if not self.fieldnames:
            raise ValueError("CSVå­—æ®µåæœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨initialize_csv_file")

        try:
            # å‡†å¤‡è¡Œæ•°æ®
            row = {
                "exp_name": result.get("exp_name"),
                "success": result.get("success"),
                "best_accuracy": result.get("best_accuracy"),
                "final_accuracy": result.get("final_accuracy"),
                "trained_epochs": result.get("trained_epochs", 0),
            }
            row.update(result.get("params", {}))

            # åªå†™å…¥fieldnamesä¸­å­˜åœ¨çš„å­—æ®µï¼Œå¿½ç•¥é¢å¤–å­—æ®µ
            filtered_row = {k: v for k, v in row.items() if k in self.fieldnames}

            # æ£€æŸ¥æ˜¯å¦æœ‰ç¼ºå¤±çš„å¿…éœ€å­—æ®µ
            missing_fields = [k for k in self.fieldnames if k not in row]
            if missing_fields:
                print(f"âš ï¸  ç¼ºå¤±å­—æ®µ: {missing_fields}ï¼Œå°†ä½¿ç”¨ç©ºå€¼å¡«å……")
                for field in missing_fields:
                    filtered_row[field] = ""

            # ä½¿ç”¨æ–‡ä»¶é”ç¡®ä¿çº¿ç¨‹å®‰å…¨
            with open(self.csv_filepath, "a", newline="", encoding="utf-8") as csvfile:
                # è·å–æ–‡ä»¶é”
                fcntl.flock(csvfile.fileno(), fcntl.LOCK_EX)

                writer = csv.DictWriter(csvfile, fieldnames=self.fieldnames)
                writer.writerow(filtered_row)
                csvfile.flush()  # å¼ºåˆ¶åˆ·æ–°ç¼“å†²åŒº

                # é‡Šæ”¾æ–‡ä»¶é”
                fcntl.flock(csvfile.fileno(), fcntl.LOCK_UN)

            print(f"âœ… CSVå†™å…¥æˆåŠŸ: {result.get('exp_name', 'unknown')}")

            # å¦‚æœæœ‰é¢å¤–å­—æ®µï¼Œç»™å‡ºæç¤º
            extra_fields = [k for k in row.keys() if k not in self.fieldnames]
            if extra_fields:
                print(f"â„¹ï¸  å¿½ç•¥é¢å¤–å­—æ®µ: {extra_fields}")

        except Exception as e:
            print(f"âš ï¸  å†™å…¥CSVå¤±è´¥: {e}")
            print(f"   æ–‡ä»¶è·¯å¾„: {self.csv_filepath}")
            print(f"   å½“å‰å­—æ®µå: {self.fieldnames}")
            print(f"   è¡Œæ•°æ®é”®: {list(row.keys()) if 'row' in locals() else 'N/A'}")
            print(f"   ç»“æœæ•°æ®: {result}")


def load_grid_config(path: str = "config/grid.yaml") -> Dict[str, Any]:
    """åŠ è½½ç½‘æ ¼æœç´¢é…ç½®"""
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def _as_list(v: Any) -> List[Any]:
    """å°†è¾“å…¥è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼

    Args:
        v: ä»»æ„ç±»å‹çš„å‚æ•°å€¼

    Returns:
        ç»Ÿä¸€æ ¼å¼åŒ–åçš„åˆ—è¡¨
    """
    if v is None:
        return []
    return v if isinstance(v, (list, tuple)) else [v]

def generate_combinations(config: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    åˆ†ç»„å¼å‚æ•°ç»„åˆç”Ÿæˆå™¨

    è®¾è®¡é€»è¾‘ï¼š
    1. ä»YAMLä¸­è·å–groupsé…ç½®ï¼Œæ¯ç»„æœ‰è‡ªå·±çš„æ¨¡å‹å’Œè¶…å‚æ•°èŒƒå›´
    2. ä¸ºæ¯ç»„å†…çš„å‚æ•°è¿›è¡Œç¬›å¡å°”ç§¯ç»„åˆ
    3. æ ¹æ®models_to_trainè¿‡æ»¤å¯ç”¨çš„æ¨¡å‹
    4. é¿å…æ— æ„ä¹‰çš„æ¨¡å‹-å‚æ•°ç»„åˆï¼ŒèŠ‚çœç®—åŠ›

    Args:
        config: ç½‘æ ¼æœç´¢é…ç½®

    Returns:
        å‚æ•°ç»„åˆåˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸ä»£è¡¨ä¸€ç»„å®éªŒå‚æ•°
    """
    generator = ParameterCombinationGenerator(config)
    return generator.generate_combinations()



# ============================================================================
# å‘åå…¼å®¹çš„å‡½æ•°æ¥å£
# ============================================================================

def get_csv_fieldnames(all_params: List[Dict[str, Any]]) -> List[str]:
    """è·å–CSVæ–‡ä»¶çš„å­—æ®µååˆ—è¡¨ï¼ˆå‘åå…¼å®¹æ¥å£ï¼‰"""
    # åˆ›å»ºä¸´æ—¶çš„ç»“æœç®¡ç†å™¨æ¥ç”Ÿæˆå­—æ®µå
    temp_manager = ExperimentResultsManager("")
    return temp_manager.get_csv_fieldnames(all_params)


def initialize_csv_file(filepath: str, fieldnames: List[str]) -> None:
    """åˆå§‹åŒ–CSVæ–‡ä»¶ï¼Œå†™å…¥è¡¨å¤´ï¼ˆå‘åå…¼å®¹æ¥å£ï¼‰"""
    manager = ExperimentResultsManager(filepath)
    manager.initialize_csv_file(fieldnames)


def append_result_to_csv(result: Dict[str, Any], filepath: str, fieldnames: List[str], experiment_id: int = None) -> None:
    """å®æ—¶è¿½åŠ å•ä¸ªç»“æœåˆ°CSVæ–‡ä»¶ï¼ˆå‘åå…¼å®¹æ¥å£ï¼‰"""
    manager = ExperimentResultsManager(filepath)
    manager.fieldnames = fieldnames  # è®¾ç½®å­—æ®µå
    manager.append_result_to_csv(result)





def save_results_to_csv(results: List[Dict[str, Any]], filename: str) -> Optional[str]:
    """ä¿å­˜å®éªŒç»“æœåˆ°CSVæ–‡ä»¶ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰

    Args:
        results: å®éªŒç»“æœåˆ—è¡¨
        filename: CSVæ–‡ä»¶å

    Returns:
        ä¿å­˜çš„æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæ— ç»“æœåˆ™è¿”å›None
    """
    if not results:
        print("âš ï¸  æ— ç»“æœæ•°æ®ï¼Œè·³è¿‡CSVä¿å­˜")
        return None

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs("runs", exist_ok=True)
    filepath = f"runs/{filename}"

    # åˆ›å»ºç»“æœç®¡ç†å™¨
    manager = ExperimentResultsManager(filepath)

    # è·å–å­—æ®µåå¹¶åˆå§‹åŒ–CSVæ–‡ä»¶
    fieldnames = manager.get_csv_fieldnames([r.get("params", {}) for r in results])
    manager.initialize_csv_file(fieldnames)

    # å†™å…¥æ‰€æœ‰ç»“æœ
    for result in results:
        manager.append_result_to_csv(result)

    print(f"ğŸ“Š å®éªŒç»“æœå·²ä¿å­˜åˆ°: {filepath}")
    print(f"   æ€»å®éªŒæ•°: {len(results)}")
    print(f"   æˆåŠŸå®éªŒ: {sum(1 for r in results if r.get('success', False))}")
    print(f"   å¤±è´¥å®éªŒ: {sum(1 for r in results if not r.get('success', False))}")

    return filepath


def apply_param_overrides(config, params):
    """åº”ç”¨å‚æ•°è¦†ç›–åˆ°é…ç½®å­—å…¸

    Args:
        config (dict): åŸºç¡€é…ç½®å­—å…¸
        params (dict): å‚æ•°è¦†ç›–å­—å…¸ï¼Œæ”¯æŒåµŒå¥—è·¯å¾„

    Returns:
        dict: åº”ç”¨è¦†ç›–åçš„é…ç½®å­—å…¸
    """
    import copy
    config = copy.deepcopy(config)
    
    for k, v in (params or {}).items():
        keys = k.split('.')
        target = config
        for key in keys[:-1]:
            target = target.setdefault(key, {})
        target[keys[-1]] = v
    
    return config


def run_single_experiment_in_process(params, exp_id, config_path):
    """è¿›ç¨‹å†…è°ƒç”¨æ–¹å¼è¿è¡Œå•ä¸ªå®éªŒï¼ˆå•å¡è®­ç»ƒï¼‰"""
    exp_name = f"grid_{exp_id}"
    
    try:
        # å¯¼å…¥è®­ç»ƒå‡½æ•°
        from src.trainers.base_trainer import run_training
        
        # åŠ è½½åŸºç¡€é…ç½®
        config = load_grid_config(config_path)
        
        # åº”ç”¨å‚æ•°è¦†ç›–
        config = apply_param_overrides(config, params)
        
        # ç›´æ¥è°ƒç”¨è®­ç»ƒå‡½æ•°
        result = run_training(config, exp_name)
        
        # æ·»åŠ å‚æ•°ä¿¡æ¯åˆ°ç»“æœä¸­
        result["params"] = params
        
        return result
        
    except Exception as e:
        return {
            "success": False,
            "exp_name": exp_name,
            "params": params,
            "best_accuracy": 0.0,
            "final_accuracy": 0.0,
            "trained_epochs": 0,
            "error": str(e)
        }


def run_single_experiment_subprocess(params, exp_id, use_multi_gpu, config_path):
    """å­è¿›ç¨‹æ–¹å¼è¿è¡Œå•ä¸ªå®éªŒï¼ˆå¤šå¡è®­ç»ƒï¼‰"""
    exp_name = f"grid_{exp_id}"
    
    # åˆ›å»ºä¸´æ—¶ç»“æœæ–‡ä»¶ç”¨äºè¿›ç¨‹é—´é€šä¿¡
    temp_result_file = f"/tmp/grid_result_{exp_id}_{random.randint(1000,9999)}.json"
    
    # ç»„è£…å‘½ä»¤
    if use_multi_gpu:
        import torch  # å±€éƒ¨å¯¼å…¥ï¼Œä»…åœ¨éœ€è¦æ—¶ä½¿ç”¨
        cmd = ["accelerate", "launch", "--multi_gpu", "--num_processes", str(torch.cuda.device_count())]
    else:
        cmd = [sys.executable, "-u"]
    
    # æ·»åŠ è®­ç»ƒè„šæœ¬å’ŒåŸºç¡€å‚æ•°
    cmd.extend(["scripts/train.py", "--config", config_path, "--exp_name", exp_name])
    cmd.extend(["--result_file", temp_result_file])  # æ–°å¢ï¼šæŒ‡å®šç»“æœæ–‡ä»¶
    
    # æ·»åŠ å‚æ•°è¦†ç›–ï¼ˆæ’é™¤groupå‚æ•°ï¼Œå®ƒåªç”¨äºè®°å½•ï¼‰
    for k, v in (params or {}).items():
        if k != "group":  # groupå‚æ•°ä¸ä¼ é€’ç»™è®­ç»ƒè„šæœ¬
            cmd.extend([f"--{k}", str(v)])

    # æ¸…ç†ç¯å¢ƒå˜é‡å¹¶è®¾ç½®å”¯ä¸€ç«¯å£
    env = os.environ.copy()
    for k in ["LOCAL_RANK", "RANK", "WORLD_SIZE", "MASTER_ADDR", "MASTER_PORT"]:
        env.pop(k, None)
    env["MASTER_ADDR"] = env.get("MASTER_ADDR", "127.0.0.1")
    env["MASTER_PORT"] = str(20000 + random.randint(0, 10000))

    # å¯åŠ¨å­è¿›ç¨‹
    process = subprocess.Popen(cmd, env=env)
    try:
        rc = process.wait()
    except KeyboardInterrupt:
        print(f"æ•è·åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ç»ˆæ­¢å­è¿›ç¨‹ {process.pid}...")
        process.terminate()
        process.wait()
        raise

    success = (rc == 0)
    
    # è¯»å–ç»“æœæ–‡ä»¶
    try:
        if os.path.exists(temp_result_file):
            with open(temp_result_file, 'r', encoding='utf-8') as f:
                result = json.load(f)
            os.remove(temp_result_file)  # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            
            # ç¡®ä¿ç»“æœåŒ…å«å¿…è¦çš„å­—æ®µ
            result["params"] = params
            result["success"] = result.get("success", success)
            result["exp_name"] = result.get("exp_name", exp_name)
            
            # ç¡®ä¿accuracyå­—æ®µä¸ä¸ºNone
            if result.get("best_accuracy") is None:
                result["best_accuracy"] = 0.0
            if result.get("final_accuracy") is None:
                result["final_accuracy"] = 0.0
                
            return result
        else:
            print(f"ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {temp_result_file}")
    except Exception as e:
        print(f"è¯»å–ç»“æœæ–‡ä»¶å¤±è´¥: {e}")
        if os.path.exists(temp_result_file):
            try:
                with open(temp_result_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                print(f"æ–‡ä»¶å†…å®¹: {content[:200]}...")  # æ˜¾ç¤ºå‰200ä¸ªå­—ç¬¦
            except:
                pass
    
    # å›é€€ï¼šè¿”å›é»˜è®¤ç»“æœ
    return {
        "success": success,
        "exp_name": exp_name,
        "params": params,
        "best_accuracy": 0.0,
        "final_accuracy": 0.0,
        "trained_epochs": 0,
        "error": "Failed to read result file" if success else "Training process failed"
    }


def run_single_experiment(params, exp_id, use_multi_gpu=False, config_path="config/grid.yaml"):
    """è¿è¡Œå•ä¸ªå®éªŒ

    Args:
        params (dict): å®éªŒå‚æ•°è¦†ç›–
        exp_id (str): å®éªŒID
        use_multi_gpu (bool): æ˜¯å¦ä½¿ç”¨å¤šGPU
        config_path (str): é…ç½®æ–‡ä»¶è·¯å¾„

    Returns:
        dict: å®éªŒç»“æœå­—å…¸
    """
    exp_name = f"grid_{exp_id}"

    # å®éªŒä¿¡æ¯å°†åœ¨è®­ç»ƒå™¨ä¸­çš„SwanLabå¯åŠ¨åæ˜¾ç¤º

    if use_multi_gpu:
        # å¤šå¡è®­ç»ƒï¼šä½¿ç”¨å­è¿›ç¨‹æ–¹å¼
        result = run_single_experiment_subprocess(params, exp_id, use_multi_gpu, config_path)
    else:
        # å•å¡è®­ç»ƒï¼šä½¿ç”¨è¿›ç¨‹å†…è°ƒç”¨æ–¹å¼
        result = run_single_experiment_in_process(params, exp_id, config_path)
    
    print(f"âœ… å®éªŒ {exp_name} å®Œæˆï¼Œæœ€ä½³: {result['best_accuracy']:.2f}% | æœ€ç»ˆ: {result['final_accuracy']:.2f}%")
    
    return result


def run_grid_search(args):
    """è¿è¡Œç½‘æ ¼æœç´¢"""
    config = load_grid_config(args.config)
    combinations = generate_combinations(config)

    if len(combinations) > args.max_experiments:
        combinations = combinations[:args.max_experiments]

    # å‡†å¤‡CSVæ–‡ä»¶
    results_dir = "runs"
    if args.results_file:
        # ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„æ–‡ä»¶å
        results_filename = args.results_file
    else:
        # ä½¿ç”¨é»˜è®¤çš„æ—¶é—´æˆ³æ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_filename = f"grid_search_results_{timestamp}.csv"
    csv_filepath = os.path.join(results_dir, results_filename)
    
    # è·å–CSVå­—æ®µå
    all_params = [params for params in combinations]
    fieldnames = get_csv_fieldnames(all_params)
    
    # åˆå§‹åŒ–CSVæ–‡ä»¶
    if args.save_results:
        os.makedirs(results_dir, exist_ok=True)
        initialize_csv_file(csv_filepath, fieldnames)
    else:
        # ä¸ä¿å­˜ç»“æœæ—¶ä¹Ÿéœ€è¦åˆå§‹åŒ–
        initialize_csv_file(csv_filepath, fieldnames)

    print(f"ğŸš€ å¼€å§‹ç½‘æ ¼æœç´¢ï¼Œå…± {len(combinations)} ä¸ªå®éªŒ")
    print(f"ğŸ“Š ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
    print(f"ğŸ’¾ ç»“æœæ–‡ä»¶: {csv_filepath}")
    
    # å¤„ç†data_percentageå‚æ•°ï¼šå¦‚æœæœªæŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤å€¼1.0
    data_percentage = args.data_percentage if args.data_percentage is not None else 1.0

    # æ˜¾ç¤ºå…¨å±€å‚æ•°è¦†ç›–
    if args.data_percentage is not None:
        print(f"ğŸ¯ å…¨å±€å‚æ•°è¦†ç›–: data_percentage={args.data_percentage}")
    else:
        print(f"ğŸ¯ ä½¿ç”¨é»˜è®¤data_percentage: {data_percentage}")

    print("=" * 60)

    results = []
    successful = 0

    for i, params in enumerate(combinations, 1):
        exp_name = f"grid_{i:03d}"

        print(f"ğŸ“Š å‡†å¤‡å®éªŒ {i}/{len(combinations)}")

        # å°†å‘½ä»¤è¡Œå‚æ•°æ·»åŠ åˆ°å®éªŒå‚æ•°ä¸­
        experiment_params = params.copy()
        # å§‹ç»ˆæ·»åŠ data_percentageå‚æ•°ï¼Œç¡®ä¿CSVè®°å½•å®Œæ•´
        experiment_params['data_percentage'] = data_percentage

        result = run_single_experiment(
            experiment_params, f"{i:03d}",
            use_multi_gpu=args.multi_gpu,
            config_path=args.config,
        )

        results.append(result)
        if result["success"]:
            successful += 1
            
        # å®æ—¶å†™å…¥CSV
        if args.save_results:
            print(f"ğŸ’¾ å†™å…¥å®éªŒç»“æœåˆ°CSV: {result.get('exp_name', 'unknown')}")
            append_result_to_csv(result, csv_filepath, fieldnames)
            
        # å®æ—¶æ˜¾ç¤ºæœ€ä½³ç»“æœ
        if successful > 0:
            current_best = max([r for r in results if r["success"]], key=lambda x: x["best_accuracy"])
            print(f"ğŸ† å½“å‰æœ€ä½³: {current_best['exp_name']} - {current_best['best_accuracy']:.2f}%")

    # æ€»ç»“
    print("=" * 60)
    print(f"ğŸ“ˆ ç½‘æ ¼æœç´¢å®Œæˆï¼")
    print(f"âœ… æˆåŠŸå®éªŒæ•°é‡: {successful}/{len(combinations)}")

    if successful > 0:
        successful_results = [r for r in results if r["success"]]
        # æ‰¾åˆ°â€œæœ€ä½³å‡†ç¡®ç‡â€æœ€é«˜çš„å®éªŒç»“æœ
        best_result = max(successful_results, key=lambda x: x["best_accuracy"])

        print(f"ğŸ† æœ€ä½³å®éªŒç»“æœ:")
        print(f"å®éªŒåç§°: {best_result['exp_name']}, æœ€ä½³å‡†ç¡®ç‡: {best_result['best_accuracy']:.2f}%, æœ€ç»ˆå‡†ç¡®ç‡: {best_result['final_accuracy']:.2f}%")
        
        # æŒ‰æœ€ä½³ç²¾åº¦æ’åºå‰nç»„ç»“æœ
        top_results = sorted(successful_results, key=lambda x: x["best_accuracy"], reverse=True)[:args.top_n]
        
        print(f"ğŸ“Š å‰{args.top_n}åå®éªŒç»“æœ:")
        for i, r in enumerate(top_results, 1):
            print(f"{i}. {r['exp_name']} - {r['best_accuracy']:.2f}% - {r['params']}")

    if args.save_results:
        print(f"ğŸ’¾ ç»“æœå·²å®æ—¶ä¿å­˜åˆ°: {csv_filepath}")

    return 0 if successful > 0 else 1


def main():
    """ä¸»å‡½æ•°ï¼šè°ƒåº¦å™¨å§‹ç»ˆå•è¿›ç¨‹ï¼Œä¸è¿›å…¥ Accelerate ç¯å¢ƒ"""
    args, _ = parse_arguments(mode="grid_search")
    return run_grid_search(args)


if __name__ == "__main__":
    sys.exit(main())