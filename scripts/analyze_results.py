#!/usr/bin/env python3
"""
ç½‘æ ¼æœç´¢ç»“æœåˆ†æå’Œå¯è§†åŒ–å·¥å…·

åŠŸèƒ½ï¼š
1. ç»Ÿä¸€ç®¡ç†å’Œåˆ†æå¤šä¸ªç½‘æ ¼æœç´¢ç»“æœæ–‡ä»¶
2. ç”Ÿæˆå¤šæ ‡ç­¾åˆ†ç±»çš„æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š
3. æä¾›æ¨¡å‹æ¨èå’Œæœ€ä½³é…ç½®å»ºè®®
4. åˆ›å»ºå¯è§†åŒ–å›¾è¡¨å’Œçƒ­åŠ›å›¾
"""

import os
import sys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import json
import argparse
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class GridSearchResultsAnalyzer:
    """ç½‘æ ¼æœç´¢ç»“æœåˆ†æå™¨"""
    
    def __init__(self, results_dir: str = "runs"):
        """åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            results_dir: ç»“æœæ–‡ä»¶æ ¹ç›®å½•
        """
        self.results_dir = Path(results_dir)
        self.all_results = []
        self.task_results = {}
        
    def discover_result_files(self) -> Dict[str, List[Path]]:
        """å‘ç°æ‰€æœ‰ç»“æœæ–‡ä»¶
        
        Returns:
            æŒ‰ä»»åŠ¡ç±»å‹åˆ†ç»„çš„ç»“æœæ–‡ä»¶å­—å…¸
        """
        task_files = {}
        
        # éå†runsç›®å½•ä¸‹çš„æ‰€æœ‰å­ç›®å½•
        for task_dir in self.results_dir.iterdir():
            if task_dir.is_dir():
                task_name = task_dir.name
                csv_files = list(task_dir.glob("grid_search_results_*.csv"))
                
                if csv_files:
                    task_files[task_name] = csv_files
                    print(f"ğŸ“ å‘ç°ä»»åŠ¡ '{task_name}': {len(csv_files)} ä¸ªç»“æœæ–‡ä»¶")
        
        return task_files
    
    def load_all_results(self) -> None:
        """åŠ è½½æ‰€æœ‰ç»“æœæ–‡ä»¶"""
        task_files = self.discover_result_files()
        
        for task_name, csv_files in task_files.items():
            task_results = []
            
            for csv_file in csv_files:
                try:
                    df = pd.read_csv(csv_file)
                    df['task_type'] = task_name
                    df['result_file'] = csv_file.name
                    task_results.append(df)
                    print(f"âœ… åŠ è½½ {csv_file.name}: {len(df)} ä¸ªå®éªŒ")
                except Exception as e:
                    print(f"âŒ åŠ è½½å¤±è´¥ {csv_file}: {e}")
            
            if task_results:
                combined_df = pd.concat(task_results, ignore_index=True)
                self.task_results[task_name] = combined_df
                self.all_results.append(combined_df)
        
        if self.all_results:
            self.combined_results = pd.concat(self.all_results, ignore_index=True)
            print(f"\nğŸ“Š æ€»è®¡åŠ è½½ {len(self.combined_results)} ä¸ªå®éªŒç»“æœ")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•ç»“æœæ–‡ä»¶")
    
    def analyze_multilabel_performance(self, task_name: str) -> Dict[str, Any]:
        """åˆ†æå¤šæ ‡ç­¾åˆ†ç±»æ€§èƒ½
        
        Args:
            task_name: ä»»åŠ¡åç§°
            
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        if task_name not in self.task_results:
            return {}
        
        df = self.task_results[task_name]
        
        # è¿‡æ»¤æˆåŠŸçš„å®éªŒ
        successful_df = df[df['success'] == True].copy()
        
        if len(successful_df) == 0:
            return {"error": "æ²¡æœ‰æˆåŠŸçš„å®éªŒ"}
        
        analysis = {
            "total_experiments": len(df),
            "successful_experiments": len(successful_df),
            "success_rate": len(successful_df) / len(df) * 100,
        }
        
        # å¤šæ ‡ç­¾æŒ‡æ ‡åˆ†æ
        multilabel_metrics = [
            'best_macro_accuracy', 'best_micro_accuracy', 'best_weighted_accuracy',
            'best_macro_f1', 'best_micro_f1', 'best_weighted_f1'
        ]
        
        for metric in multilabel_metrics:
            if metric in successful_df.columns:
                values = successful_df[metric].dropna()
                if len(values) > 0:
                    analysis[f"{metric}_stats"] = {
                        "mean": float(values.mean()),
                        "std": float(values.std()),
                        "min": float(values.min()),
                        "max": float(values.max()),
                        "median": float(values.median())
                    }
        
        # æ‰¾åˆ°æœ€ä½³æ¨¡å‹
        if 'best_weighted_f1' in successful_df.columns:
            # è¿‡æ»¤æ‰NaNå€¼
            valid_f1_df = successful_df.dropna(subset=['best_weighted_f1'])
            if len(valid_f1_df) > 0:
                best_idx = valid_f1_df['best_weighted_f1'].idxmax()
                analysis["best_model"] = {
                    "exp_name": valid_f1_df.loc[best_idx, 'exp_name'],
                    "model_type": valid_f1_df.loc[best_idx, 'model.type'],
                    "weighted_f1": valid_f1_df.loc[best_idx, 'best_weighted_f1'],
                    "macro_accuracy": valid_f1_df.loc[best_idx, 'best_macro_accuracy'] if 'best_macro_accuracy' in valid_f1_df.columns else None,
                    "micro_accuracy": valid_f1_df.loc[best_idx, 'best_micro_accuracy'] if 'best_micro_accuracy' in valid_f1_df.columns else None,
                    "weighted_accuracy": valid_f1_df.loc[best_idx, 'best_weighted_accuracy'] if 'best_weighted_accuracy' in valid_f1_df.columns else None
                }
        
        return analysis
    
    def generate_performance_comparison(self, task_name: str, output_dir: str) -> None:
        """ç”Ÿæˆæ€§èƒ½å¯¹æ¯”å›¾è¡¨
        
        Args:
            task_name: ä»»åŠ¡åç§°
            output_dir: è¾“å‡ºç›®å½•
        """
        if task_name not in self.task_results:
            return
        
        df = self.task_results[task_name]
        successful_df = df[df['success'] == True].copy()
        
        if len(successful_df) == 0:
            print(f"âš ï¸ ä»»åŠ¡ {task_name} æ²¡æœ‰æˆåŠŸçš„å®éªŒ")
            return
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # 1. æ¨¡å‹æ€§èƒ½å¯¹æ¯”å›¾
        if 'model.type' in successful_df.columns:
            plt.figure(figsize=(12, 8))
            
            # å‡†å¤‡æ•°æ®
            metrics_to_plot = ['best_macro_f1', 'best_micro_f1', 'best_weighted_f1']
            available_metrics = [m for m in metrics_to_plot if m in successful_df.columns]
            
            if available_metrics:
                model_performance = successful_df.groupby('model.type')[available_metrics].mean()
                
                # ç»˜åˆ¶æ¡å½¢å›¾
                ax = model_performance.plot(kind='bar', figsize=(12, 6))
                plt.title(f'{task_name} - æ¨¡å‹æ€§èƒ½å¯¹æ¯” (F1åˆ†æ•°)', fontsize=14, fontweight='bold')
                plt.xlabel('æ¨¡å‹ç±»å‹', fontsize=12)
                plt.ylabel('F1åˆ†æ•°', fontsize=12)
                plt.legend(['å®å¹³å‡F1', 'å¾®å¹³å‡F1', 'åŠ æƒå¹³å‡F1'])
                plt.xticks(rotation=45)
                plt.tight_layout()
                
                # ä¿å­˜å›¾è¡¨
                plt.savefig(os.path.join(output_dir, f'{task_name}_model_f1_comparison.png'), 
                           dpi=300, bbox_inches='tight')
                plt.close()
        
        # 2. å‡†ç¡®ç‡å¯¹æ¯”å›¾
        accuracy_metrics = ['best_macro_accuracy', 'best_micro_accuracy', 'best_weighted_accuracy']
        available_acc_metrics = [m for m in accuracy_metrics if m in successful_df.columns]
        
        if available_acc_metrics and 'model.type' in successful_df.columns:
            plt.figure(figsize=(12, 6))
            
            model_accuracy = successful_df.groupby('model.type')[available_acc_metrics].mean()
            
            ax = model_accuracy.plot(kind='bar', figsize=(12, 6))
            plt.title(f'{task_name} - æ¨¡å‹å‡†ç¡®ç‡å¯¹æ¯”', fontsize=14, fontweight='bold')
            plt.xlabel('æ¨¡å‹ç±»å‹', fontsize=12)
            plt.ylabel('å‡†ç¡®ç‡', fontsize=12)
            plt.legend(['å®å¹³å‡å‡†ç¡®ç‡', 'å¾®å¹³å‡å‡†ç¡®ç‡', 'åŠ æƒå¹³å‡å‡†ç¡®ç‡'])
            plt.xticks(rotation=45)
            plt.tight_layout()
            
            plt.savefig(os.path.join(output_dir, f'{task_name}_model_accuracy_comparison.png'), 
                       dpi=300, bbox_inches='tight')
            plt.close()
        
        print(f"ğŸ“Š æ€§èƒ½å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜åˆ°: {output_dir}")
    
    def generate_summary_report(self, output_file: str) -> None:
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        report = {
            "analysis_timestamp": pd.Timestamp.now().isoformat(),
            "total_tasks": len(self.task_results),
            "tasks": {}
        }
        
        for task_name in self.task_results.keys():
            analysis = self.analyze_multilabel_performance(task_name)
            report["tasks"][task_name] = analysis
        
        # ä¿å­˜JSONæŠ¥å‘Š
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“‹ æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        
        # æ‰“å°ç®€è¦æŠ¥å‘Š
        print("\n" + "="*60)
        print("ğŸ“Š ç½‘æ ¼æœç´¢ç»“æœæ±‡æ€»æŠ¥å‘Š")
        print("="*60)
        
        for task_name, analysis in report["tasks"].items():
            if "error" not in analysis:
                print(f"\nğŸ¯ ä»»åŠ¡: {task_name}")
                print(f"  æ€»å®éªŒæ•°: {analysis['total_experiments']}")
                print(f"  æˆåŠŸå®éªŒæ•°: {analysis['successful_experiments']}")
                print(f"  æˆåŠŸç‡: {analysis['success_rate']:.1f}%")
                
                if "best_model" in analysis:
                    best = analysis["best_model"]
                    print(f"  ğŸ† æœ€ä½³æ¨¡å‹: {best['model_type']}")
                    print(f"    åŠ æƒF1: {best['weighted_f1']:.4f}")
                    print(f"    åŠ æƒå‡†ç¡®ç‡: {best['weighted_accuracy']:.4f}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ç½‘æ ¼æœç´¢ç»“æœåˆ†æå·¥å…·")
    parser.add_argument("--results_dir", type=str, default="runs", 
                       help="ç»“æœæ–‡ä»¶æ ¹ç›®å½•")
    parser.add_argument("--output_dir", type=str, default="analysis_output", 
                       help="åˆ†æç»“æœè¾“å‡ºç›®å½•")
    parser.add_argument("--task", type=str, default=None, 
                       help="æŒ‡å®šåˆ†æçš„ä»»åŠ¡ç±»å‹ï¼Œä¸æŒ‡å®šåˆ™åˆ†ææ‰€æœ‰ä»»åŠ¡")
    
    args = parser.parse_args()
    
    print("ğŸ” å¼€å§‹åˆ†æç½‘æ ¼æœç´¢ç»“æœ...")
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = GridSearchResultsAnalyzer(args.results_dir)
    
    # åŠ è½½æ‰€æœ‰ç»“æœ
    analyzer.load_all_results()
    
    if not analyzer.task_results:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•ç»“æœæ–‡ä»¶")
        return 1
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    report_file = os.path.join(args.output_dir, "grid_search_summary_report.json")
    analyzer.generate_summary_report(report_file)
    
    # ä¸ºæ¯ä¸ªä»»åŠ¡ç”Ÿæˆæ€§èƒ½å¯¹æ¯”å›¾è¡¨
    if args.task:
        if args.task in analyzer.task_results:
            analyzer.generate_performance_comparison(args.task, args.output_dir)
        else:
            print(f"âŒ æœªæ‰¾åˆ°ä»»åŠ¡ '{args.task}' çš„ç»“æœ")
    else:
        for task_name in analyzer.task_results.keys():
            analyzer.generate_performance_comparison(task_name, args.output_dir)
    
    print(f"\nâœ… åˆ†æå®Œæˆï¼ç»“æœä¿å­˜åœ¨: {args.output_dir}")
    return 0


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
