"""
æŸå¤±å‡½æ•°å·¥å‚æ¨¡å—
åŒ…å«å›¾åƒåˆ†ç±»å’Œè§†é¢‘åˆ†ç±»ä»»åŠ¡çš„å¸¸ç”¨æŸå¤±å‡½æ•°å®šä¹‰å’Œå·¥å‚å‡½æ•°
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


class MultilabelBCELoss(nn.Module):
    """
    å¤šæ ‡ç­¾äºŒå…ƒäº¤å‰ç†µæŸå¤±å‡½æ•°

    ç”¨äºå¤šæ ‡ç­¾åˆ†ç±»ä»»åŠ¡ï¼Œæ¯ä¸ªæ ‡ç­¾ç‹¬ç«‹è¿›è¡ŒäºŒå…ƒåˆ†ç±»ã€‚
    æ”¯æŒç±»åˆ«æƒé‡å’Œä½ç½®æƒé‡æ¥å¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚

    Args:
        pos_weight (torch.Tensor, optional): æ­£æ ·æœ¬æƒé‡ï¼Œç”¨äºå¤„ç†æ­£è´Ÿæ ·æœ¬ä¸å¹³è¡¡
        weight (torch.Tensor, optional): ç±»åˆ«æƒé‡ï¼Œç”¨äºå¤„ç†ç±»åˆ«ä¸å¹³è¡¡
        reduction (str, optional): æŸå¤±èšåˆæ–¹å¼ï¼Œ'mean'ã€'sum'æˆ–'none'ï¼Œé»˜è®¤ä¸º'mean'
    """

    def __init__(self, pos_weight=None, weight=None, reduction='mean'):
        super(MultilabelBCELoss, self).__init__()
        self.pos_weight = pos_weight
        self.weight = weight
        self.reduction = reduction

    def forward(self, inputs, targets):
        """
        è®¡ç®—å¤šæ ‡ç­¾BCEæŸå¤±

        Args:
            inputs (torch.Tensor): æ¨¡å‹è¾“å‡ºçš„logitsï¼Œå½¢çŠ¶ä¸º(batch_size, num_classes)
            targets (torch.Tensor): çœŸå®æ ‡ç­¾ï¼Œå½¢çŠ¶ä¸º(batch_size, num_classes)ï¼Œå€¼ä¸º0æˆ–1

        Returns:
            torch.Tensor: è®¡ç®—å¾—åˆ°çš„å¤šæ ‡ç­¾BCEæŸå¤±
        """
        # ç¡®ä¿pos_weightå’Œweightåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        pos_weight = self.pos_weight
        weight = self.weight

        if pos_weight is not None:
            pos_weight = pos_weight.to(inputs.device)
        if weight is not None:
            weight = weight.to(inputs.device)

        # ä½¿ç”¨sigmoidæ¿€æ´»å‡½æ•°å°†logitsè½¬æ¢ä¸ºæ¦‚ç‡
        loss = F.binary_cross_entropy_with_logits(
            inputs, targets,
            pos_weight=pos_weight,
            weight=weight,
            reduction=self.reduction
        )
        return loss


class FocalLoss(nn.Module):
    """
    Focal LossæŸå¤±å‡½æ•°ï¼Œç”¨äºè§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜
    
    é€šè¿‡é™ä½æ˜“åˆ†ç±»æ ·æœ¬çš„æƒé‡ï¼Œè®©æ¨¡å‹æ›´å…³æ³¨éš¾åˆ†ç±»æ ·æœ¬ã€‚
    åŸè®ºæ–‡ï¼šhttps://arxiv.org/abs/1708.02002
    
    Args:
        alpha (float, optional): å¹³è¡¡å› å­ï¼Œç”¨äºå¹³è¡¡æ­£è´Ÿæ ·æœ¬ï¼Œé»˜è®¤ä¸º1
        gamma (float, optional): è°ƒåˆ¶å› å­ï¼Œç”¨äºè°ƒæ•´éš¾æ˜“æ ·æœ¬çš„æƒé‡ï¼Œé»˜è®¤ä¸º2
        reduction (str, optional): æŸå¤±èšåˆæ–¹å¼ï¼Œ'mean'æˆ–'sum'ï¼Œé»˜è®¤ä¸º'mean'
    """
    
    def __init__(self, alpha=1, gamma=2, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
    
    def forward(self, inputs, targets):
        """
        è®¡ç®—Focal Loss
        
        Args:
            inputs (torch.Tensor): æ¨¡å‹è¾“å‡ºçš„logitsï¼Œå½¢çŠ¶ä¸º(batch_size, num_classes)
            targets (torch.Tensor): çœŸå®æ ‡ç­¾ï¼Œå½¢çŠ¶ä¸º(batch_size,)
            
        Returns:
            torch.Tensor: è®¡ç®—å¾—åˆ°çš„Focal Loss
        """
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss


class MultilabelFocalLoss(nn.Module):
    """
    å¤šæ ‡ç­¾Focal LossæŸå¤±å‡½æ•°

    ç»“åˆå¤šæ ‡ç­¾äºŒå…ƒäº¤å‰ç†µæŸå¤±å’ŒFocal Lossæœºåˆ¶ï¼Œä¸“é—¨ç”¨äºè§£å†³å¤šæ ‡ç­¾åˆ†ç±»ä¸­çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚
    é€šè¿‡é™ä½æ˜“åˆ†ç±»æ ·æœ¬çš„æƒé‡ï¼Œè®©æ¨¡å‹æ›´ä¸“æ³¨äºéš¾åˆ†ç±»çš„æ ·æœ¬ï¼Œç‰¹åˆ«æ˜¯å°‘æ•°ç±»åˆ«æ ·æœ¬ã€‚

    æ ¸å¿ƒç‰¹æ€§ï¼š
    1. æ”¯æŒå¤šæ ‡ç­¾åˆ†ç±»ï¼ˆæ¯ä¸ªæ ·æœ¬å¯ä»¥æœ‰å¤šä¸ªæ­£æ ‡ç­¾ï¼‰
    2. è‡ªåŠ¨é™ä½æ˜“åˆ†ç±»æ ·æœ¬çš„æƒé‡ï¼ˆé€šè¿‡gammaå‚æ•°æ§åˆ¶ï¼‰
    3. æ”¯æŒç±»åˆ«å¹³è¡¡ï¼ˆé€šè¿‡alphaå‚æ•°æ§åˆ¶ï¼‰
    4. æ”¯æŒæ­£æ ·æœ¬æƒé‡ï¼ˆé€šè¿‡pos_weightå‚æ•°å¤„ç†æ­£è´Ÿæ ·æœ¬ä¸å¹³è¡¡ï¼‰

    Args:
        alpha (float or torch.Tensor, optional): ç±»åˆ«å¹³è¡¡å‚æ•°ï¼Œç”¨äºå¹³è¡¡æ­£è´Ÿæ ·æœ¬
            - å¦‚æœæ˜¯floatï¼Œæ‰€æœ‰ç±»åˆ«ä½¿ç”¨ç›¸åŒçš„alphaå€¼
            - å¦‚æœæ˜¯Tensorï¼Œæ¯ä¸ªç±»åˆ«ä½¿ç”¨ä¸åŒçš„alphaå€¼
            - é»˜è®¤ä¸º1.0ï¼ˆä¸è¿›è¡Œç±»åˆ«å¹³è¡¡ï¼‰
        gamma (float, optional): èšç„¦å‚æ•°ï¼Œç”¨äºè°ƒæ•´éš¾æ˜“æ ·æœ¬çš„æƒé‡
            - gamma=0æ—¶é€€åŒ–ä¸ºæ ‡å‡†BCEæŸå¤±
            - gamma>0æ—¶é™ä½æ˜“åˆ†ç±»æ ·æœ¬çš„æƒé‡
            - é€šå¸¸å–å€¼2.0ï¼Œé»˜è®¤ä¸º2.0
        pos_weight (torch.Tensor, optional): æ­£æ ·æœ¬æƒé‡ï¼Œç”¨äºå¤„ç†æ­£è´Ÿæ ·æœ¬ä¸å¹³è¡¡
            - å½¢çŠ¶ä¸º(num_classes,)ï¼Œæ¯ä¸ªç±»åˆ«ä¸€ä¸ªæƒé‡å€¼
            - é»˜è®¤ä¸ºNoneï¼ˆä¸ä½¿ç”¨æ­£æ ·æœ¬æƒé‡ï¼‰
        reduction (str, optional): æŸå¤±èšåˆæ–¹å¼ï¼Œ'mean'ã€'sum'æˆ–'none'ï¼Œé»˜è®¤ä¸º'mean'

    æ•°å­¦å…¬å¼ï¼š
        å¯¹äºæ¯ä¸ªç±»åˆ«cå’Œæ ·æœ¬iï¼š
        FL(p_ic) = -Î±_c * (1 - p_ic)^Î³ * log(p_ic)

        å…¶ä¸­ï¼š
        - p_ic æ˜¯æ ·æœ¬iåœ¨ç±»åˆ«cä¸Šçš„é¢„æµ‹æ¦‚ç‡
        - Î±_c æ˜¯ç±»åˆ«cçš„å¹³è¡¡å‚æ•°
        - Î³ æ˜¯èšç„¦å‚æ•°

    ç¤ºä¾‹ï¼š
        >>> # åŸºæœ¬ç”¨æ³•
        >>> loss_fn = MultilabelFocalLoss(alpha=1.0, gamma=2.0)
        >>>
        >>> # ä½¿ç”¨ç±»åˆ«å¹³è¡¡å’Œæ­£æ ·æœ¬æƒé‡
        >>> alpha = torch.tensor([0.25, 0.75])  # ä¸ºä¸¤ä¸ªç±»åˆ«è®¾ç½®ä¸åŒçš„alpha
        >>> pos_weight = torch.tensor([2.0, 3.0])  # ä¸ºä¸¤ä¸ªç±»åˆ«è®¾ç½®ä¸åŒçš„æ­£æ ·æœ¬æƒé‡
        >>> loss_fn = MultilabelFocalLoss(alpha=alpha, gamma=2.0, pos_weight=pos_weight)
    """

    def __init__(self, alpha=1.0, gamma=1.0, pos_weight=None, reduction='mean'):
        """
        ğŸ”§ ä¼˜åŒ–: é™ä½gammaé»˜è®¤å€¼ä»2.0åˆ°1.0

        åŸå› : gamma=2.0å¯¹äºæåº¦ä¸å¹³è¡¡çš„æ•°æ®é›†è¿‡äºæ¿€è¿›ï¼Œä¼šå¯¼è‡´:
        1. è¿‡åº¦é™ä½æ˜“åˆ†ç±»æ ·æœ¬(é€šå¸¸æ˜¯è´Ÿæ ·æœ¬)çš„æƒé‡
        2. æ¨¡å‹è¿‡åº¦å…³æ³¨éš¾åˆ†ç±»æ ·æœ¬
        3. ç»“åˆé«˜pos_weightæ—¶ï¼Œå¯¼è‡´æ¨¡å‹è¿‡åº¦é¢„æµ‹æ­£ç±»

        gamma=1.0æä¾›æ›´æ¸©å’Œçš„èšç„¦æ•ˆæœï¼Œé€‚åˆæåº¦ä¸å¹³è¡¡çš„å¤šæ ‡ç­¾åˆ†ç±»
        """
        super(MultilabelFocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.pos_weight = pos_weight
        self.reduction = reduction

        # å¦‚æœalphaæ˜¯æ ‡é‡ï¼Œåœ¨forwardä¸­ä¼šæ ¹æ®ç±»åˆ«æ•°é‡æ‰©å±•
        if isinstance(alpha, (int, float)):
            self.alpha_scalar = alpha
            self.alpha_tensor = None
        else:
            self.alpha_scalar = None
            self.alpha_tensor = alpha

    def forward(self, inputs, targets):
        """
        è®¡ç®—å¤šæ ‡ç­¾Focal Loss

        Args:
            inputs (torch.Tensor): æ¨¡å‹è¾“å‡ºçš„logitsï¼Œå½¢çŠ¶ä¸º(batch_size, num_classes)
            targets (torch.Tensor): çœŸå®æ ‡ç­¾ï¼Œå½¢çŠ¶ä¸º(batch_size, num_classes)ï¼Œå€¼ä¸º0æˆ–1

        Returns:
            torch.Tensor: è®¡ç®—å¾—åˆ°çš„å¤šæ ‡ç­¾Focal Loss
        """
        # ç¡®ä¿è¾“å…¥åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        device = inputs.device
        batch_size, num_classes = inputs.shape

        # å°†logitsè½¬æ¢ä¸ºæ¦‚ç‡
        probs = torch.sigmoid(inputs)

        # å¤„ç†alphaå‚æ•°
        if self.alpha_scalar is not None:
            # å¦‚æœalphaæ˜¯æ ‡é‡ï¼Œä¸ºæ¯ä¸ªç±»åˆ«åˆ›å»ºç›¸åŒçš„alphaå€¼
            alpha = torch.full((num_classes,), self.alpha_scalar, device=device)
        elif self.alpha_tensor is not None:
            # å¦‚æœalphaæ˜¯tensorï¼Œç¡®ä¿åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            alpha = self.alpha_tensor.to(device)
            if alpha.shape[0] != num_classes:
                raise ValueError(f"alpha tensorçš„é•¿åº¦({alpha.shape[0]})å¿…é¡»ç­‰äºç±»åˆ«æ•°é‡({num_classes})")
        else:
            # é»˜è®¤æƒ…å†µä¸‹ï¼Œæ‰€æœ‰ç±»åˆ«çš„alphaä¸º1.0
            alpha = torch.ones(num_classes, device=device)

        # å¤„ç†pos_weightå‚æ•°
        pos_weight = self.pos_weight
        if pos_weight is not None:
            pos_weight = pos_weight.to(device)
            if pos_weight.shape[0] != num_classes:
                raise ValueError(f"pos_weight tensorçš„é•¿åº¦({pos_weight.shape[0]})å¿…é¡»ç­‰äºç±»åˆ«æ•°é‡({num_classes})")

        # è®¡ç®—äºŒå…ƒäº¤å‰ç†µæŸå¤±ï¼ˆä¸è¿›è¡Œreductionï¼‰
        bce_loss = F.binary_cross_entropy_with_logits(
            inputs, targets,
            pos_weight=pos_weight,
            reduction='none'
        )

        # è®¡ç®—ptï¼ˆæ­£ç¡®åˆ†ç±»çš„æ¦‚ç‡ï¼‰
        # å¯¹äºæ­£æ ·æœ¬ï¼špt = p
        # å¯¹äºè´Ÿæ ·æœ¬ï¼špt = 1 - p
        pt = torch.where(targets == 1, probs, 1 - probs)

        # ğŸ”§ ä¿®å¤ï¼šalphaæƒé‡é€»è¾‘ï¼ˆä½ä¼˜å…ˆçº§ä¿®å¤ï¼‰
        # è®¡ç®—alphaæƒé‡
        if pos_weight is not None:
            # å¦‚æœä½¿ç”¨äº†pos_weightï¼Œä¸å†ä½¿ç”¨alphaè¿›è¡Œé¢å¤–çš„ç±»åˆ«å¹³è¡¡
            # é¿å…åŒé‡åŠ æƒå¯¼è‡´çš„è¿‡åº¦æƒ©ç½šæˆ–å¥–åŠ±
            alpha_weight = torch.ones_like(targets)
        else:
            # å¦‚æœæ²¡æœ‰ä½¿ç”¨pos_weightï¼Œä½¿ç”¨ä¼ ç»Ÿçš„alphaæƒé‡åˆ†é…
            # æ­£æ ·æœ¬æƒé‡ä¸ºalphaï¼Œè´Ÿæ ·æœ¬æƒé‡ä¸º1-alpha
            alpha_weight = torch.where(targets == 1, alpha, 1 - alpha)

        # è®¡ç®—Focal Loss
        # FL = -Î± * (1 - pt)^Î³ * log(pt)
        # ç”±äºbce_loss = -log(pt)ï¼Œæ‰€ä»¥ï¼š
        # FL = Î± * (1 - pt)^Î³ * bce_loss
        focal_weight = alpha_weight * (1 - pt) ** self.gamma
        focal_loss = focal_weight * bce_loss

        # åº”ç”¨reduction
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        elif self.reduction == 'none':
            return focal_loss
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„reductionæ–¹å¼: {self.reduction}")


class LabelSmoothingLoss(nn.Module):
    """
    æ ‡ç­¾å¹³æ»‘æŸå¤±å‡½æ•°ï¼Œç”¨äºæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›
    
    é€šè¿‡åœ¨çœŸå®æ ‡ç­¾ä¸­æ·»åŠ å™ªå£°ï¼Œé˜²æ­¢æ¨¡å‹å¯¹è®­ç»ƒæ•°æ®è¿‡æ‹Ÿåˆã€‚
    åŸè®ºæ–‡ï¼šhttps://arxiv.org/abs/1512.00567
    
    Args:
        num_classes (int): ç±»åˆ«æ•°é‡
        smoothing (float, optional): å¹³æ»‘ç³»æ•°ï¼Œå–å€¼èŒƒå›´[0,1)ï¼Œé»˜è®¤ä¸º0.1
    """
    
    def __init__(self, num_classes, smoothing=0.1):
        super(LabelSmoothingLoss, self).__init__()
        self.num_classes = num_classes
        self.smoothing = smoothing
        self.confidence = 1.0 - smoothing
    
    def forward(self, inputs, targets):
        """
        è®¡ç®—æ ‡ç­¾å¹³æ»‘æŸå¤±
        
        Args:
            inputs (torch.Tensor): æ¨¡å‹è¾“å‡ºçš„logitsï¼Œå½¢çŠ¶ä¸º(batch_size, num_classes)
            targets (torch.Tensor): çœŸå®æ ‡ç­¾ï¼Œå½¢çŠ¶ä¸º(batch_size,)
            
        Returns:
            torch.Tensor: è®¡ç®—å¾—åˆ°çš„æ ‡ç­¾å¹³æ»‘æŸå¤±
        """
        log_probs = F.log_softmax(inputs, dim=1)
        targets_one_hot = torch.zeros_like(log_probs).scatter_(1, targets.unsqueeze(1), 1)
        targets_smooth = targets_one_hot * self.confidence + (1 - targets_one_hot) * self.smoothing / (self.num_classes - 1)
        loss = (-targets_smooth * log_probs).sum(dim=1).mean()
        return loss


def get_loss_function(loss_config=None, loss_name=None, **kwargs):
    """
    æŸå¤±å‡½æ•°å·¥å‚å‡½æ•°ï¼Œåˆ›å»ºå¹¶é…ç½®æŸå¤±å‡½æ•°å®ä¾‹

    Args:
        loss_config (dict, optional): æŸå¤±å‡½æ•°é…ç½®å­—å…¸
        loss_name (str, optional): æŸå¤±å‡½æ•°åç§°ï¼Œç”¨äºå‘åå…¼å®¹
        **kwargs: æŸå¤±å‡½æ•°å‚æ•°ï¼Œç”¨äºå‘åå…¼å®¹

    Returns:
        torch.nn.Module: é…ç½®å¥½çš„æŸå¤±å‡½æ•°å®ä¾‹

    ç¤ºä¾‹ï¼š
        >>> loss_fn = get_loss_function({'type': 'crossentropy', 'label_smoothing': 0.1})
        >>> loss_fn = get_loss_function(loss_name='crossentropy', label_smoothing=0.1)
    """
    # ç®€åŒ–çš„é…ç½®è§£æ
    if loss_config:
        loss_name = loss_config.get('type') or loss_config.get('name', 'crossentropy')
        params = loss_config.get('params', {}) if 'params' in loss_config else {k: v for k, v in loss_config.items() if k not in ['type', 'name']}
    else:
        loss_name = loss_name or 'crossentropy'
        params = kwargs

    loss_name = loss_name.lower()

    if loss_name == "crossentropy":
        return nn.CrossEntropyLoss(
            weight=params.get('weight', None),
            ignore_index=params.get('ignore_index', -100),
            reduction=params.get('reduction', 'mean'),
            label_smoothing=params.get('label_smoothing', 0.0)
        )
    elif loss_name == "focal":
        return FocalLoss(
            alpha=params.get('alpha', 1.0),
            gamma=params.get('gamma', 2.0),
            reduction=params.get('reduction', 'mean')
        )
    elif loss_name == "labelsmoothing":
        return LabelSmoothingLoss(
            num_classes=params.get('num_classes', 10),
            smoothing=params.get('smoothing', 0.1)
        )
    elif loss_name == "mse":
        return nn.MSELoss(
            reduction=params.get('reduction', 'mean')
        )
    elif loss_name == "l1":
        return nn.L1Loss(
            reduction=params.get('reduction', 'mean')
        )
    elif loss_name == "smoothl1":
        return nn.SmoothL1Loss(
            reduction=params.get('reduction', 'mean'),
            beta=params.get('beta', 1.0)
        )
    elif loss_name == "multilabel_bce":
        # å¤„ç†æ­£æ ·æœ¬æƒé‡
        pos_weight = params.get('pos_weight', None)
        num_classes = params.get('num_classes', None)

        if pos_weight is not None and not isinstance(pos_weight, torch.Tensor):
            # å¦‚æœæ˜¯æ ‡é‡ï¼Œåˆ›å»ºå¯¹åº”ç»´åº¦çš„tensor
            if isinstance(pos_weight, (int, float)):
                # åŠ¨æ€ç¡®å®šç±»åˆ«æ•°é‡
                if num_classes is None:
                    # å‘åå…¼å®¹ï¼šå¦‚æœæ²¡æœ‰æŒ‡å®šnum_classesï¼Œé»˜è®¤ä½¿ç”¨24ï¼ˆåŸå§‹æ–°ç”Ÿå„¿æ•°æ®ï¼‰
                    num_classes = 24
                pos_weight = torch.full((num_classes,), pos_weight)
            else:
                pos_weight = torch.tensor(pos_weight)

        return MultilabelBCELoss(
            pos_weight=pos_weight,
            weight=params.get('weight', None),
            reduction=params.get('reduction', 'mean')
        )
    elif loss_name == "focal_multilabel_bce" or loss_name == "multilabel_focal":
        # å¤„ç†alphaå‚æ•°ï¼ˆç±»åˆ«å¹³è¡¡å‚æ•°ï¼‰
        alpha = params.get('alpha', 1.0)
        if alpha is not None and not isinstance(alpha, torch.Tensor):
            if isinstance(alpha, (list, tuple)):
                alpha = torch.tensor(alpha, dtype=torch.float32)
            # å¦‚æœæ˜¯æ ‡é‡ï¼Œä¿æŒä¸ºæ ‡é‡ï¼Œåœ¨forwardä¸­å¤„ç†

        # å¤„ç†pos_weightå‚æ•°ï¼ˆæ­£æ ·æœ¬æƒé‡ï¼‰
        pos_weight = params.get('pos_weight', None)
        num_classes = params.get('num_classes', None)

        if pos_weight is not None and not isinstance(pos_weight, torch.Tensor):
            # å¦‚æœæ˜¯æ ‡é‡ï¼Œåˆ›å»ºå¯¹åº”ç»´åº¦çš„tensor
            if isinstance(pos_weight, (int, float)):
                # åŠ¨æ€ç¡®å®šç±»åˆ«æ•°é‡
                if num_classes is None:
                    # å¯¹äºæ–°ç”Ÿå„¿å¤šæ ‡ç­¾æ•°æ®ï¼Œé»˜è®¤ä½¿ç”¨7ä¸ªç±»åˆ«
                    num_classes = 7
                pos_weight = torch.full((num_classes,), pos_weight)
            elif isinstance(pos_weight, (list, tuple)):
                pos_weight = torch.tensor(pos_weight, dtype=torch.float32)

        return MultilabelFocalLoss(
            alpha=alpha,
            gamma=params.get('gamma', 1.0),  # ğŸ”§ é™ä½é»˜è®¤gammaä»2.0åˆ°1.0
            pos_weight=pos_weight,
            reduction=params.get('reduction', 'mean')
        )
    elif loss_name == "focal_multilabel_balanced" or loss_name == "multilabel_focal_balanced":
        # æ”¹è¿›ç‰ˆå¤šæ ‡ç­¾Focal Lossï¼Œä¸“é—¨ä¸ºä¸¥é‡ç±»åˆ«ä¸å¹³è¡¡è®¾è®¡
        # ä½¿ç”¨æ›´ä¿å®ˆçš„å‚æ•°é…ç½®ï¼Œé¿å…è¿‡åº¦é¢„æµ‹é—®é¢˜

        # å¤„ç†pos_weightå‚æ•°
        pos_weight = params.get('pos_weight', None)
        num_classes = params.get('num_classes', None)

        if pos_weight is not None and not isinstance(pos_weight, torch.Tensor):
            if isinstance(pos_weight, (int, float)):
                if num_classes is None:
                    num_classes = 7
                pos_weight = torch.full((num_classes,), pos_weight)
            elif isinstance(pos_weight, (list, tuple)):
                pos_weight = torch.tensor(pos_weight, dtype=torch.float32)

        # ä½¿ç”¨æ›´ä¿å®ˆçš„é»˜è®¤å‚æ•°
        return MultilabelFocalLoss(
            alpha=params.get('alpha', 1.0),  # é»˜è®¤ä¸ä½¿ç”¨alphaæƒé‡
            gamma=params.get('gamma', 1.0),  # æ›´ä¿å®ˆçš„gammaå€¼
            pos_weight=pos_weight,
            reduction=params.get('reduction', 'mean')
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æŸå¤±å‡½æ•°: {loss_name}ã€‚æ”¯æŒçš„æŸå¤±å‡½æ•°: crossentropy, focal, labelsmoothing, mse, l1, smoothl1, multilabel_bce, focal_multilabel_bce, focal_multilabel_balanced")
